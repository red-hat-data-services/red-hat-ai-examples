{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4d8595-c34f-4ab0-a0d7-f41d6aec95cd",
   "metadata": {},
   "source": [
    "## LoRA/QLoRA Fine-Tuning with Kubeflow Trainer and Training Hub on OpenShift AI\n",
    "\n",
    "This notebook demonstrates how to use Training Hub's LoRA (Low-Rank Adaptation) and QLoRA capabilities for parameter-efficient fine-tuning. We'll train a model to convert natural language questions into SQL queries using the popular [sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that:\n",
    "- Freezes the pre-trained model weights\n",
    "- Injects trainable low-rank matrices into each layer\n",
    "- Reduces trainable parameters by ~10,000x compared to full fine-tuning\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "\n",
    "**QLoRA** extends LoRA by adding 4-bit quantization, further reducing memory requirements while maintaining quality.\n",
    "\n",
    "## Training Task: Natural Language to SQL\n",
    "\n",
    "We'll train the model to understand database schemas and generate SQL queries from natural language questions. For example:\n",
    "\n",
    "**Input:**\n",
    "```\n",
    "Table: employees (id, name, department, salary)\n",
    "Question: What is the average salary in the engineering department?\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```sql\n",
    "SELECT AVG(salary) FROM employees WHERE department = 'engineering'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83ac4b-06f1-4c5d-b9fa-372cd3dd5ad2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf4f5f-244b-4283-a9a1-765f1ff5a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional workbench dependencies\n",
    "!pip install --upgrade pip --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install unsloth --quiet\n",
    "# Install Kubeflow Trainer V2.\n",
    "!pip install kubeflow --no-cache-dir --index-url https://console.redhat.com/api/pypi/public-rhai/rhoai/3.3/cuda12.9-ubi9/simple/ --quiet\n",
    "# !pip install kubeflow --no-cache-dir --index-url https://console.redhat.com/api/pypi/public-rhai/rhoai/3.3/cpu-ubi9/simple/\n",
    "# !pip install kubeflow --no-cache-dir --index-url https://console.redhat.com/api/pypi/public-rhai/rhoai/3.3/rocm6.4-ubi9/simple/\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from kubernetes import client as k8s\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee4db1-b144-4c93-92f9-37a195033649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to show only essential information\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "# Suppress verbose logging from transformers and other libraries\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"âœ… Logging configured for notebook environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef7bfb-c5b8-49bc-ae03-b1d19bdd6541",
   "metadata": {},
   "source": [
    "## Authenticate to your OpenShift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce03ef-189e-409d-a8e0-8dd486de83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_server = \"<REPLACE WITH OPENSHIFT SERVER>\"\n",
    "token = \"<REPLACE WITH OPENSHIFT TOKEN>\"\n",
    "PVC_NAME = \"shared\"  # Replace if the shared RWX storage name is different than in the example provided\n",
    "PVC_PATH = \"shared\"  # Replace if the shared RWX storage path is different than in the example provided\n",
    "configuration = k8s.Configuration()\n",
    "configuration.host = api_server\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "# configuration.verify_ssl = False\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "api_client = k8s.ApiClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219c28e-5d66-4fdb-b746-a7616336a50e",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Dataset\n",
    "\n",
    "We'll use the sql-create-context dataset from HuggingFace. This dataset contains:\n",
    "\n",
    "    Natural language questions\n",
    "    Database schema context (CREATE TABLE statements)\n",
    "    Corresponding SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d953abf-5777-4a2a-a6e2-541a2a405202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad43a62-ae35-4f07-8380-ec7998d8b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the format of the intial messages.\n",
    "def convert_to_messages(example):\n",
    "    \"\"\"\n",
    "    Convert a sql-create-context example to chat template format.\n",
    "\n",
    "    The user provides the database schema and question.\n",
    "    The assistant responds with the SQL query.\n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"Given the following database schema:\n",
    "\n",
    "{example[\"context\"]}\n",
    "\n",
    "Write a SQL query to answer this question: {example[\"question\"]}\"\"\"\n",
    "\n",
    "    assistant_message = example[\"answer\"]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_message},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# Show an example of the converted format\n",
    "sample_converted = convert_to_messages(dataset[0])\n",
    "print(\"Converted format:\")\n",
    "print(json.dumps(sample_converted, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940a0b6-4ab7-413e-96da-4c5d48acad53",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Data\n",
    "\n",
    "Training Hub expects data in the chat template format with a messages field containing the conversation. We'll convert each example into a user message (question + context) and an assistant message (SQL query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac0f3e-d897-43ba-9f3e-14369ba1bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset Preparation.\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"\\nDataset columns: {dataset.column_names}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Sample entry:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show a sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nQuestion: {sample['question']}\")\n",
    "print(f\"\\nContext (Schema):\\n{sample['context']}\")\n",
    "print(f\"\\nAnswer (SQL): {sample['answer']}\")\n",
    "\n",
    "print(f\"Original training set size: {len(dataset)}\")\n",
    "\n",
    "# For a quick demonstration, we'll use a subset of the data\n",
    "# You can increase this for better results (full dataset is ~78k examples)\n",
    "TRAIN_SIZE = 100  # Adjust based on your time/compute budget\n",
    "\n",
    "# Shuffle and select a subset\n",
    "train_dataset = dataset.shuffle(seed=42).select(range(min(TRAIN_SIZE, len(dataset))))\n",
    "\n",
    "# Convert to messages format\n",
    "train_data = [convert_to_messages(example) for example in train_dataset]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "\n",
    "# Save training data to JSONL format\n",
    "OUTPUT_DIR = Path(f\"{PVC_PATH}/lora_text_sql_output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_file = OUTPUT_DIR / \"train_data.jsonl\"\n",
    "\n",
    "with open(training_file, \"w\") as f:\n",
    "    for example in train_data:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "print(f\"Training data saved to: {training_file}\")\n",
    "print(f\"File size: {training_file.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "data_path = f\"{PVC_PATH}/lora_text_sql_output/train_data.jsonl\"\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de91052-984d-4b2b-b60e-8c6d3086b94c",
   "metadata": {},
   "source": [
    "## 3. Configure and Run LoRA Training\n",
    "\n",
    "Now we'll use Training Hub's lora_sft function to train the model. Key parameters:\n",
    "LoRA Parameters\n",
    "\n",
    "    lora_r: Rank of the low-rank matrices (higher = more capacity, more memory)\n",
    "    lora_alpha: Scaling factor (typically 2x the rank)\n",
    "    target_modules: Which layers to apply LoRA to\n",
    "\n",
    "QLoRA Parameters (Optional)\n",
    "\n",
    "    load_in_4bit: Enable 4-bit quantization to reduce memory\n",
    "    bnb_4bit_quant_type: Quantization type ('nf4' recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016ae6f-a01a-4714-a72b-9080acad89c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# You can also try these alternatives:\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"    # Larger, more capable\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Smaller, faster training\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Alternative architecture\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16  # Rank - start small, increase if needed\n",
    "LORA_ALPHA = 32  # Alpha - typically 2x rank\n",
    "LORA_DROPOUT = 0.0  # Dropout - 0.0 is optimized for Unsloth\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 2  # More epochs = better learning, longer training\n",
    "LEARNING_RATE = 2e-4  # Standard LoRA learning rate\n",
    "MAX_SEQ_LEN = 1024  # Maximum sequence length\n",
    "MICRO_BATCH_SIZE = 16  # Batch size per GPU (reduce if OOM)\n",
    "GRADIENT_ACCUMULATION = 4  # Effective batch = micro_batch * grad_accum\n",
    "\n",
    "# QLoRA settings (set to True to enable 4-bit quantization)\n",
    "USE_QLORA = True  # Set to True if you have limited GPU memory\n",
    "\n",
    "# Multi-GPU Configuration:\n",
    "enable_model_splitting = False\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA Rank: {LORA_R}\")\n",
    "print(f\"  LoRA Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective Batch Size: {MICRO_BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  QLoRA (4-bit): {USE_QLORA}\")\n",
    "print(f\"  ENABLE MODEL SPLITTING: {enable_model_splitting}\")\n",
    "\n",
    "params = {\n",
    "    # Model and data path\n",
    "    \"model_path\": MODEL_NAME,\n",
    "    \"data_path\": f\"/mnt/{data_path}\",\n",
    "    \"ckpt_output_dir\": f\"/mnt/{PVC_PATH}/checkpoints-logs-dir\",\n",
    "    \"data_output_path\": f\"/mnt/{PVC_PATH}/lora-json/_data\",\n",
    "    # Important for LORA\n",
    "    \"lr_scheduler\": \"cosine\",\n",
    "    \"warmup_steps\": 0,\n",
    "    \"seed\": 42,\n",
    "    # LoRA configuration\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "    # Training configuration\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"micro_batch_size\": MICRO_BATCH_SIZE,\n",
    "    \"max_seq_len\": MAX_SEQ_LEN,\n",
    "    \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION,\n",
    "    # Dataset format\n",
    "    \"dataset_type\": \"chat_template\",\n",
    "    \"field_messages\": \"messages\",\n",
    "    # Quantization\n",
    "    \"load_in_4bit\": USE_QLORA,\n",
    "    # GPU configuration\n",
    "    \"nproc_per_node\": 2,\n",
    "    \"nnodes\": 2,\n",
    "    # Logging\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 200,\n",
    "    \"save_total_limit\": 3,\n",
    "    # Multi-GPU Configuration\n",
    "    \"enable_model_splitting\": enable_model_splitting,\n",
    "    # Model Checkpointing\n",
    "    \"save_final_checkpoint\": True,\n",
    "    \"checkpoint_at_epoch\": 2,\n",
    "}\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302cfd9-42f2-4b94-b7c5-dbb4e4ac559b",
   "metadata": {},
   "source": [
    "## Training with LORA SFT and Kubeflow Trainer\n",
    " we launch a training job via Kubeflow Trainer with configured hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eace042-f662-4b36-9fb0-c6f543c4efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.common.types import KubernetesBackendConfig\n",
    "from kubeflow.trainer import TrainerClient\n",
    "from kubeflow.trainer.rhai import TrainingHubAlgorithms, TrainingHubTrainer\n",
    "\n",
    "backend_cfg = KubernetesBackendConfig(client_configuration=api_client.configuration)\n",
    "client = TrainerClient(backend_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb066eae-39ce-4c20-91e0-86aa7afde30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for runtime in client.list_runtimes():\n",
    "    if runtime.name == \"training-hub\":\n",
    "        th_runtime = runtime\n",
    "        print(\"Found runtime: \" + str(th_runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec647f-0c8b-48bb-9acc-69bda8315c1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from kubeflow.trainer.options.kubernetes import (\n",
    "    ContainerOverride,\n",
    "    PodSpecOverride,\n",
    "    PodTemplateOverride,\n",
    "    PodTemplateOverrides,\n",
    ")\n",
    "\n",
    "cache_root = f\"/mnt/{PVC_PATH}/.cache/huggingface\"\n",
    "\n",
    "job_name = client.train(\n",
    "    trainer=TrainingHubTrainer(\n",
    "        algorithm=TrainingHubAlgorithms.LORA_SFT,\n",
    "        func_args=params,\n",
    "        resources_per_node={\n",
    "            \"cpu\": 3,\n",
    "            \"memory\": \"16Gi\",\n",
    "            \"nvidia.com/gpu\": 2,\n",
    "        },\n",
    "    ),\n",
    "    options=[\n",
    "        PodTemplateOverrides(\n",
    "            PodTemplateOverride(\n",
    "                target_jobs=[\"node\"],\n",
    "                spec=PodSpecOverride(\n",
    "                    volumes=[\n",
    "                        {\n",
    "                            \"name\": \"work\",\n",
    "                            \"persistentVolumeClaim\": {\"claimName\": PVC_NAME},\n",
    "                        },\n",
    "                    ],\n",
    "                    containers=[\n",
    "                        ContainerOverride(\n",
    "                            name=\"node\",  # Target the existing container\n",
    "                            volume_mounts=[\n",
    "                                {\n",
    "                                    \"name\": \"work\",\n",
    "                                    \"mountPath\": f\"/mnt/{PVC_PATH}\",\n",
    "                                    \"readOnly\": False,\n",
    "                                },\n",
    "                            ],\n",
    "                        )\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    runtime=th_runtime,\n",
    ")\n",
    "\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbda75-8ed3-4872-b4c3-e7d495e7d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow job logs\n",
    "logs = client.get_job_logs(job_name, follow=True)\n",
    "for line in logs:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c663d1d-d87c-43d0-95e9-530307252fab",
   "metadata": {},
   "source": [
    "## Loading the Model from the Desired Checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73e162-3046-45a3-86d7-c7464c07d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_PATH = \"./shared/checkpoints-logs-dir/checkpoint-16\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CHECKPOINTS_PATH,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=USE_QLORA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c95b7-75e1-444c-8245-58ef4f043507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(question: str, schema: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate a SQL query from a natural language question.\n",
    "\n",
    "    Args:\n",
    "        question: Natural language question\n",
    "        schema: Database schema (CREATE TABLE statements)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        Generated SQL query\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Given the following database schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Write a SQL query to answer this question: {question}\"\"\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode response (only the new tokens)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44ae5a-521b-4335-bc29-e1cb915ee8fa",
   "metadata": {},
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "Let's load the trained model and test it on some SQL generation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60325a-4216-4bb6-a095-17923831b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with examples from the dataset\n",
    "test_examples = [\n",
    "    {\n",
    "        \"schema\": \"CREATE TABLE employees (id INT, name VARCHAR, department VARCHAR, salary DECIMAL, hire_date DATE)\",\n",
    "        \"question\": \"What is the average salary of employees in the engineering department?\",\n",
    "    },\n",
    "    {\n",
    "        \"schema\": \"CREATE TABLE orders (order_id INT, customer_id INT, product_name VARCHAR, quantity INT, order_date DATE)\",\n",
    "        \"question\": \"How many orders were placed in the last 30 days?\",\n",
    "    },\n",
    "    {\n",
    "        \"schema\": \"CREATE TABLE students (student_id INT, name VARCHAR, grade INT, subject VARCHAR, score DECIMAL)\",\n",
    "        \"question\": \"Find the top 5 students with the highest average score across all subjects.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Testing the trained model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Schema: {example['schema']}\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "\n",
    "    sql = generate_sql(example[\"question\"], example[\"schema\"])\n",
    "    print(f\"Generated SQL: {sql}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd68750-7e65-4a7b-ad11-80658ed68ba7",
   "metadata": {},
   "source": [
    "## Final Analysis and Summary\n",
    "In this notebook, we demonstrated how LORA/QLORA can be used fine tuning Qwen 2.5 1.5B Instruct model, \n",
    "we were able to fine tune the model to understand natural languages to sql queries generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
