{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA/QLoRA Fine-Tuning with Training Hub\n",
    "\n",
    "This notebook demonstrates how to use Training Hub's LoRA (Low-Rank Adaptation) and QLoRA capabilities for parameter-efficient fine-tuning. We'll train a model to convert natural language questions into SQL queries using the popular [sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that:\n",
    "- Freezes the pre-trained model weights\n",
    "- Injects trainable low-rank matrices into each layer\n",
    "- Reduces trainable parameters by ~10,000x compared to full fine-tuning\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "\n",
    "**QLoRA** extends LoRA by adding 4-bit quantization, further reducing memory requirements while maintaining quality.\n",
    "\n",
    "## Training Task: Natural Language to SQL\n",
    "\n",
    "We'll train the model to understand database schemas and generate SQL queries from natural language questions. For example:\n",
    "\n",
    "**Input:**\n",
    "```\n",
    "Table: employees (id, name, department, salary)\n",
    "Question: What is the average salary in the engineering department?\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```sql\n",
    "SELECT AVG(salary) FROM employees WHERE department = 'engineering'\n",
    "```\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "This notebook is designed to run on a single GPU:\n",
    "- **Minimum**: 16GB VRAM (with QLoRA 4-bit quantization)\n",
    "- **Recommended**: 24GB VRAM (for faster training with larger batch sizes)\n",
    "- Works on: A10, A100, L4, L40S, RTX 3090/4090, and similar GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies. Training Hub uses Unsloth for optimized LoRA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training-hub with LoRA dependencies\n",
    "# Note: Unsloth requires specific versions of dependencies\n",
    "!pip install 'training-hub[lora]'\n",
    "\n",
    "# Import required libraries\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Dataset\n",
    "\n",
    "We'll use the [sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset from HuggingFace. This dataset contains:\n",
    "- Natural language questions\n",
    "- Database schema context (CREATE TABLE statements)\n",
    "- Corresponding SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"\\nDataset columns: {dataset.column_names}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Sample entry:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show a sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nQuestion: {sample['question']}\")\n",
    "print(f\"\\nContext (Schema):\\n{sample['context']}\")\n",
    "print(f\"\\nAnswer (SQL): {sample['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Data\n",
    "\n",
    "Training Hub expects data in the **chat template format** with a `messages` field containing the conversation. We'll convert each example into a user message (question + context) and an assistant message (SQL query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_messages(example):\n",
    "    \"\"\"\n",
    "    Convert a sql-create-context example to chat template format.\n",
    "\n",
    "    The user provides the database schema and question.\n",
    "    The assistant responds with the SQL query.\n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"Given the following database schema:\n",
    "\n",
    "{example[\"context\"]}\n",
    "\n",
    "Write a SQL query to answer this question: {example[\"question\"]}\"\"\"\n",
    "\n",
    "    assistant_message = example[\"answer\"]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_message},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# Show an example of the converted format\n",
    "sample_converted = convert_to_messages(dataset[0])\n",
    "print(\"Converted format:\")\n",
    "print(json.dumps(sample_converted, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a quick demonstration, we'll use a subset of the data\n",
    "# You can increase this for better results (full dataset is ~78k examples)\n",
    "TRAIN_SIZE = 100  # Adjust based on your time/compute budget\n",
    "\n",
    "# Shuffle and select a subset\n",
    "train_dataset = dataset.shuffle(seed=42).select(range(min(TRAIN_SIZE, len(dataset))))\n",
    "\n",
    "# Convert to messages format\n",
    "train_data = [convert_to_messages(example) for example in train_dataset]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training data to JSONL format\n",
    "OUTPUT_DIR = Path(\"./lora_sql_output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_file = OUTPUT_DIR / \"train_data.jsonl\"\n",
    "\n",
    "with open(training_file, \"w\") as f:\n",
    "    for example in train_data:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "print(f\"Training data saved to: {training_file}\")\n",
    "print(f\"File size: {training_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure and Run LoRA Training\n",
    "\n",
    "Now we'll use Training Hub's `lora_sft` function to train the model. Key parameters:\n",
    "\n",
    "### LoRA Parameters\n",
    "- **lora_r**: Rank of the low-rank matrices (higher = more capacity, more memory)\n",
    "- **lora_alpha**: Scaling factor (typically 2x the rank)\n",
    "- **target_modules**: Which layers to apply LoRA to\n",
    "\n",
    "### QLoRA Parameters (Optional)\n",
    "- **load_in_4bit**: Enable 4-bit quantization to reduce memory\n",
    "- **bnb_4bit_quant_type**: Quantization type ('nf4' recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# Feel free to adjust these based on your GPU and requirements\n",
    "\n",
    "# Model selection - using Qwen2.5 1.5B as a good balance of capability and speed\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# You can also try these alternatives:\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"    # Larger, more capable\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Smaller, faster training\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Alternative architecture\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16  # Rank - start small, increase if needed\n",
    "LORA_ALPHA = 32  # Alpha - typically 2x rank\n",
    "LORA_DROPOUT = 0.0  # Dropout - 0.0 is optimized for Unsloth\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 1  # More epochs = better learning, longer training\n",
    "LEARNING_RATE = 2e-4  # Standard LoRA learning rate\n",
    "MAX_SEQ_LEN = 1024  # Maximum sequence length\n",
    "MICRO_BATCH_SIZE = 8  # Batch size per GPU (reduce if OOM)\n",
    "GRADIENT_ACCUMULATION = 4  # Effective batch = micro_batch * grad_accum\n",
    "\n",
    "# QLoRA settings (set to True to enable 4-bit quantization)\n",
    "USE_QLORA = False  # Set to True if you have limited GPU memory\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA Rank: {LORA_R}\")\n",
    "print(f\"  LoRA Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective Batch Size: {MICRO_BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  QLoRA (4-bit): {USE_QLORA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_hub import lora_sft\n",
    "\n",
    "# Run LoRA training with Training Hub\n",
    "checkpoint_dir = OUTPUT_DIR / \"checkpoints\"\n",
    "\n",
    "result = lora_sft(\n",
    "    # Required parameters\n",
    "    model_path=MODEL_NAME,\n",
    "    data_path=str(training_file),\n",
    "    ckpt_output_dir=str(checkpoint_dir),\n",
    "    # LoRA configuration\n",
    "    lora_r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    # QLoRA (4-bit quantization) - reduces memory significantly\n",
    "    load_in_4bit=USE_QLORA,\n",
    "    # Training parameters\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    micro_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    # Dataset format\n",
    "    dataset_type=\"chat_template\",\n",
    "    field_messages=\"messages\",\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Model saved to: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the Trained Model\n",
    "\n",
    "Let's load the trained model and test it on some SQL generation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# The model and tokenizer are returned in the result\n",
    "model = result[\"model\"]\n",
    "tokenizer = result[\"tokenizer\"]\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"Model ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(question: str, schema: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate a SQL query from a natural language question.\n",
    "\n",
    "    Args:\n",
    "        question: Natural language question\n",
    "        schema: Database schema (CREATE TABLE statements)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        Generated SQL query\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Given the following database schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Write a SQL query to answer this question: {question}\"\"\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode response (only the new tokens)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with examples from the dataset\n",
    "test_examples = [\n",
    "    {\n",
    "        \"schema\": \"CREATE TABLE employees (id INT, name VARCHAR, department VARCHAR, salary DECIMAL, hire_date DATE)\",\n",
    "        \"question\": \"What is the average salary of employees in the engineering department?\",\n",
    "    },\n",
    "    {\n",
    "        \"schema\": \"CREATE TABLE orders (order_id INT, customer_id INT, product_name VARCHAR, quantity INT, order_date DATE)\",\n",
    "        \"question\": \"How many orders were placed in the last 30 days?\",\n",
    "    },\n",
    "    {\n",
    "        \"schema\": \"CREATE TABLE students (student_id INT, name VARCHAR, grade INT, subject VARCHAR, score DECIMAL)\",\n",
    "        \"question\": \"Find the top 5 students with the highest average score across all subjects.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Testing the trained model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Schema: {example['schema']}\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "\n",
    "    sql = generate_sql(example[\"question\"], example[\"schema\"])\n",
    "    print(f\"Generated SQL: {sql}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save and Load the Model (Optional)\n",
    "\n",
    "If you want to use the model later, you can save it and reload it. The checkpoint directory already contains the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the saved files\n",
    "print(\"Saved model files:\")\n",
    "for file in sorted(checkpoint_dir.glob(\"*\")):\n",
    "    if file.is_file():\n",
    "        size_mb = file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {file.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reload the model later:\n",
    "# from unsloth import FastLanguageModel\n",
    "#\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=str(checkpoint_dir),\n",
    "#     max_seq_length=1024,\n",
    "#     load_in_4bit=False,\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Loaded** the sql-create-context dataset from HuggingFace\n",
    "2. **Prepared** the data in chat template format for Training Hub\n",
    "3. **Trained** a LoRA adapter using Training Hub's `lora_sft` function\n",
    "4. **Tested** the model on SQL generation examples\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **LoRA** enables efficient fine-tuning by only training a small number of parameters\n",
    "- **QLoRA** (4-bit quantization) can further reduce memory requirements\n",
    "- Training Hub handles all the complexity of setting up Unsloth, LoRA configs, and training\n",
    "- The chat template format makes it easy to prepare instruction-following data\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different models (larger = more capable, smaller = faster)\n",
    "- Increase training data size for better results\n",
    "- Experiment with LoRA rank (higher rank = more capacity)\n",
    "- Enable QLoRA for training on GPUs with limited memory\n",
    "- Add W&B logging for experiment tracking (`wandb_project=\"my-project\"`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
