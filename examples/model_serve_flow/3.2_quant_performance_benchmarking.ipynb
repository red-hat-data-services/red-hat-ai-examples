{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954c85aa-5c52-4da1-92fe-ca22d32dc60c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'guidellm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mguidellm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbenchmark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerativeBenchmarksReport\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'guidellm'"
     ]
    }
   ],
   "source": [
    "from guidellm.benchmark import GenerativeBenchmarksReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d68b1-dd85-4056-ac9a-752d04b49b54",
   "metadata": {},
   "source": [
    "## GuideLLM \n",
    "GuideLLM is an open source benchmarking tool designed to evaluate the performance of LLMs served through vLLM. It provides fine-grained metrics such as:\n",
    "\n",
    "Token throughput\n",
    "Latency (time-to-first-token, inter-token, request latency)\n",
    "Concurrency scaling\n",
    "Request-level diagnostics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ab78b-46b0-44d7-94ce-c947b97f7833",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "**target**: The URL of the vLLM model server to benchmark.\n",
    "\n",
    "**profile**: Defines the traffic pattern. Optons include:\n",
    "- ``synchronous``: Runs requests one at a time (sequential)\n",
    "- ``throughput``: Tests maximum throughput by running requests in parallel\n",
    "- ``concurrent``: Runs a fixed number of parallel request streams\n",
    "- ``constant``: Sends requests at a fixed rate per second\n",
    "- ``poisson``: Sends requests following a Poisson distribution\n",
    "- ``sweep``: Automatically determines optimal performance points (default)\n",
    "\n",
    "\n",
    "**rate**: GuideLLM supports multiple workload simulation modes, known as rate types (see full list). Each rate type determines which benchmarks are run. The example above uses sweep, which runs a series of benchmarks for 30 seconds each: first, a synchronous test that sends one request at a time (representing minimal traffic), then a throughput test where all requests are sent in parallel to identify the system's maximum RPS. Finally, it runs intermediate RPS levels to capture latency metrics across the full traffic spectrum.\n",
    "\n",
    "**data**: Specifies the dataset source. This can be a file path, Hugging Face dataset ID, synthetic data configuration, or in-memory data. In this case, we will be setting it to define a synthetic data configuration. \n",
    "Synthetic datasets allow you to generate data on the fly with customizable parameters. This is useful for controlled experiments, stress testing, and simulating specific scenarios. For example, you might want to evaluate how a model handles long prompts or generates outputs with specific characteristics. Data can be configured for different use cases like chat, RAG, code generation etc.\n",
    "Important config parameters:\n",
    "- ``prompt_tokens``: : Average number of tokens in prompts.\n",
    "- ``output_tokens``: Average number of tokens in outputs.\n",
    "- ``samples``: Number of samples to generate (default: 1000)\n",
    "- ``source``: Source text for generation (default: prideandprejudice.txt.gz). This can be any text file, URL containing a text file, or a compressed text file. The text is used to sample from at a word and punctuation granularity and then combined into a single string of the desired lengths.\n",
    "\n",
    "**rate**: The numeric rate value whose meaning depends on profile - for sweep it's the number of benchmarks, for concurrent it's simultaneous requests, for constant/poisson it's requests per second\n",
    "\n",
    "**max-seconds**: Maximum duration in seconds for each benchmark run (can also use **--max-requests** to limit by request count instead)\n",
    "\n",
    "\n",
    "**processor**: Specifies the tokenizer to use. This is only required for synthetic data generation or when local calculations are specified through configuration settings. By default, the processor is set to the --model argument. If --model is not supplied, it defaults to the model retrieved from the backend. The tokenizer is used to calculate the number of tokens to adjust the input length based on ``prompt_tokens``.  Using the modelâ€™s native tokenizer ensures the prompt token count matches what the model actually receives and the output token count reflects the true workload.\n",
    "\n",
    "**Note**: For synthetic data generation, a source file has to be provided which can be continuous text in a compatible format like txt. Input prompts (number can be specified using the ``source`` param) are then sampled from this file, with prompts having a length of ``prompt_tokens`` tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791188c-3ebc-47f3-b107-5aa8f8837d55",
   "metadata": {},
   "source": [
    "### Token Configuration for Different Use Cases\n",
    "\n",
    "GuideLLM allows to configure both **input (prompt) tokens** and **output (completion) tokens** depending on the workload to be evaluated.  \n",
    "Different use cases benefit from different token budgets, and these values can be fully adjusted based on the requirements.\n",
    "\n",
    "Below are example token configurations commonly used when benchmarking LLMs:\n",
    "\n",
    "#### Chat Use Case\n",
    "Chat-style interactions typically have *moderate* prompt length and *short to medium* responses.\n",
    "\n",
    "- **Input tokens:** ~1024  \n",
    "- **Output tokens:** ~512  \n",
    "- **Why:** Chat prompts are usually concise, and responses should be coherent but not excessively long.\n",
    "\n",
    "---\n",
    "\n",
    "#### RAG (Retrieval-Augmented Generation)\n",
    "RAG workloads include retrieved documents in the prompt, so input size is much larger while answers remain relatively short.\n",
    "\n",
    "- **Input tokens:** ~2,000  \n",
    "- **Output tokens:** ~500  \n",
    "- **Why:** Retrieved context contributes heavily to prompt length; outputs should stay grounded and precise.\n",
    "\n",
    "---\n",
    "\n",
    "#### Reasoning (e.g., long-form reasoning, code explanation, chain-of-thought tasks)\n",
    "Reasoning tasks often need short prompts but *longer* answers to capture detailed step-by-step reasoning.\n",
    "\n",
    "- **Input tokens:** ~300  \n",
    "- **Output tokens:** ~1,500  \n",
    "- **Why:** These tasks require extended reasoning or multi-step analysis, so the model needs more room in its output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50440d7d-599e-46d2-b477-2e5cb44e6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:8001\"\n",
    "prompt_tokens = 1024\n",
    "output_tokens = 512\n",
    "max_requests = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d15c4-8451-488b-833a-9958ba679b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarking compressed model\n",
    "# make sure the compressed model is running on a vLLM server\n",
    "!guidellm benchmark \\\n",
    "--target \"http://localhost:8001\" \\\n",
    "--rate-type sweep \\\n",
    "--max-requests 5 \\\n",
    "--max-seconds 5 \\\n",
    "--data \"prompt_tokens=100,output_tokens=100\" \\\n",
    "--output-path compressed_benchmarks.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427a672-1cfd-4206-b8bc-ac03bbd0bdc4",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "GuideLLM provides multiple metrics tat can be used to evaluate the performace of a LLM.\n",
    "\n",
    "These performance metrics include:\n",
    "\n",
    "- ``Request Rate`` (Requests Per Second): The number of requests processed per second.\n",
    "Indicates the throughput of the system and its ability to handle concurrent workloads.\n",
    "- ``Request Concurrency``: The number of requests being processed simultaneously.\n",
    "Helps evaluate the system's capacity to handle parallel workloads.\n",
    "\n",
    "- ``Output Tokens Per Second``\n",
    "The average number of output tokens generated per second as a throughput metric across all requests. Provides insights into the server's performance and efficiency in generating output tokens.\n",
    "- ``Total Tokens Per Second``: The combined rate of prompt and output tokens processed per second as a throughput metric across all requests.\n",
    "Provides insights into the server's overall performance and efficiency in processing both prompt and output tokens.\n",
    "\n",
    "- ``Request Latency``: The time taken to process a single request, from start to finish.\n",
    "A critical metric for evaluating the responsiveness of the system.\n",
    "\n",
    "- ``Time to First Token (TTFT)``: The time taken to generate the first token of the output.\n",
    "Indicates the initial response time of the model, which is crucial for user-facing applications.\n",
    "\n",
    "- ``Inter-Token Latency (ITL)``: The average time between generating consecutive tokens in the output, excluding the first token. Helps assess the smoothness and speed of token generation.\n",
    "  \n",
    "- ``Time Per Output Token``: The average time taken to generate each output token, including the first token. Provides a detailed view of the model's token generation efficiency.\n",
    "  \n",
    "- ``Statistical Summaries``\n",
    "GuideLLM provides detailed statistical summaries for each of the above metrics using the StatusDistributionSummary and DistributionSummary models. These summaries include the following statistics:\n",
    "\n",
    "    **Summary Statistics**\n",
    "    \n",
    "        - Mean: The average value of the metric.\n",
    "        - Median: The middle value of the metric when sorted.\n",
    "        - Mode: The most frequently occurring value of the metric.\n",
    "        - Variance: The measure of how much the values of the metric vary.\n",
    "        - Standard Deviation (Std Dev): The square root of the variance, indicating the   spread of the values.\n",
    "       - Min: The minimum value of the metric.\n",
    "       - Max: The maximum value of the metric.\n",
    "       - Count: The total number of data points for the metric.\n",
    "       - Total Sum: The sum of all values for the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0c53b-3f15-460a-a6ea-785e3eb92609",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = GenerativeBenchmarksReport.load_file(\n",
    "    path=\"compressed_benchmarks.json\",\n",
    ")\n",
    "base_benchmarks = report.benchmarks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
