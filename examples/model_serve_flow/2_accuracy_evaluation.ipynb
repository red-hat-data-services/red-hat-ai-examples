{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dec15d-d789-4e22-b9ea-5eec04bf9230",
   "metadata": {},
   "source": [
    "## Evaluate Accuracy of the Compressed Model\n",
    "After compression, it’s important to ensure the model maintains its generative capabilities. This step evaluates the compressed model on standard benchmarks.\n",
    "\n",
    "**Goal**: Verify that compression does not degrade model quality.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- We will create a function called **evaluate** that uses simple_evaluate from LM Eval to test the compressed model.\n",
    "\n",
    "- Benchmark on multiple datasets:\n",
    "\n",
    "    - MMLU: General knowledge across subjects.\n",
    "\n",
    "    - IFeval: Instruction-following tasks.\n",
    "\n",
    "    - ARC: Logical and scientific reasoning.\n",
    "    \n",
    "    - HellaSwag: Commonsense completion.\n",
    "\n",
    "- Collect metrics like accuracy, accuracy_norm, and task-specific scores.\n",
    "\n",
    "- Save results as JSON for later comparison.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Quantitative metrics for the compressed model.\n",
    "\n",
    "- Confidence that the model is ready for system-level performance benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1f45e6-3f29-4922-b85f-2b77be643530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lm_eval.utils import make_table\n",
    "import torch\n",
    "import json\n",
    "from typing import Union\n",
    "from utils import evaluate, extract_task_metrics, save_pickle, load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e43e6d-4fbe-4bd4-91ae-156b6a1c3af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfb397-d989-4a4b-8636-511a25c12ed6",
   "metadata": {},
   "source": [
    "## Define evaluation benchmarking datasets\n",
    "The following benchmark datasets can be used for evaluating on multiple tasks:\n",
    "- MMLU: General knowledge across 57 subjects\n",
    "- IFeval: Instruction-following capability\n",
    "- ARC: Logical & scientific reasoning\n",
    "- HellaSwag: Commonsense completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383befc1-e7b7-4540-af75-a3d5ce28786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tasks you want to evaluate the model on\n",
    "tasks = [\n",
    "    \"mmlu\",\n",
    "    \"arc_easy\",\n",
    "    \"hellaswag\",\n",
    "    \"ifeval\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddc52-4acd-4230-9681-f84201f19e33",
   "metadata": {},
   "source": [
    "### Evaluating the Compressed Model with `simple_evaluate`\n",
    "\n",
    "`simple_evaluate` is the **main entry point** in LM Evaluation Harness to evaluate a model across one or multiple benchmark datasets. It handles:\n",
    "\n",
    "1. Wrapping your model (or creating an LM object) to provide a **standardized interface**.\n",
    "2. Preparing inputs and optionally applying **few-shot examples** or **chat/instruction templates**.\n",
    "3. Running the model on benchmark tasks and collecting outputs.\n",
    "4. Computing **evaluation metrics** (accuracy, accuracy_norm, etc.) for each task.\n",
    "5. Returning a **results dictionary** that includes task-level metrics and model configuration info.\n",
    "\n",
    "We have wrapped **simple_evaluate** in a helper function **evaluate** which can be found in [utils.py](utils.py).\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "- **LM object**:  \n",
    "  LM Evaluation Harness wraps all models (Hugging Face, custom, or preloaded) in an `LM` object. This object provides a consistent interface (`loglikelihood`, `generate`, etc.) regardless of model backend.\n",
    "\n",
    "- **model_args**:  \n",
    "  Optional dictionary or string containing model-specific arguments (e.g., temperature, top-k, top-p). Ignored if passing a pre-wrapped LM object.\n",
    "\n",
    "- **apply_chat_template**:  \n",
    "  If your model is chat-based or instruction-following, this parameter allows you to prepend a prompt template to match the model's training format.  \n",
    "  \n",
    "**Parameters used here:**\n",
    "- `model`: Path or name of the model to evaluate (can be a string or an LM object).\n",
    "- `model_args`: Optional dictionary to provide model-specific arguments (e.g., batch size, device).\n",
    "- `tasks`: List of task names or objects to evaluate.\n",
    "- `num_fewshot`: Number of examples in the few-shot context (set to 0 for zero-shot).\n",
    "- `batch_size`: Number of samples to process per batch.\n",
    "- `device`: Device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
    "- `apply_chat_template`: Whether to wrap inputs in a chat-style template; useful for chat or instruction-tuned models.\n",
    "- `verbosity`: Set logging level; use `\"DEBUG\"` to inspect inputs/outputs for debugging. Default is None.\n",
    "- `log_samples`: Whether to log per-sample outputs for inspection.\n",
    "\n",
    "\n",
    "**NOTE**: Running the evaluation on the entire list of tasks can take long. So for testing, you can use a single task instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b759ca-440b-4491-abba-486fe2c1afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting directories\n",
    "compressed_model_path = \"./compressed_model\"\n",
    "compressed_results_dir = \"results/compressed_accuracy\"\n",
    "base_model_path = \"./base_model\"\n",
    "base_results_dir = \"results/base_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c849dd-535e-442c-82e9-d1fbeed500d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['run_compressed']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "Compressing model: 224it [00:00, 1212.52it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.19s/it]\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_abstract_algebra from None to 0\n",
      "Overwriting default num_fewshot of mmlu_anatomy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_astronomy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_biology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_chemistry from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_computer_science from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_mathematics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_physics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_computer_security from None to 0\n",
      "Overwriting default num_fewshot of mmlu_conceptual_physics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_electrical_engineering from None to 0\n",
      "Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_biology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_physics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_statistics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_machine_learning from None to 0\n",
      "Overwriting default num_fewshot of mmlu_business_ethics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_medicine from None to 0\n",
      "Overwriting default num_fewshot of mmlu_global_facts from None to 0\n",
      "Overwriting default num_fewshot of mmlu_human_aging from None to 0\n",
      "Overwriting default num_fewshot of mmlu_management from None to 0\n",
      "Overwriting default num_fewshot of mmlu_marketing from None to 0\n",
      "Overwriting default num_fewshot of mmlu_medical_genetics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_miscellaneous from None to 0\n",
      "Overwriting default num_fewshot of mmlu_nutrition from None to 0\n",
      "Overwriting default num_fewshot of mmlu_professional_accounting from None to 0\n",
      "Overwriting default num_fewshot of mmlu_professional_medicine from None to 0\n",
      "Overwriting default num_fewshot of mmlu_virology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_econometrics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_geography from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_psychology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_human_sexuality from None to 0\n",
      "Overwriting default num_fewshot of mmlu_professional_psychology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_public_relations from None to 0\n",
      "Overwriting default num_fewshot of mmlu_security_studies from None to 0\n",
      "Overwriting default num_fewshot of mmlu_sociology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_formal_logic from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_european_history from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_us_history from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_world_history from None to 0\n",
      "Overwriting default num_fewshot of mmlu_international_law from None to 0\n",
      "Overwriting default num_fewshot of mmlu_jurisprudence from None to 0\n",
      "Overwriting default num_fewshot of mmlu_logical_fallacies from None to 0\n",
      "Overwriting default num_fewshot of mmlu_moral_disputes from None to 0\n",
      "Overwriting default num_fewshot of mmlu_moral_scenarios from None to 0\n",
      "Overwriting default num_fewshot of mmlu_philosophy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_prehistory from None to 0\n",
      "Overwriting default num_fewshot of mmlu_professional_law from None to 0\n",
      "Overwriting default num_fewshot of mmlu_world_religions from None to 0\n",
      "Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "100%|██████████| 541/541 [00:00<00:00, 3778.23it/s]\n",
      "100%|██████████| 10042/10042 [00:07<00:00, 1266.45it/s]\n",
      "100%|██████████| 2376/2376 [00:04<00:00, 593.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 815.12it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 823.27it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 225.14it/s]\n",
      "100%|██████████| 144/144 [00:00<00:00, 824.19it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 823.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 822.75it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 821.71it/s]\n",
      "100%|██████████| 102/102 [00:00<00:00, 823.13it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 823.41it/s]\n",
      "100%|██████████| 235/235 [00:00<00:00, 828.46it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 832.53it/s]\n",
      "100%|██████████| 378/378 [00:00<00:00, 823.29it/s]\n",
      "100%|██████████| 310/310 [00:00<00:00, 825.51it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 822.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 814.33it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 825.74it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 827.75it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 830.43it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 822.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 823.81it/s]\n",
      "100%|██████████| 265/265 [00:00<00:00, 825.13it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 815.45it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 816.32it/s]\n",
      "100%|██████████| 223/223 [00:00<00:00, 820.54it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 817.89it/s]\n",
      "100%|██████████| 234/234 [00:00<00:00, 823.78it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 825.31it/s]\n",
      "100%|██████████| 783/783 [00:00<00:00, 823.08it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 826.03it/s]\n",
      "100%|██████████| 282/282 [00:00<00:00, 822.14it/s]\n",
      "100%|██████████| 272/272 [00:00<00:00, 827.28it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 831.01it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 826.96it/s]\n",
      "100%|██████████| 198/198 [00:00<00:00, 808.23it/s]\n",
      "100%|██████████| 193/193 [00:00<00:00, 823.67it/s]\n",
      "100%|██████████| 390/390 [00:00<00:00, 827.38it/s]\n",
      "100%|██████████| 238/238 [00:00<00:00, 830.22it/s]\n",
      "100%|██████████| 545/545 [00:00<00:00, 827.78it/s]\n",
      "100%|██████████| 131/131 [00:00<00:00, 823.79it/s]\n",
      "100%|██████████| 612/612 [00:00<00:00, 822.75it/s]\n",
      "100%|██████████| 110/110 [00:00<00:00, 825.00it/s]\n",
      "100%|██████████| 245/245 [00:00<00:00, 828.67it/s]\n",
      "100%|██████████| 201/201 [00:00<00:00, 827.49it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 819.85it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 825.57it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 824.92it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 823.78it/s]\n",
      "100%|██████████| 237/237 [00:00<00:00, 823.23it/s]\n",
      "100%|██████████| 121/121 [00:00<00:00, 821.67it/s]\n",
      "100%|██████████| 108/108 [00:00<00:00, 822.95it/s]\n",
      "100%|██████████| 163/163 [00:00<00:00, 826.82it/s]\n",
      "100%|██████████| 346/346 [00:00<00:00, 826.40it/s]\n",
      "100%|██████████| 895/895 [00:01<00:00, 824.68it/s]\n",
      "100%|██████████| 311/311 [00:00<00:00, 823.06it/s]\n",
      "100%|██████████| 324/324 [00:00<00:00, 831.56it/s]\n",
      "100%|██████████| 1534/1534 [00:01<00:00, 827.55it/s]\n",
      "100%|██████████| 171/171 [00:00<00:00, 828.15it/s]\n",
      "Running generate_until requests:   0%|          | 0/541 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests: 100%|██████████| 541/541 [3:09:08<00:00, 20.98s/it]  \n",
      "Running loglikelihood requests:  85%|████████▍ | 89748/105837 [24:53<06:25, 41.75it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the compressed model and save results in pkl format\n",
    "# comp_acc = evaluate(compressed_model_path, tasks, limit=None, batch_size=16, apply_chat_template=True, verbosity=None)\n",
    "# save_pickle(compressed_results_dir, comp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff8288-c452-4d04-89cc-ddf57339469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the base model and save results in pkl format\n",
    "base_acc = evaluate(base_model_path, tasks, limit=None, batch_size=16, apply_chat_template=True, verbosity=None)\n",
    "save_pickle(base_results_dir, base_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20e51ac-f4c3-4276-b2fe-df595a963443",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'arc_easy': {'alias': 'arc_easy',\n",
       "   'acc,none': 0.5067340067340067,\n",
       "   'acc_stderr,none': 0.010258852980991717,\n",
       "   'acc_norm,none': 0.4393939393939394,\n",
       "   'acc_norm_stderr,none': 0.010184134315437384},\n",
       "  'hellaswag': {'alias': 'hellaswag',\n",
       "   'acc,none': 0.44373630750846443,\n",
       "   'acc_stderr,none': 0.004958089432669621,\n",
       "   'acc_norm,none': 0.5502887870942044,\n",
       "   'acc_norm_stderr,none': 0.0049644793245523094},\n",
       "  'ifeval': {'alias': 'ifeval',\n",
       "   'prompt_level_strict_acc,none': 0.1367837338262477,\n",
       "   'prompt_level_strict_acc_stderr,none': 0.014787002800682885,\n",
       "   'inst_level_strict_acc,none': 0.23501199040767387,\n",
       "   'inst_level_strict_acc_stderr,none': 'N/A',\n",
       "   'prompt_level_loose_acc,none': 0.16081330868761554,\n",
       "   'prompt_level_loose_acc_stderr,none': 0.015808599888607115,\n",
       "   'inst_level_loose_acc,none': 0.2577937649880096,\n",
       "   'inst_level_loose_acc_stderr,none': 'N/A'},\n",
       "  'mmlu': {'acc,none': 0.2486113089303518,\n",
       "   'acc_stderr,none': 0.0036413387703834654,\n",
       "   'alias': 'mmlu'},\n",
       "  'mmlu_humanities': {'acc,none': 0.2565356004250797,\n",
       "   'acc_stderr,none': 0.006363035883646292,\n",
       "   'alias': ' - humanities'},\n",
       "  'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "   'acc,none': 0.1984126984126984,\n",
       "   'acc_stderr,none': 0.03567016675276862},\n",
       "  'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "   'acc,none': 0.24848484848484848,\n",
       "   'acc_stderr,none': 0.03374402644139407},\n",
       "  'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "   'acc,none': 0.2107843137254902,\n",
       "   'acc_stderr,none': 0.028626547912437423},\n",
       "  'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "   'acc,none': 0.27848101265822783,\n",
       "   'acc_stderr,none': 0.029178682304842548},\n",
       "  'mmlu_international_law': {'alias': '  - international_law',\n",
       "   'acc,none': 0.3140495867768595,\n",
       "   'acc_stderr,none': 0.042369647530410184},\n",
       "  'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "   'acc,none': 0.25925925925925924,\n",
       "   'acc_stderr,none': 0.042365112580946315},\n",
       "  'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "   'acc,none': 0.2822085889570552,\n",
       "   'acc_stderr,none': 0.035361178866647414},\n",
       "  'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "   'acc,none': 0.2543352601156069,\n",
       "   'acc_stderr,none': 0.02344582627654551},\n",
       "  'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "   'acc,none': 0.2424581005586592,\n",
       "   'acc_stderr,none': 0.01433352205921795},\n",
       "  'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "   'acc,none': 0.31189710610932475,\n",
       "   'acc_stderr,none': 0.026311858071854183},\n",
       "  'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "   'acc,none': 0.27469135802469136,\n",
       "   'acc_stderr,none': 0.024836057868294684},\n",
       "  'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "   'acc,none': 0.24445893089960888,\n",
       "   'acc_stderr,none': 0.010976425013113885},\n",
       "  'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "   'acc,none': 0.3157894736842105,\n",
       "   'acc_stderr,none': 0.03565079670708305},\n",
       "  'mmlu_other': {'acc,none': 0.2539427100096556,\n",
       "   'acc_stderr,none': 0.0077913269982332976,\n",
       "   'alias': ' - other'},\n",
       "  'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "   'acc,none': 0.15,\n",
       "   'acc_stderr,none': 0.035887028128263665},\n",
       "  'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "   'acc,none': 0.27169811320754716,\n",
       "   'acc_stderr,none': 0.027377706624670706},\n",
       "  'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "   'acc,none': 0.20809248554913296,\n",
       "   'acc_stderr,none': 0.030952890217749857},\n",
       "  'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "   'acc,none': 0.37,\n",
       "   'acc_stderr,none': 0.048523658709390974},\n",
       "  'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "   'acc,none': 0.27802690582959644,\n",
       "   'acc_stderr,none': 0.030069584874494053},\n",
       "  'mmlu_management': {'alias': '  - management',\n",
       "   'acc,none': 0.1941747572815534,\n",
       "   'acc_stderr,none': 0.03916667762822582},\n",
       "  'mmlu_marketing': {'alias': '  - marketing',\n",
       "   'acc,none': 0.2606837606837607,\n",
       "   'acc_stderr,none': 0.02876034895652342},\n",
       "  'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "   'acc,none': 0.2,\n",
       "   'acc_stderr,none': 0.04020151261036849},\n",
       "  'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "   'acc,none': 0.2656449553001277,\n",
       "   'acc_stderr,none': 0.015794302487888694},\n",
       "  'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "   'acc,none': 0.26143790849673204,\n",
       "   'acc_stderr,none': 0.025160998214292445},\n",
       "  'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "   'acc,none': 0.2872340425531915,\n",
       "   'acc_stderr,none': 0.026992199173064307},\n",
       "  'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "   'acc,none': 0.1948529411764706,\n",
       "   'acc_stderr,none': 0.024060599423487476},\n",
       "  'mmlu_virology': {'alias': '  - virology',\n",
       "   'acc,none': 0.26506024096385544,\n",
       "   'acc_stderr,none': 0.034360240379449694},\n",
       "  'mmlu_social_sciences': {'acc,none': 0.23269418264543387,\n",
       "   'acc_stderr,none': 0.007609674281095825,\n",
       "   'alias': ' - social sciences'},\n",
       "  'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "   'acc,none': 0.2719298245614035,\n",
       "   'acc_stderr,none': 0.04185774424022053},\n",
       "  'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "   'acc,none': 0.21717171717171718,\n",
       "   'acc_stderr,none': 0.02937661648494561},\n",
       "  'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "   'acc,none': 0.21243523316062177,\n",
       "   'acc_stderr,none': 0.02951928261681729},\n",
       "  'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "   'acc,none': 0.20256410256410257,\n",
       "   'acc_stderr,none': 0.020377660970371435},\n",
       "  'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "   'acc,none': 0.2647058823529412,\n",
       "   'acc_stderr,none': 0.028657491285071956},\n",
       "  'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "   'acc,none': 0.21834862385321102,\n",
       "   'acc_stderr,none': 0.01771260052872277},\n",
       "  'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "   'acc,none': 0.21374045801526717,\n",
       "   'acc_stderr,none': 0.03595461611774691},\n",
       "  'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "   'acc,none': 0.2761437908496732,\n",
       "   'acc_stderr,none': 0.018087276935663185},\n",
       "  'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "   'acc,none': 0.20909090909090908,\n",
       "   'acc_stderr,none': 0.03895091015724137},\n",
       "  'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "   'acc,none': 0.17551020408163265,\n",
       "   'acc_stderr,none': 0.02435280072297003},\n",
       "  'mmlu_sociology': {'alias': '  - sociology',\n",
       "   'acc,none': 0.24875621890547264,\n",
       "   'acc_stderr,none': 0.030567675938916686},\n",
       "  'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "   'acc,none': 0.27,\n",
       "   'acc_stderr,none': 0.04461960433384737},\n",
       "  'mmlu_stem': {'acc,none': 0.2470662860767523,\n",
       "   'acc_stderr,none': 0.007663561936909282,\n",
       "   'alias': ' - stem'},\n",
       "  'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "   'acc,none': 0.27,\n",
       "   'acc_stderr,none': 0.04461960433384737},\n",
       "  'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "   'acc,none': 0.34074074074074073,\n",
       "   'acc_stderr,none': 0.04094376269996792},\n",
       "  'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "   'acc,none': 0.18421052631578946,\n",
       "   'acc_stderr,none': 0.031546980450822305},\n",
       "  'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "   'acc,none': 0.2847222222222222,\n",
       "   'acc_stderr,none': 0.037738099906869334},\n",
       "  'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "   'acc,none': 0.19,\n",
       "   'acc_stderr,none': 0.039427724440366255},\n",
       "  'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "   'acc,none': 0.16,\n",
       "   'acc_stderr,none': 0.03684529491774706},\n",
       "  'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "   'acc,none': 0.23,\n",
       "   'acc_stderr,none': 0.04229525846816507},\n",
       "  'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "   'acc,none': 0.16666666666666666,\n",
       "   'acc_stderr,none': 0.0370828466241654},\n",
       "  'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "   'acc,none': 0.28,\n",
       "   'acc_stderr,none': 0.045126085985421296},\n",
       "  'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "   'acc,none': 0.2936170212765957,\n",
       "   'acc_stderr,none': 0.02977164271249127},\n",
       "  'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "   'acc,none': 0.2206896551724138,\n",
       "   'acc_stderr,none': 0.03455930201924806},\n",
       "  'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "   'acc,none': 0.2566137566137566,\n",
       "   'acc_stderr,none': 0.022494510767503178},\n",
       "  'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "   'acc,none': 0.26129032258064516,\n",
       "   'acc_stderr,none': 0.02499305339776474},\n",
       "  'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "   'acc,none': 0.28078817733990147,\n",
       "   'acc_stderr,none': 0.031618563353586135},\n",
       "  'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "   'acc,none': 0.22,\n",
       "   'acc_stderr,none': 0.041633319989322654},\n",
       "  'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "   'acc,none': 0.2740740740740741,\n",
       "   'acc_stderr,none': 0.027195934804085668},\n",
       "  'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "   'acc,none': 0.2119205298013245,\n",
       "   'acc_stderr,none': 0.033367670865679794},\n",
       "  'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "   'acc,none': 0.18518518518518517,\n",
       "   'acc_stderr,none': 0.02649191472735521},\n",
       "  'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "   'acc,none': 0.26785714285714285,\n",
       "   'acc_stderr,none': 0.04203277291467762}},\n",
       " 'groups': {'mmlu': {'acc,none': 0.2486113089303518,\n",
       "   'acc_stderr,none': 0.0036413387703834654,\n",
       "   'alias': 'mmlu'},\n",
       "  'mmlu_humanities': {'acc,none': 0.2565356004250797,\n",
       "   'acc_stderr,none': 0.006363035883646292,\n",
       "   'alias': ' - humanities'},\n",
       "  'mmlu_other': {'acc,none': 0.2539427100096556,\n",
       "   'acc_stderr,none': 0.0077913269982332976,\n",
       "   'alias': ' - other'},\n",
       "  'mmlu_social_sciences': {'acc,none': 0.23269418264543387,\n",
       "   'acc_stderr,none': 0.007609674281095825,\n",
       "   'alias': ' - social sciences'},\n",
       "  'mmlu_stem': {'acc,none': 0.2470662860767523,\n",
       "   'acc_stderr,none': 0.007663561936909282,\n",
       "   'alias': ' - stem'}},\n",
       " 'group_subtasks': {'mmlu_humanities': ['mmlu_formal_logic',\n",
       "   'mmlu_high_school_european_history',\n",
       "   'mmlu_high_school_us_history',\n",
       "   'mmlu_high_school_world_history',\n",
       "   'mmlu_international_law',\n",
       "   'mmlu_jurisprudence',\n",
       "   'mmlu_logical_fallacies',\n",
       "   'mmlu_moral_disputes',\n",
       "   'mmlu_moral_scenarios',\n",
       "   'mmlu_philosophy',\n",
       "   'mmlu_prehistory',\n",
       "   'mmlu_professional_law',\n",
       "   'mmlu_world_religions'],\n",
       "  'mmlu_social_sciences': ['mmlu_econometrics',\n",
       "   'mmlu_high_school_geography',\n",
       "   'mmlu_high_school_government_and_politics',\n",
       "   'mmlu_high_school_macroeconomics',\n",
       "   'mmlu_high_school_microeconomics',\n",
       "   'mmlu_high_school_psychology',\n",
       "   'mmlu_human_sexuality',\n",
       "   'mmlu_professional_psychology',\n",
       "   'mmlu_public_relations',\n",
       "   'mmlu_security_studies',\n",
       "   'mmlu_sociology',\n",
       "   'mmlu_us_foreign_policy'],\n",
       "  'mmlu_other': ['mmlu_business_ethics',\n",
       "   'mmlu_clinical_knowledge',\n",
       "   'mmlu_college_medicine',\n",
       "   'mmlu_global_facts',\n",
       "   'mmlu_human_aging',\n",
       "   'mmlu_management',\n",
       "   'mmlu_marketing',\n",
       "   'mmlu_medical_genetics',\n",
       "   'mmlu_miscellaneous',\n",
       "   'mmlu_nutrition',\n",
       "   'mmlu_professional_accounting',\n",
       "   'mmlu_professional_medicine',\n",
       "   'mmlu_virology'],\n",
       "  'mmlu_stem': ['mmlu_abstract_algebra',\n",
       "   'mmlu_anatomy',\n",
       "   'mmlu_astronomy',\n",
       "   'mmlu_college_biology',\n",
       "   'mmlu_college_chemistry',\n",
       "   'mmlu_college_computer_science',\n",
       "   'mmlu_college_mathematics',\n",
       "   'mmlu_college_physics',\n",
       "   'mmlu_computer_security',\n",
       "   'mmlu_conceptual_physics',\n",
       "   'mmlu_electrical_engineering',\n",
       "   'mmlu_elementary_mathematics',\n",
       "   'mmlu_high_school_biology',\n",
       "   'mmlu_high_school_chemistry',\n",
       "   'mmlu_high_school_computer_science',\n",
       "   'mmlu_high_school_mathematics',\n",
       "   'mmlu_high_school_physics',\n",
       "   'mmlu_high_school_statistics',\n",
       "   'mmlu_machine_learning'],\n",
       "  'mmlu': ['mmlu_stem',\n",
       "   'mmlu_other',\n",
       "   'mmlu_social_sciences',\n",
       "   'mmlu_humanities'],\n",
       "  'arc_easy': [],\n",
       "  'hellaswag': [],\n",
       "  'ifeval': []},\n",
       " 'configs': {'arc_easy': {'task': 'arc_easy',\n",
       "   'tag': ['ai2_arc'],\n",
       "   'dataset_path': 'allenai/ai2_arc',\n",
       "   'dataset_name': 'ARC-Easy',\n",
       "   'training_split': 'train',\n",
       "   'validation_split': 'validation',\n",
       "   'test_split': 'test',\n",
       "   'doc_to_text': 'Question: {{question}}\\nAnswer:',\n",
       "   'doc_to_target': '{{choices.label.index(answerKey)}}',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': '{{choices.text}}',\n",
       "   'description': '',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True},\n",
       "    {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': True,\n",
       "   'doc_to_decontamination_query': 'Question: {{question}}\\nAnswer:',\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'hellaswag': {'task': 'hellaswag',\n",
       "   'tag': ['multiple_choice'],\n",
       "   'dataset_path': 'Rowan/hellaswag',\n",
       "   'training_split': 'train',\n",
       "   'validation_split': 'validation',\n",
       "   'process_docs': 'def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\\n    def _process_doc(doc):\\n        ctx = doc[\"ctx_a\"] + \" \" + doc[\"ctx_b\"].capitalize()\\n        out_doc = {\\n            \"query\": preprocess(doc[\"activity_label\"] + \": \" + ctx),\\n            \"choices\": [preprocess(ending) for ending in doc[\"endings\"]],\\n            \"gold\": int(doc[\"label\"]),\\n        }\\n        return out_doc\\n\\n    return dataset.map(_process_doc)\\n',\n",
       "   'doc_to_text': '{{query}}',\n",
       "   'doc_to_target': '{{label}}',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': 'choices',\n",
       "   'description': '',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True},\n",
       "    {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'ifeval': {'task': 'ifeval',\n",
       "   'dataset_path': 'google/IFEval',\n",
       "   'test_split': 'train',\n",
       "   'doc_to_text': 'prompt',\n",
       "   'doc_to_target': 0,\n",
       "   'unsafe_code': False,\n",
       "   'process_results': 'def process_results(doc, results):\\n    inp = InputExample(\\n        key=doc[\"key\"],\\n        instruction_id_list=doc[\"instruction_id_list\"],\\n        prompt=doc[\"prompt\"],\\n        kwargs=doc[\"kwargs\"],\\n    )\\n    response = results[0]\\n\\n    out_strict = test_instruction_following_strict(inp, response)\\n    out_loose = test_instruction_following_loose(inp, response)\\n\\n    return {\\n        \"prompt_level_strict_acc\": out_strict.follow_all_instructions,\\n        \"inst_level_strict_acc\": out_strict.follow_instruction_list,\\n        \"prompt_level_loose_acc\": out_loose.follow_all_instructions,\\n        \"inst_level_loose_acc\": out_loose.follow_instruction_list,\\n    }\\n',\n",
       "   'description': '',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'prompt_level_strict_acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True},\n",
       "    {'metric': 'inst_level_strict_acc',\n",
       "     'aggregation': 'def agg_inst_level_acc(items):\\n    flat_items = [item for sublist in items for item in sublist]\\n    inst_level_acc = sum(flat_items) / len(flat_items)\\n    return inst_level_acc\\n',\n",
       "     'higher_is_better': True},\n",
       "    {'metric': 'prompt_level_loose_acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True},\n",
       "    {'metric': 'inst_level_loose_acc',\n",
       "     'aggregation': 'def agg_inst_level_acc(items):\\n    flat_items = [item for sublist in items for item in sublist]\\n    inst_level_acc = sum(flat_items) / len(flat_items)\\n    return inst_level_acc\\n',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'generate_until',\n",
       "   'generation_kwargs': {'until': [],\n",
       "    'do_sample': False,\n",
       "    'temperature': 0.0,\n",
       "    'max_gen_toks': 1280},\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 4.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_abstract_algebra': {'task': 'mmlu_abstract_algebra',\n",
       "   'task_alias': 'abstract_algebra',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'abstract_algebra',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about abstract algebra.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_anatomy': {'task': 'mmlu_anatomy',\n",
       "   'task_alias': 'anatomy',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'anatomy',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about anatomy.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_astronomy': {'task': 'mmlu_astronomy',\n",
       "   'task_alias': 'astronomy',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'astronomy',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about astronomy.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_business_ethics': {'task': 'mmlu_business_ethics',\n",
       "   'task_alias': 'business_ethics',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'business_ethics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about business ethics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_clinical_knowledge': {'task': 'mmlu_clinical_knowledge',\n",
       "   'task_alias': 'clinical_knowledge',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'clinical_knowledge',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about clinical knowledge.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_college_biology': {'task': 'mmlu_college_biology',\n",
       "   'task_alias': 'college_biology',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'college_biology',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about college biology.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_college_chemistry': {'task': 'mmlu_college_chemistry',\n",
       "   'task_alias': 'college_chemistry',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'college_chemistry',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about college chemistry.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_college_computer_science': {'task': 'mmlu_college_computer_science',\n",
       "   'task_alias': 'college_computer_science',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'college_computer_science',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about college computer science.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_college_mathematics': {'task': 'mmlu_college_mathematics',\n",
       "   'task_alias': 'college_mathematics',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'college_mathematics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about college mathematics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_college_medicine': {'task': 'mmlu_college_medicine',\n",
       "   'task_alias': 'college_medicine',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'college_medicine',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about college medicine.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_college_physics': {'task': 'mmlu_college_physics',\n",
       "   'task_alias': 'college_physics',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'college_physics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about college physics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_computer_security': {'task': 'mmlu_computer_security',\n",
       "   'task_alias': 'computer_security',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'computer_security',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about computer security.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_conceptual_physics': {'task': 'mmlu_conceptual_physics',\n",
       "   'task_alias': 'conceptual_physics',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'conceptual_physics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about conceptual physics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_econometrics': {'task': 'mmlu_econometrics',\n",
       "   'task_alias': 'econometrics',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'econometrics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about econometrics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_electrical_engineering': {'task': 'mmlu_electrical_engineering',\n",
       "   'task_alias': 'electrical_engineering',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'electrical_engineering',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about electrical engineering.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_elementary_mathematics': {'task': 'mmlu_elementary_mathematics',\n",
       "   'task_alias': 'elementary_mathematics',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'elementary_mathematics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about elementary mathematics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_formal_logic': {'task': 'mmlu_formal_logic',\n",
       "   'task_alias': 'formal_logic',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'formal_logic',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about formal logic.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_global_facts': {'task': 'mmlu_global_facts',\n",
       "   'task_alias': 'global_facts',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'global_facts',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about global facts.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_biology': {'task': 'mmlu_high_school_biology',\n",
       "   'task_alias': 'high_school_biology',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_biology',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school biology.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_chemistry': {'task': 'mmlu_high_school_chemistry',\n",
       "   'task_alias': 'high_school_chemistry',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_chemistry',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school chemistry.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_computer_science': {'task': 'mmlu_high_school_computer_science',\n",
       "   'task_alias': 'high_school_computer_science',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_computer_science',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school computer science.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_european_history': {'task': 'mmlu_high_school_european_history',\n",
       "   'task_alias': 'high_school_european_history',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_european_history',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school european history.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_geography': {'task': 'mmlu_high_school_geography',\n",
       "   'task_alias': 'high_school_geography',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_geography',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school geography.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_government_and_politics': {'task': 'mmlu_high_school_government_and_politics',\n",
       "   'task_alias': 'high_school_government_and_politics',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_government_and_politics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school government and politics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_macroeconomics': {'task': 'mmlu_high_school_macroeconomics',\n",
       "   'task_alias': 'high_school_macroeconomics',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_macroeconomics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school macroeconomics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_mathematics': {'task': 'mmlu_high_school_mathematics',\n",
       "   'task_alias': 'high_school_mathematics',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_mathematics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school mathematics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_microeconomics': {'task': 'mmlu_high_school_microeconomics',\n",
       "   'task_alias': 'high_school_microeconomics',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_microeconomics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school microeconomics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_physics': {'task': 'mmlu_high_school_physics',\n",
       "   'task_alias': 'high_school_physics',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_physics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school physics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_psychology': {'task': 'mmlu_high_school_psychology',\n",
       "   'task_alias': 'high_school_psychology',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_psychology',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school psychology.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_statistics': {'task': 'mmlu_high_school_statistics',\n",
       "   'task_alias': 'high_school_statistics',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_statistics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school statistics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_us_history': {'task': 'mmlu_high_school_us_history',\n",
       "   'task_alias': 'high_school_us_history',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_us_history',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school us history.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_high_school_world_history': {'task': 'mmlu_high_school_world_history',\n",
       "   'task_alias': 'high_school_world_history',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'high_school_world_history',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about high school world history.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_human_aging': {'task': 'mmlu_human_aging',\n",
       "   'task_alias': 'human_aging',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'human_aging',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about human aging.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_human_sexuality': {'task': 'mmlu_human_sexuality',\n",
       "   'task_alias': 'human_sexuality',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'human_sexuality',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about human sexuality.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_international_law': {'task': 'mmlu_international_law',\n",
       "   'task_alias': 'international_law',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'international_law',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about international law.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_jurisprudence': {'task': 'mmlu_jurisprudence',\n",
       "   'task_alias': 'jurisprudence',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'jurisprudence',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about jurisprudence.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_logical_fallacies': {'task': 'mmlu_logical_fallacies',\n",
       "   'task_alias': 'logical_fallacies',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'logical_fallacies',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about logical fallacies.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_machine_learning': {'task': 'mmlu_machine_learning',\n",
       "   'task_alias': 'machine_learning',\n",
       "   'tag': 'mmlu_stem_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'machine_learning',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about machine learning.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_management': {'task': 'mmlu_management',\n",
       "   'task_alias': 'management',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'management',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about management.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_marketing': {'task': 'mmlu_marketing',\n",
       "   'task_alias': 'marketing',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'marketing',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about marketing.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_medical_genetics': {'task': 'mmlu_medical_genetics',\n",
       "   'task_alias': 'medical_genetics',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'medical_genetics',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about medical genetics.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_miscellaneous': {'task': 'mmlu_miscellaneous',\n",
       "   'task_alias': 'miscellaneous',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'miscellaneous',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about miscellaneous.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_moral_disputes': {'task': 'mmlu_moral_disputes',\n",
       "   'task_alias': 'moral_disputes',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'moral_disputes',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about moral disputes.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_moral_scenarios': {'task': 'mmlu_moral_scenarios',\n",
       "   'task_alias': 'moral_scenarios',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'moral_scenarios',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about moral scenarios.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_nutrition': {'task': 'mmlu_nutrition',\n",
       "   'task_alias': 'nutrition',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'nutrition',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about nutrition.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_philosophy': {'task': 'mmlu_philosophy',\n",
       "   'task_alias': 'philosophy',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'philosophy',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about philosophy.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_prehistory': {'task': 'mmlu_prehistory',\n",
       "   'task_alias': 'prehistory',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'prehistory',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about prehistory.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_professional_accounting': {'task': 'mmlu_professional_accounting',\n",
       "   'task_alias': 'professional_accounting',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'professional_accounting',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about professional accounting.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_professional_law': {'task': 'mmlu_professional_law',\n",
       "   'task_alias': 'professional_law',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'professional_law',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about professional law.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_professional_medicine': {'task': 'mmlu_professional_medicine',\n",
       "   'task_alias': 'professional_medicine',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'professional_medicine',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about professional medicine.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_professional_psychology': {'task': 'mmlu_professional_psychology',\n",
       "   'task_alias': 'professional_psychology',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'professional_psychology',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about professional psychology.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_public_relations': {'task': 'mmlu_public_relations',\n",
       "   'task_alias': 'public_relations',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'public_relations',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about public relations.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_security_studies': {'task': 'mmlu_security_studies',\n",
       "   'task_alias': 'security_studies',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'security_studies',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about security studies.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_sociology': {'task': 'mmlu_sociology',\n",
       "   'task_alias': 'sociology',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'sociology',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about sociology.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_us_foreign_policy': {'task': 'mmlu_us_foreign_policy',\n",
       "   'task_alias': 'us_foreign_policy',\n",
       "   'tag': 'mmlu_social_sciences_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'us_foreign_policy',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about us foreign policy.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_virology': {'task': 'mmlu_virology',\n",
       "   'task_alias': 'virology',\n",
       "   'tag': 'mmlu_other_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'virology',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about virology.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}},\n",
       "  'mmlu_world_religions': {'task': 'mmlu_world_religions',\n",
       "   'task_alias': 'world_religions',\n",
       "   'tag': 'mmlu_humanities_tasks',\n",
       "   'dataset_path': 'cais/mmlu',\n",
       "   'dataset_name': 'world_religions',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'dev',\n",
       "   'doc_to_text': '{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:',\n",
       "   'doc_to_target': 'answer',\n",
       "   'unsafe_code': False,\n",
       "   'doc_to_choice': ['A', 'B', 'C', 'D'],\n",
       "   'description': 'The following are multiple choice questions (with answers) about world religions.\\n\\n',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'fewshot_config': {'sampler': 'first_n'},\n",
       "   'num_fewshot': 0,\n",
       "   'metric_list': [{'metric': 'acc',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True}],\n",
       "   'output_type': 'multiple_choice',\n",
       "   'repeats': 1,\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 1.0, 'pretrained': './base_model'}}},\n",
       " 'versions': {'arc_easy': 1.0,\n",
       "  'hellaswag': 1.0,\n",
       "  'ifeval': 4.0,\n",
       "  'mmlu': 2,\n",
       "  'mmlu_abstract_algebra': 1.0,\n",
       "  'mmlu_anatomy': 1.0,\n",
       "  'mmlu_astronomy': 1.0,\n",
       "  'mmlu_business_ethics': 1.0,\n",
       "  'mmlu_clinical_knowledge': 1.0,\n",
       "  'mmlu_college_biology': 1.0,\n",
       "  'mmlu_college_chemistry': 1.0,\n",
       "  'mmlu_college_computer_science': 1.0,\n",
       "  'mmlu_college_mathematics': 1.0,\n",
       "  'mmlu_college_medicine': 1.0,\n",
       "  'mmlu_college_physics': 1.0,\n",
       "  'mmlu_computer_security': 1.0,\n",
       "  'mmlu_conceptual_physics': 1.0,\n",
       "  'mmlu_econometrics': 1.0,\n",
       "  'mmlu_electrical_engineering': 1.0,\n",
       "  'mmlu_elementary_mathematics': 1.0,\n",
       "  'mmlu_formal_logic': 1.0,\n",
       "  'mmlu_global_facts': 1.0,\n",
       "  'mmlu_high_school_biology': 1.0,\n",
       "  'mmlu_high_school_chemistry': 1.0,\n",
       "  'mmlu_high_school_computer_science': 1.0,\n",
       "  'mmlu_high_school_european_history': 1.0,\n",
       "  'mmlu_high_school_geography': 1.0,\n",
       "  'mmlu_high_school_government_and_politics': 1.0,\n",
       "  'mmlu_high_school_macroeconomics': 1.0,\n",
       "  'mmlu_high_school_mathematics': 1.0,\n",
       "  'mmlu_high_school_microeconomics': 1.0,\n",
       "  'mmlu_high_school_physics': 1.0,\n",
       "  'mmlu_high_school_psychology': 1.0,\n",
       "  'mmlu_high_school_statistics': 1.0,\n",
       "  'mmlu_high_school_us_history': 1.0,\n",
       "  'mmlu_high_school_world_history': 1.0,\n",
       "  'mmlu_human_aging': 1.0,\n",
       "  'mmlu_human_sexuality': 1.0,\n",
       "  'mmlu_humanities': 2,\n",
       "  'mmlu_international_law': 1.0,\n",
       "  'mmlu_jurisprudence': 1.0,\n",
       "  'mmlu_logical_fallacies': 1.0,\n",
       "  'mmlu_machine_learning': 1.0,\n",
       "  'mmlu_management': 1.0,\n",
       "  'mmlu_marketing': 1.0,\n",
       "  'mmlu_medical_genetics': 1.0,\n",
       "  'mmlu_miscellaneous': 1.0,\n",
       "  'mmlu_moral_disputes': 1.0,\n",
       "  'mmlu_moral_scenarios': 1.0,\n",
       "  'mmlu_nutrition': 1.0,\n",
       "  'mmlu_other': 2,\n",
       "  'mmlu_philosophy': 1.0,\n",
       "  'mmlu_prehistory': 1.0,\n",
       "  'mmlu_professional_accounting': 1.0,\n",
       "  'mmlu_professional_law': 1.0,\n",
       "  'mmlu_professional_medicine': 1.0,\n",
       "  'mmlu_professional_psychology': 1.0,\n",
       "  'mmlu_public_relations': 1.0,\n",
       "  'mmlu_security_studies': 1.0,\n",
       "  'mmlu_social_sciences': 2,\n",
       "  'mmlu_sociology': 1.0,\n",
       "  'mmlu_stem': 2,\n",
       "  'mmlu_us_foreign_policy': 1.0,\n",
       "  'mmlu_virology': 1.0,\n",
       "  'mmlu_world_religions': 1.0},\n",
       " 'n-shot': {'arc_easy': 0,\n",
       "  'hellaswag': 0,\n",
       "  'ifeval': 0,\n",
       "  'mmlu_abstract_algebra': 0,\n",
       "  'mmlu_anatomy': 0,\n",
       "  'mmlu_astronomy': 0,\n",
       "  'mmlu_business_ethics': 0,\n",
       "  'mmlu_clinical_knowledge': 0,\n",
       "  'mmlu_college_biology': 0,\n",
       "  'mmlu_college_chemistry': 0,\n",
       "  'mmlu_college_computer_science': 0,\n",
       "  'mmlu_college_mathematics': 0,\n",
       "  'mmlu_college_medicine': 0,\n",
       "  'mmlu_college_physics': 0,\n",
       "  'mmlu_computer_security': 0,\n",
       "  'mmlu_conceptual_physics': 0,\n",
       "  'mmlu_econometrics': 0,\n",
       "  'mmlu_electrical_engineering': 0,\n",
       "  'mmlu_elementary_mathematics': 0,\n",
       "  'mmlu_formal_logic': 0,\n",
       "  'mmlu_global_facts': 0,\n",
       "  'mmlu_high_school_biology': 0,\n",
       "  'mmlu_high_school_chemistry': 0,\n",
       "  'mmlu_high_school_computer_science': 0,\n",
       "  'mmlu_high_school_european_history': 0,\n",
       "  'mmlu_high_school_geography': 0,\n",
       "  'mmlu_high_school_government_and_politics': 0,\n",
       "  'mmlu_high_school_macroeconomics': 0,\n",
       "  'mmlu_high_school_mathematics': 0,\n",
       "  'mmlu_high_school_microeconomics': 0,\n",
       "  'mmlu_high_school_physics': 0,\n",
       "  'mmlu_high_school_psychology': 0,\n",
       "  'mmlu_high_school_statistics': 0,\n",
       "  'mmlu_high_school_us_history': 0,\n",
       "  'mmlu_high_school_world_history': 0,\n",
       "  'mmlu_human_aging': 0,\n",
       "  'mmlu_human_sexuality': 0,\n",
       "  'mmlu_international_law': 0,\n",
       "  'mmlu_jurisprudence': 0,\n",
       "  'mmlu_logical_fallacies': 0,\n",
       "  'mmlu_machine_learning': 0,\n",
       "  'mmlu_management': 0,\n",
       "  'mmlu_marketing': 0,\n",
       "  'mmlu_medical_genetics': 0,\n",
       "  'mmlu_miscellaneous': 0,\n",
       "  'mmlu_moral_disputes': 0,\n",
       "  'mmlu_moral_scenarios': 0,\n",
       "  'mmlu_nutrition': 0,\n",
       "  'mmlu_philosophy': 0,\n",
       "  'mmlu_prehistory': 0,\n",
       "  'mmlu_professional_accounting': 0,\n",
       "  'mmlu_professional_law': 0,\n",
       "  'mmlu_professional_medicine': 0,\n",
       "  'mmlu_professional_psychology': 0,\n",
       "  'mmlu_public_relations': 0,\n",
       "  'mmlu_security_studies': 0,\n",
       "  'mmlu_sociology': 0,\n",
       "  'mmlu_us_foreign_policy': 0,\n",
       "  'mmlu_virology': 0,\n",
       "  'mmlu_world_religions': 0},\n",
       " 'higher_is_better': {'arc_easy': {'acc': True, 'acc_norm': True},\n",
       "  'hellaswag': {'acc': True, 'acc_norm': True},\n",
       "  'ifeval': {'prompt_level_strict_acc': True,\n",
       "   'inst_level_strict_acc': True,\n",
       "   'prompt_level_loose_acc': True,\n",
       "   'inst_level_loose_acc': True},\n",
       "  'mmlu': {'acc': True},\n",
       "  'mmlu_abstract_algebra': {'acc': True},\n",
       "  'mmlu_anatomy': {'acc': True},\n",
       "  'mmlu_astronomy': {'acc': True},\n",
       "  'mmlu_business_ethics': {'acc': True},\n",
       "  'mmlu_clinical_knowledge': {'acc': True},\n",
       "  'mmlu_college_biology': {'acc': True},\n",
       "  'mmlu_college_chemistry': {'acc': True},\n",
       "  'mmlu_college_computer_science': {'acc': True},\n",
       "  'mmlu_college_mathematics': {'acc': True},\n",
       "  'mmlu_college_medicine': {'acc': True},\n",
       "  'mmlu_college_physics': {'acc': True},\n",
       "  'mmlu_computer_security': {'acc': True},\n",
       "  'mmlu_conceptual_physics': {'acc': True},\n",
       "  'mmlu_econometrics': {'acc': True},\n",
       "  'mmlu_electrical_engineering': {'acc': True},\n",
       "  'mmlu_elementary_mathematics': {'acc': True},\n",
       "  'mmlu_formal_logic': {'acc': True},\n",
       "  'mmlu_global_facts': {'acc': True},\n",
       "  'mmlu_high_school_biology': {'acc': True},\n",
       "  'mmlu_high_school_chemistry': {'acc': True},\n",
       "  'mmlu_high_school_computer_science': {'acc': True},\n",
       "  'mmlu_high_school_european_history': {'acc': True},\n",
       "  'mmlu_high_school_geography': {'acc': True},\n",
       "  'mmlu_high_school_government_and_politics': {'acc': True},\n",
       "  'mmlu_high_school_macroeconomics': {'acc': True},\n",
       "  'mmlu_high_school_mathematics': {'acc': True},\n",
       "  'mmlu_high_school_microeconomics': {'acc': True},\n",
       "  'mmlu_high_school_physics': {'acc': True},\n",
       "  'mmlu_high_school_psychology': {'acc': True},\n",
       "  'mmlu_high_school_statistics': {'acc': True},\n",
       "  'mmlu_high_school_us_history': {'acc': True},\n",
       "  'mmlu_high_school_world_history': {'acc': True},\n",
       "  'mmlu_human_aging': {'acc': True},\n",
       "  'mmlu_human_sexuality': {'acc': True},\n",
       "  'mmlu_humanities': {'acc': True},\n",
       "  'mmlu_international_law': {'acc': True},\n",
       "  'mmlu_jurisprudence': {'acc': True},\n",
       "  'mmlu_logical_fallacies': {'acc': True},\n",
       "  'mmlu_machine_learning': {'acc': True},\n",
       "  'mmlu_management': {'acc': True},\n",
       "  'mmlu_marketing': {'acc': True},\n",
       "  'mmlu_medical_genetics': {'acc': True},\n",
       "  'mmlu_miscellaneous': {'acc': True},\n",
       "  'mmlu_moral_disputes': {'acc': True},\n",
       "  'mmlu_moral_scenarios': {'acc': True},\n",
       "  'mmlu_nutrition': {'acc': True},\n",
       "  'mmlu_other': {'acc': True},\n",
       "  'mmlu_philosophy': {'acc': True},\n",
       "  'mmlu_prehistory': {'acc': True},\n",
       "  'mmlu_professional_accounting': {'acc': True},\n",
       "  'mmlu_professional_law': {'acc': True},\n",
       "  'mmlu_professional_medicine': {'acc': True},\n",
       "  'mmlu_professional_psychology': {'acc': True},\n",
       "  'mmlu_public_relations': {'acc': True},\n",
       "  'mmlu_security_studies': {'acc': True},\n",
       "  'mmlu_social_sciences': {'acc': True},\n",
       "  'mmlu_sociology': {'acc': True},\n",
       "  'mmlu_stem': {'acc': True},\n",
       "  'mmlu_us_foreign_policy': {'acc': True},\n",
       "  'mmlu_virology': {'acc': True},\n",
       "  'mmlu_world_religions': {'acc': True}},\n",
       " 'n-samples': {'ifeval': {'original': 541, 'effective': 541},\n",
       "  'hellaswag': {'original': 10042, 'effective': 10042},\n",
       "  'arc_easy': {'original': 2376, 'effective': 2376},\n",
       "  'mmlu_abstract_algebra': {'original': 100, 'effective': 100},\n",
       "  'mmlu_anatomy': {'original': 135, 'effective': 135},\n",
       "  'mmlu_astronomy': {'original': 152, 'effective': 152},\n",
       "  'mmlu_college_biology': {'original': 144, 'effective': 144},\n",
       "  'mmlu_college_chemistry': {'original': 100, 'effective': 100},\n",
       "  'mmlu_college_computer_science': {'original': 100, 'effective': 100},\n",
       "  'mmlu_college_mathematics': {'original': 100, 'effective': 100},\n",
       "  'mmlu_college_physics': {'original': 102, 'effective': 102},\n",
       "  'mmlu_computer_security': {'original': 100, 'effective': 100},\n",
       "  'mmlu_conceptual_physics': {'original': 235, 'effective': 235},\n",
       "  'mmlu_electrical_engineering': {'original': 145, 'effective': 145},\n",
       "  'mmlu_elementary_mathematics': {'original': 378, 'effective': 378},\n",
       "  'mmlu_high_school_biology': {'original': 310, 'effective': 310},\n",
       "  'mmlu_high_school_chemistry': {'original': 203, 'effective': 203},\n",
       "  'mmlu_high_school_computer_science': {'original': 100, 'effective': 100},\n",
       "  'mmlu_high_school_mathematics': {'original': 270, 'effective': 270},\n",
       "  'mmlu_high_school_physics': {'original': 151, 'effective': 151},\n",
       "  'mmlu_high_school_statistics': {'original': 216, 'effective': 216},\n",
       "  'mmlu_machine_learning': {'original': 112, 'effective': 112},\n",
       "  'mmlu_business_ethics': {'original': 100, 'effective': 100},\n",
       "  'mmlu_clinical_knowledge': {'original': 265, 'effective': 265},\n",
       "  'mmlu_college_medicine': {'original': 173, 'effective': 173},\n",
       "  'mmlu_global_facts': {'original': 100, 'effective': 100},\n",
       "  'mmlu_human_aging': {'original': 223, 'effective': 223},\n",
       "  'mmlu_management': {'original': 103, 'effective': 103},\n",
       "  'mmlu_marketing': {'original': 234, 'effective': 234},\n",
       "  'mmlu_medical_genetics': {'original': 100, 'effective': 100},\n",
       "  'mmlu_miscellaneous': {'original': 783, 'effective': 783},\n",
       "  'mmlu_nutrition': {'original': 306, 'effective': 306},\n",
       "  'mmlu_professional_accounting': {'original': 282, 'effective': 282},\n",
       "  'mmlu_professional_medicine': {'original': 272, 'effective': 272},\n",
       "  'mmlu_virology': {'original': 166, 'effective': 166},\n",
       "  'mmlu_econometrics': {'original': 114, 'effective': 114},\n",
       "  'mmlu_high_school_geography': {'original': 198, 'effective': 198},\n",
       "  'mmlu_high_school_government_and_politics': {'original': 193,\n",
       "   'effective': 193},\n",
       "  'mmlu_high_school_macroeconomics': {'original': 390, 'effective': 390},\n",
       "  'mmlu_high_school_microeconomics': {'original': 238, 'effective': 238},\n",
       "  'mmlu_high_school_psychology': {'original': 545, 'effective': 545},\n",
       "  'mmlu_human_sexuality': {'original': 131, 'effective': 131},\n",
       "  'mmlu_professional_psychology': {'original': 612, 'effective': 612},\n",
       "  'mmlu_public_relations': {'original': 110, 'effective': 110},\n",
       "  'mmlu_security_studies': {'original': 245, 'effective': 245},\n",
       "  'mmlu_sociology': {'original': 201, 'effective': 201},\n",
       "  'mmlu_us_foreign_policy': {'original': 100, 'effective': 100},\n",
       "  'mmlu_formal_logic': {'original': 126, 'effective': 126},\n",
       "  'mmlu_high_school_european_history': {'original': 165, 'effective': 165},\n",
       "  'mmlu_high_school_us_history': {'original': 204, 'effective': 204},\n",
       "  'mmlu_high_school_world_history': {'original': 237, 'effective': 237},\n",
       "  'mmlu_international_law': {'original': 121, 'effective': 121},\n",
       "  'mmlu_jurisprudence': {'original': 108, 'effective': 108},\n",
       "  'mmlu_logical_fallacies': {'original': 163, 'effective': 163},\n",
       "  'mmlu_moral_disputes': {'original': 346, 'effective': 346},\n",
       "  'mmlu_moral_scenarios': {'original': 895, 'effective': 895},\n",
       "  'mmlu_philosophy': {'original': 311, 'effective': 311},\n",
       "  'mmlu_prehistory': {'original': 324, 'effective': 324},\n",
       "  'mmlu_professional_law': {'original': 1534, 'effective': 1534},\n",
       "  'mmlu_world_religions': {'original': 171, 'effective': 171}},\n",
       " 'config': {'model': 'hf',\n",
       "  'model_args': {'pretrained': './base_model'},\n",
       "  'model_num_parameters': 1100048384,\n",
       "  'model_dtype': torch.float32,\n",
       "  'model_revision': 'main',\n",
       "  'model_sha': '',\n",
       "  'batch_size': 16,\n",
       "  'batch_sizes': [],\n",
       "  'device': 'cuda',\n",
       "  'use_cache': None,\n",
       "  'limit': None,\n",
       "  'bootstrap_iters': 100000,\n",
       "  'gen_kwargs': None,\n",
       "  'random_seed': 0,\n",
       "  'numpy_seed': 1234,\n",
       "  'torch_seed': 1234,\n",
       "  'fewshot_seed': 1234},\n",
       " 'git_hash': None,\n",
       " 'date': 1764693212.8134558,\n",
       " 'pretty_env_info': 'PyTorch version: 2.8.0+cu128\\nIs debug build: False\\nCUDA used to build PyTorch: 12.8\\nROCM used to build PyTorch: N/A\\n\\nOS: Red Hat Enterprise Linux 9.6 (Plow) (x86_64)\\nGCC version: (GCC) 11.5.0 20240719 (Red Hat 11.5.0-5)\\nClang version: Could not collect\\nCMake version: Could not collect\\nLibc version: glibc-2.34\\n\\nPython version: 3.12.9 (main, Aug 14 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)] (64-bit runtime)\\nPython platform: Linux-5.14.0-570.60.1.el9_6.x86_64-x86_64-with-glibc2.34\\nIs CUDA available: True\\nCUDA runtime version: 12.8.93\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA L40S\\nNvidia driver version: 580.95.05\\ncuDNN version: Probably one of the following:\\n/usr/lib64/libcudnn.so.9.11.0\\n/usr/lib64/libcudnn_adv.so.9.11.0\\n/usr/lib64/libcudnn_cnn.so.9.11.0\\n/usr/lib64/libcudnn_engines_precompiled.so.9.11.0\\n/usr/lib64/libcudnn_engines_runtime_compiled.so.9.11.0\\n/usr/lib64/libcudnn_graph.so.9.11.0\\n/usr/lib64/libcudnn_heuristic.so.9.11.0\\n/usr/lib64/libcudnn_ops.so.9.11.0\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                            x86_64\\nCPU op-mode(s):                          32-bit, 64-bit\\nAddress sizes:                           48 bits physical, 48 bits virtual\\nByte Order:                              Little Endian\\nCPU(s):                                  192\\nOn-line CPU(s) list:                     0-191\\nVendor ID:                               AuthenticAMD\\nModel name:                              AMD EPYC 7R13 Processor\\nCPU family:                              25\\nModel:                                   1\\nThread(s) per core:                      2\\nCore(s) per socket:                      48\\nSocket(s):                               2\\nStepping:                                1\\nBogoMIPS:                                5299.99\\nFlags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\\nHypervisor vendor:                       KVM\\nVirtualization type:                     full\\nL1d cache:                               3 MiB (96 instances)\\nL1i cache:                               3 MiB (96 instances)\\nL2 cache:                                48 MiB (96 instances)\\nL3 cache:                                384 MiB (12 instances)\\nNUMA node(s):                            2\\nNUMA node0 CPU(s):                       0-47,96-143\\nNUMA node1 CPU(s):                       48-95,144-191\\nVulnerability Gather data sampling:      Not affected\\nVulnerability Indirect target selection: Not affected\\nVulnerability Itlb multihit:             Not affected\\nVulnerability L1tf:                      Not affected\\nVulnerability Mds:                       Not affected\\nVulnerability Meltdown:                  Not affected\\nVulnerability Mmio stale data:           Not affected\\nVulnerability Reg file data sampling:    Not affected\\nVulnerability Retbleed:                  Not affected\\nVulnerability Spec rstack overflow:      Mitigation; Safe RET\\nVulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl\\nVulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization\\nVulnerability Spectre v2:                Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\\nVulnerability Srbds:                     Not affected\\nVulnerability Tsx async abort:           Not affected\\n\\nVersions of relevant libraries:\\n[pip3] numpy==2.2.6\\n[pip3] nvidia-cublas-cu12==12.8.4.1\\n[pip3] nvidia-cuda-cupti-cu12==12.8.90\\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.93\\n[pip3] nvidia-cuda-runtime-cu12==12.8.90\\n[pip3] nvidia-cudnn-cu12==9.10.2.21\\n[pip3] nvidia-cufft-cu12==11.3.3.83\\n[pip3] nvidia-curand-cu12==10.3.9.90\\n[pip3] nvidia-cusolver-cu12==11.7.3.90\\n[pip3] nvidia-cusparse-cu12==12.5.8.93\\n[pip3] nvidia-cusparselt-cu12==0.7.1\\n[pip3] nvidia-nccl-cu12==2.27.3\\n[pip3] nvidia-nvjitlink-cu12==12.8.93\\n[pip3] nvidia-nvtx-cu12==12.8.90\\n[pip3] torch==2.8.0\\n[pip3] torchaudio==2.8.0\\n[pip3] torchvision==0.23.0\\n[pip3] triton==3.4.0\\n[conda] Could not collect',\n",
       " 'transformers_version': '4.55.2',\n",
       " 'lm_eval_version': '0.4.9.2',\n",
       " 'upper_git_hash': None,\n",
       " 'tokenizer_pad_token': ['</s>', '2'],\n",
       " 'tokenizer_eos_token': ['</s>', '2'],\n",
       " 'tokenizer_bos_token': ['<s>', '1'],\n",
       " 'eot_token_id': 2,\n",
       " 'max_length': 2048}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_results = load_pickle(base_results_dir)\n",
    "comp_results = load_pickle(compressed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36511cfb-fac8-4f8c-be17-c2d98ffb8560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the results\n",
    "print(make_table(base_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79a881-9bcc-4860-a3a1-7b7da82d0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_table(comp_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210929e-e642-4d73-8a4d-8cb19e7d3249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
