{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61914ae6-02e2-4628-b84b-4a4e8fdd0699",
   "metadata": {},
   "source": [
    "## Launch an Inference Server (vLLM) for the Compressed Model\n",
    "\n",
    "This step sets up a vLLM inference server to host your compressed model and exposes an OpenAI-compatible API endpoint. This server is required so that GuideLLM can benchmark system-level performance like throughput, latency, and time-to-first-token.\n",
    "\n",
    "**Goal**: Make the compressed model accessible via an API for performance evaluation.\n",
    "\n",
    "**Output**: vLLM server running with the compressed model, ready to accept requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e1c76cc-bc87-48b0-8e81-22395b086dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from utils import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e71a286-8e8d-4a24-aa1a-1a1ab3bcce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the logging level for vLLM inference\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"DEBUG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be26ee6-7bab-49ca-a87f-a641ba332f41",
   "metadata": {},
   "source": [
    "### vLLM config for single node\n",
    "For **single-node**, **single-GPU** or **multi-GPU (but not multinode)** vLLM serving, the main arguments are:\n",
    "\n",
    "``--model``: Model path or Hugging Face repo ID (required).\n",
    "\n",
    "``--tensor-parallel-size``: Number of GPUs to use (set to 1 for single GPU, or >1 for multi-GPU tensor parallelism).\n",
    "\n",
    "``--port``: Port for the API server (default is 8000).\n",
    "\n",
    "``--host``: Host IP address (default is 127.0.0.1).\n",
    "\n",
    "``--gpu-memory-utilization``: controls what fraction of each GPU’s memory vLLM will use for the model executor and KV cache. For example, --gpu-memory-utilization 0.5 means vLLM will use 50% of the GPU memory.\n",
    "\n",
    "``--quantization``: Method used to quantize the weights. \n",
    " \n",
    "``--max-model-len``: argument sets the maximum context length (in tokens) that vLLM can process for both prompt and output combined. If not specified, it defaults to the model’s config value. Setting a higher value allows longer prompts/completions but increases GPU memory usage for the KV cache; setting it lower saves memory but limits context length. Set this to prevent problems with memory if the model’s default context length is too long.\n",
    "\n",
    "For **multi-node** vLLM serving, use:\n",
    "\n",
    " ``--tensor-parallel-size`` Number of GPUs per node (or total GPUs if not using pipeline parallelism).\n",
    " \n",
    "``--pipeline-parallel-size`` Number of nodes (optional, for pipeline parallelism).\n",
    "\n",
    "Additionally, for multi-node setup, a Ray cluster is also needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a83ed-6594-43ee-ad74-f86d443810a4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Run this command in terminal to serve the compressed model using vLLM\n",
    "```\n",
    "vllm serve \\\n",
    "  --model \"compressed_model\" \\\n",
    "  --host 127.0.0.1 \\\n",
    "  --port 8000 \\\n",
    "  --gpu-memory-utilization 0.8 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  --pipeline-parallel-size 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b98215-95ba-4fd9-a83f-2e592f510880",
   "metadata": {},
   "source": [
    "Once the server starts, you will see something like this:\n",
    "\n",
    "```INFO:     Started server process [37883]```\\\n",
    "```INFO:     Waiting for application startup.```\\\n",
    "```INFO:     Application startup complete.```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05cad1-020d-4be3-b0d5-2a68785ee26d",
   "metadata": {},
   "source": [
    "## A test run to see if the vLLM server is accessible\n",
    "We use a helper function **generate** (defined in [utils.py](./utils.py)) to simplify sending requests to our locally-served VLLM model.\n",
    "This function wraps the OpenAI-compatible Chat Completions API exposed by VLLM.\n",
    "### Why we Use the OpenAI SDK with vLLM\n",
    "vLLM implements an OpenAI-compatible REST API, meaning:\n",
    "\n",
    "- vLLM starts a local web server (e.g., http://127.0.0.1:8000/v1)\n",
    "\n",
    "- it exposes the same endpoints (/v1/chat/completions, /v1/completions) as OpenAI\n",
    "\n",
    "- accepts the same request schema (messages=[{\"role\": \"...\"}])\n",
    "\n",
    "- the same client interface as OpenAI\n",
    "\n",
    "- The OpenAI SDK doesn't know (or care) that it isn’t talking to OpenAI — it just sends HTTP requests to the specified url in the expected format\n",
    "\n",
    "An **alternate** way is to send POST requests using python's **requests** module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8c92cbc-c2b9-42d8-9169-2ce44f2ba2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-01 15:09:41] INFO _client.py:1025: HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing refers to a branch of computing that operates using the principles of quantum mechanics. It is distinct from classical computing, which operates using bits (0 and 1) stored in memory on a computer's chip.\n",
      "\n",
      "Under the theory of quantum mechanics, quantum bits (qubits) can exist in multiple states simultaneously, making them an ideal candidates for computation. Quantum computing uses qubits to perform a variety of operations, including arithmetic, logical operations, and quantum simulations.\n",
      "\n",
      "Here's how quantum computing works:\n",
      "\n",
      "1. Quantum bits are encoded into a state according to their computational value.\n",
      "\n",
      "2. The state of the qubits is measured, and the results are processed using classical computers.\n",
      "\n",
      "3. The results are then transformed back into the original state by applying classical operations such as rotation, flipping, and addition.\n",
      "\n",
      "4. The process is repeated multiple times, until an answer is obtained.\n",
      "\n",
      "The advantage of quantum computing is that it offers an incredibly fast and efficient way to perform certain calculations. This makes it ideal for solving complex problems that would be impossible for classical computers to handle. However, quantum computing also requires specialized hardware and programming skills to implement and maintain.\n"
     ]
    }
   ],
   "source": [
    "response = generate(\"compressed_model\", \"Explain quantum computing simply?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49932d1b-3779-4308-9be7-3875f5bcbaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
