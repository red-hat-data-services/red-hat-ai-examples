{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61914ae6-02e2-4628-b84b-4a4e8fdd0699",
   "metadata": {},
   "source": [
    "## Launch an Inference Server (vLLM) for the Compressed Model\n",
    "\n",
    "This step sets up a vLLM inference server to host your compressed model and exposes an OpenAI-compatible API endpoint. This server is required so that GuideLLM can benchmark system-level performance like throughput, latency, and time-to-first-token.\n",
    "\n",
    "**Goal**: Make the compressed model accessible via an API for performance evaluation.\n",
    "\n",
    "**Output**: vLLM server running with the compressed model, ready to accept requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1c76cc-bc87-48b0-8e81-22395b086dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from utils import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e71a286-8e8d-4a24-aa1a-1a1ab3bcce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the logging level for vLLM inference\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"DEBUG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be26ee6-7bab-49ca-a87f-a641ba332f41",
   "metadata": {},
   "source": [
    "### vLLM config for single node\n",
    "For **single-node**, **single-GPU** or **multi-GPU (but not multinode)** vLLM serving, the main arguments are:\n",
    "\n",
    "``--model``: Model path or Hugging Face repo ID (required).\n",
    "\n",
    "``--tensor-parallel-size``: Number of GPUs to use (set to 1 for single GPU, or >1 for multi-GPU tensor parallelism).\n",
    "\n",
    "``--port``: Port for the API server (default is 8000).\n",
    "\n",
    "``--host``: Host IP address (default is 127.0.0.1).\n",
    "\n",
    "``--gpu-memory-utilization``: controls what fraction of each GPU’s memory vLLM will use for the model executor and KV cache. For example, --gpu-memory-utilization 0.5 means vLLM will use 50% of the GPU memory.\n",
    "\n",
    "``--quantization``: Method used to quantize the weights. \n",
    " \n",
    "``--max-model-len``: argument sets the maximum context length (in tokens) that vLLM can process for both prompt and output combined. If not specified, it defaults to the model’s config value. Setting a higher value allows longer prompts/completions but increases GPU memory usage for the KV cache; setting it lower saves memory but limits context length. Set this to prevent problems with memory if the model’s default context length is too long.\n",
    "\n",
    "For **multi-node** vLLM serving, use:\n",
    "\n",
    " ``--tensor-parallel-size`` Number of GPUs per node (or total GPUs if not using pipeline parallelism).\n",
    " \n",
    "``--pipeline-parallel-size`` Number of nodes (optional, for pipeline parallelism).\n",
    "\n",
    "Additionally, for multi-node setup, a Ray cluster is also needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a83ed-6594-43ee-ad74-f86d443810a4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Run this command in terminal to serve the compressed model using vLLM\n",
    "```\n",
    "vllm serve \\\n",
    "  --model \"compressed_model\" \\\n",
    "  --host 127.0.0.1 \\\n",
    "  --port 8000 \\\n",
    "  --gpu-memory-utilization 0.8 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  --pipeline-parallel-size 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b98215-95ba-4fd9-a83f-2e592f510880",
   "metadata": {},
   "source": [
    "Once the server starts, you will see something like this:\n",
    "\n",
    "```INFO:     Started server process [37883]```\\\n",
    "```INFO:     Waiting for application startup.```\\\n",
    "```INFO:     Application startup complete.```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05cad1-020d-4be3-b0d5-2a68785ee26d",
   "metadata": {},
   "source": [
    "## A test run to see if the vLLM server is accessible\n",
    "We use a helper function **generate** (defined in [utils.py](./utils.py)) to simplify sending requests to our locally-served VLLM model.\n",
    "This function wraps the OpenAI-compatible Chat Completions API exposed by VLLM.\n",
    "### Why we Use the OpenAI SDK with vLLM\n",
    "vLLM implements an OpenAI-compatible REST API, meaning:\n",
    "\n",
    "- vLLM starts a local web server (e.g., http://127.0.0.1:8000/v1)\n",
    "\n",
    "- it exposes the same endpoints (/v1/chat/completions, /v1/completions) as OpenAI\n",
    "\n",
    "- accepts the same request schema (messages=[{\"role\": \"...\"}])\n",
    "\n",
    "- the same client interface as OpenAI\n",
    "\n",
    "- The OpenAI SDK doesn't know (or care) that it isn’t talking to OpenAI — it just sends HTTP requests to the specified url in the expected format\n",
    "\n",
    "An **alternate** way is to send POST requests using python's **requests** module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c92cbc-c2b9-42d8-9169-2ce44f2ba2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a complex topic, but I'll try to break it down in simple terms.\n",
      "\n",
      "**Classical Computing**\n",
      "\n",
      "Imagine you have a combination lock with 10 numbers (0-9). To open the lock, you need to try each number one by one, until you find the correct combination. This is like a classical computer, which uses \"bits\" (0s and 1s) to process information.\n",
      "\n",
      "**Quantum Computing**\n",
      "\n",
      "Now, imagine you have a special lock that can try all 10 numbers simultaneously, and it will open the lock as soon as it finds the correct combination. This is like a quantum computer, which uses \"qubits\" (quantum bits) to process information.\n",
      "\n",
      "Qubits are special because they can exist in multiple states at the same time, unlike classical bits which are either 0 or 1. This property, called superposition, allows quantum computers to process multiple possibilities simultaneously, making them incredibly fast for certain types of calculations.\n",
      "\n",
      "**How it Works**\n",
      "\n",
      "Quantum computers use quantum bits (qubits) to perform calculations. Qubits are made up of tiny particles, such as atoms or photons, that can exist in multiple states at the same time. When you apply a quantum operation to a qubit, it\n"
     ]
    }
   ],
   "source": [
    "response = generate(\"compressed_model\", \"Explain quantum computing simply?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49932d1b-3779-4308-9be7-3875f5bcbaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
