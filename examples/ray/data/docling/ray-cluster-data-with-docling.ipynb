{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadab689-3497-4195-b867-95c036484b2a",
   "metadata": {},
   "source": [
    "# Distributed PDF Processing with Ray Data, Docling, and RayCluster\n",
    "\n",
    "This notebook demonstrates how to create a **RayCluster** and submit document processing jobs to it using the CodeFlare SDK. Unlike the RayJob approach, here you manage the cluster lifecycle yourself, which allows you to submit multiple jobs to the same cluster.\n",
    "\n",
    "## What you will do\n",
    "\n",
    "1. Authenticate to your OpenShift cluster\n",
    "2. Create a Ray Data processing script that converts PDFs to structured JSON using Docling\n",
    "3. Create a RayCluster with PVC mounts and configured resources\n",
    "4. Submit a processing job to the running cluster\n",
    "5. Monitor the job status and retrieve logs\n",
    "6. Clean up resources when done\n",
    "\n",
    "## When to use this approach\n",
    "\n",
    "Use the **RayCluster** approach when you want a long-lived cluster to submit multiple jobs interactively or iteratively. You control when the cluster is created and torn down.\n",
    "\n",
    "> **Note:** If you have a single batch job and want automatic cluster lifecycle management, see the companion notebook `ray-data-with-docling.ipynb`.\n",
    "\n",
    "## Pipeline overview\n",
    "\n",
    "The submitted script (`ray_data_process_async.py`) uses [Docling](https://github.com/DS4SD/docling) and [Ray Data](https://docs.ray.io/en/latest/data/data.html) to:\n",
    "\n",
    "- Read PDF files from a Persistent Volume Claim (PVC)\n",
    "- Convert PDFs to structured JSON using distributed actor pools\n",
    "- Write results back to the PVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23c378-b38b-49b4-998a-5d4c908e0a0a",
   "metadata": {},
   "source": [
    "## Import SDK components\n",
    "\n",
    "We import the following components:\n",
    "\n",
    "- **CodeFlare SDK**: `TokenAuthentication` for OpenShift authentication, `Cluster` to create and manage a RayCluster, and `ClusterConfiguration` to define cluster resources\n",
    "- **Kubernetes client**: `V1VolumeMount`, `V1Volume`, and `V1PersistentVolumeClaimVolumeSource` to configure PVC mounts on the cluster pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d185d13-f2f4-4f14-bea9-d467a9849c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk import Cluster, ClusterConfiguration\n",
    "from kubernetes import client as kclient\n",
    "from kubernetes.client import (\n",
    "    V1PersistentVolumeClaimVolumeSource,\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    ")\n",
    "from kubernetes.client.rest import ApiException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac86e1c",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "Configure authentication to your OpenShift cluster. If you're running within an OpenShift environment with default kubeconfig, authentication may be automatic. Otherwise, provide your token and server URL.\n",
    "\n",
    "> **Note**: Replace the token and server values with your own credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1af1a7-1687-4001-ba01-37d465acdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"sha256~your-long-oauth-token-here\"\n",
    "API_URL = \"https://api.your-cluster.example.com:6443\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14705caf-01aa-40fa-ae01-f905cf9f51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the below `oc login` command using your Token and Server URL. Ensure the command is prepended by `!` and not `%`. This will work when running both locally and within RHOAI.\n",
    "!oc login --token={TOKEN} --server={API_URL}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d0b16b",
   "metadata": {},
   "source": [
    "## Create the processing script\n",
    "\n",
    "The cell below writes `ray_data_process_async.py`, the script that will be submitted as a job to the RayCluster. This script is **optimized for maximum throughput**:\n",
    "\n",
    "| Optimization | Implementation | Impact |\n",
    "|---|---|---|\n",
    "| **One-time model loading** | `DoclingProcessor` loads Docling models once per actor | Avoids repeated startup overhead per file |\n",
    "| **Parallel PVC writes** | Each actor writes directly to PVC | N actors = N concurrent writes |\n",
    "| **Streaming execution** | Read, process, and write stages overlap via `iter_batches()` | Keeps all actors busy |\n",
    "| **Prefetching** | `prefetch_batches=2` keeps data ready for actors | Reduces I/O wait time |\n",
    "| **Configurable scaling** | `MIN_ACTORS`/`MAX_ACTORS` environment variables | Tune for your cluster size |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80545c-195a-44ca-82ca-d77626f80bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ray_data_process_async.py\n",
    "\"\"\"\n",
    "Ray Data Pipeline: PDF to Markdown Conversion ( Optimized)\n",
    "\n",
    "This script uses an DoclingProcessor to overlap CPU-heavy document conversion\n",
    "with I/O-heavy PVC (Persistent Volume Claim) writes. \n",
    "\n",
    "Key Optimizations:\n",
    "1. Actor: Handlers yield control during I/O, allowing better pipelining.\n",
    "2. ThreadPoolExecutor: Offloads synchronous Docling calls so the event loop doesn't hang.\n",
    "3. Overlapped I/O: Writes Markdown and JSON to the PVC in parallel.\n",
    "4. Ray Data Pipelining: max_tasks_in_flight_per_actor allows buffering work.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ray\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import tqdm\n",
    "\n",
    "# --- PVC Configuration ---\n",
    "PVC_MOUNT_PATH = os.environ.get(\"PVC_MOUNT_PATH\", \"/mnt/data\")\n",
    "INPUT_PATH = os.environ.get(\"INPUT_PATH\", \"input/pdfs/10000\")\n",
    "OUTPUT_PATH = os.environ.get(\"OUTPUT_PATH\", \"output\")\n",
    "\n",
    "# --- Performance Tuning Parameters ---\n",
    "NUM_FILES = int(os.environ.get(\"NUM_FILES\", \"5000\"))\n",
    "MIN_ACTORS = int(os.environ.get(\"MIN_ACTORS\", \"8\"))\n",
    "MAX_ACTORS = int(os.environ.get(\"MAX_ACTORS\", \"8\"))\n",
    "CPUS_PER_ACTOR = int(os.environ.get(\"CPUS_PER_ACTOR\", \"8\"))\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"1\"))\n",
    "TIMEOUT_SECONDS = 600\n",
    "\n",
    "class DoclingProcessor:\n",
    "    \"\"\"\n",
    "    Stateful Ray Actor that handles PDF conversion.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        import socket\n",
    "        from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "        from docling.datamodel.pipeline_options import PdfPipelineOptions, AcceleratorOptions\n",
    "        from docling.datamodel.base_models import InputFormat\n",
    "        from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "\n",
    "        # Hardware-level thread tuning\n",
    "        os.environ[\"OMP_NUM_THREADS\"] = str(CPUS_PER_ACTOR)\n",
    "        os.environ[\"MKL_NUM_THREADS\"] = str(CPUS_PER_ACTOR)\n",
    "        \n",
    "        self.hostname = socket.gethostname()\n",
    "        self.processed_count = 0\n",
    "\n",
    "        # Initialize Docling Converter\n",
    "        pipeline_options = PdfPipelineOptions()\n",
    "        pipeline_options.do_ocr = False\n",
    "        pipeline_options.do_table_structure = True\n",
    "        pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "            num_threads=CPUS_PER_ACTOR,\n",
    "            device=\"cpu\"\n",
    "        )\n",
    "        \n",
    "        pdf_format_config = PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "        #    backend=PyPdfiumDocumentBackend\n",
    "        )\n",
    "\n",
    "        self.converter = DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: pdf_format_config,\n",
    "                           #InputFormat.Model: \n",
    "                           }\n",
    "        )\n",
    "        \n",
    "        # Prepare output paths\n",
    "        self.output_base = Path(PVC_MOUNT_PATH) / OUTPUT_PATH\n",
    "        self.markdown_dir = self.output_base / \"markdown\"\n",
    "        self.json_dir = self.output_base / \"json\"\n",
    "        self.markdown_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.json_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"[{self.hostname}] üü¢ Actor initialized (CPUs: {CPUS_PER_ACTOR})\")\n",
    "\n",
    "    def __call__(self, batch: Dict[str, List]) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        Processes a batch of PDFs. \n",
    "        \"\"\"\n",
    "        from docling.datamodel.base_models import DocumentStream\n",
    "        import io\n",
    "        import orjson\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for file_bytes, file_path in zip(batch[\"bytes\"], batch[\"path\"]):\n",
    "            fname = os.path.basename(file_path)\n",
    "            fname_base = fname.rsplit('.', 1)[0]\n",
    "            status = \"success\"\n",
    "            error_msg = \"\"\n",
    "            page_count = 0\n",
    "            docling_duration = 0.0\n",
    "            output_size_bytes = 0\n",
    "\n",
    "            try:\n",
    "                if not file_bytes or len(file_bytes) < 100:\n",
    "                    raise ValueError(\"File empty or too small\")\n",
    "\n",
    "                docling_start = time.time()\n",
    "                \n",
    "                # --- Step 1: Heavy CPU Conversion ---\n",
    "                stream = DocumentStream(name=fname, stream=io.BytesIO(file_bytes))\n",
    "                doc = self.converter.convert(stream)\n",
    "\n",
    "                page_count = len(doc.document.pages) if hasattr(doc.document, 'pages') else 0\n",
    "                #md_out = doc.document.export_to_markdown()\n",
    "                json_out = doc.document.export_to_dict()\n",
    "                \n",
    "                docling_duration = time.time() - docling_start\n",
    "\n",
    "                # --- Step 2: Parallel I/O Writes ---\n",
    "                #md_path = self.markdown_dir / f\"{fname_base}.md\"\n",
    "                json_path = self.json_dir / f\"{fname_base}.json\"\n",
    "\n",
    "                json_bytes = orjson.dumps(json_out, option=orjson.OPT_INDENT_2)\n",
    "\n",
    "                #self._write_with_retry(md_path, md_out)\n",
    "                output_size_bytes = len(json_bytes)\n",
    "                self._write_with_retry(json_path, json_bytes)\n",
    "\n",
    "                \n",
    "                self.processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                status = \"error\"\n",
    "                error_msg = str(e)[:150]\n",
    "\n",
    "            results.append({\n",
    "                \"filename\": str(fname),\n",
    "                \"status\": str(status),\n",
    "                \"page_count\": int(page_count),  \n",
    "                \"error\": str(error_msg),\n",
    "                \n",
    "                # Timing\n",
    "                \"docling_duration_s\": float(round(docling_duration, 2)),\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \n",
    "                # Size metrics\n",
    "                \"file_size_mb\": float(round(len(file_bytes) / (1024 * 1024), 3)),\n",
    "                \"output_size_kb\": float(round(len(orjson.dumps(json_out)) / 1024, 2)) if status == \"success\" else 0.0,\n",
    "                \n",
    "                # Efficiency\n",
    "                \"pages_per_second\": float(round(page_count / docling_duration, 2)) if docling_duration > 0 else 0.0,\n",
    "                \n",
    "                # Distribution tracking\n",
    "                \"actor_hostname\": self.hostname,\n",
    "            })\n",
    "\n",
    "        return {\"results\": results}\n",
    "    \n",
    "    def _write_with_retry(self, path: Path, content: str, max_retries: int = 3):\n",
    "        \"\"\"Worker-local disk write with retry logic.\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                path.write_bytes(content)\n",
    "                return\n",
    "            except Exception:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(1)\n",
    "\n",
    "def ray_data_process():\n",
    "    input_full_path = os.path.join(PVC_MOUNT_PATH, INPUT_PATH)\n",
    "    \n",
    "    # 1. SET GLOBAL PARALLELISM BEFORE READING\n",
    "    ctx = ray.data.DataContext.get_current()\n",
    "    ctx.execution_options.preserve_order = False\n",
    "    ctx.execution_options.actor_locality_enabled = True\n",
    "    # Force the minimum number of blocks globally\n",
    "    ctx.min_parallelism = 100 \n",
    "    ctx.target_min_block_size = 1 * 1024 * 1024\n",
    "    ctx.target_max_block_size = 2 * 1024 * 1024  # 2 MB\n",
    "    ctx.target_shuffle_block_size = 1 * 1024 * 1024\n",
    "    target_blocks = MAX_ACTORS * 4\n",
    "\n",
    "    # 2. READ WITH EXPLICIT PARALLELISM\n",
    "    # This forces the initial 'ReadBinary' stage to create 100+ blocks\n",
    "    ds = ray.data.read_binary_files(\n",
    "        input_full_path, \n",
    "        include_paths=True,\n",
    "        override_num_blocks=target_blocks\n",
    "    )\n",
    "    \n",
    "    ds = ds.filter(lambda row: row[\"path\"].lower().endswith(\".pdf\")).limit(NUM_FILES)\n",
    "\n",
    "    # 3. FORCE REPARTITION WITH SHUFFLE\n",
    "    # Setting shuffle=True forces Ray to actually redistribute the data \n",
    "    # instead of just marking the metadata.\n",
    "    # ds = ds.repartition(num_blocks=target_blocks, shuffle=False)\n",
    "\n",
    "    # Stage 2: Map with Actor Pool\n",
    "    results_ds = ds.map_batches(\n",
    "        DoclingProcessor,\n",
    "        compute=ray.data.ActorPoolStrategy(\n",
    "            min_size=MIN_ACTORS,\n",
    "            max_size=MAX_ACTORS,\n",
    "        ),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_cpus=CPUS_PER_ACTOR,\n",
    "    )\n",
    "    print(f\"DEBUG: Target blocks: {target_blocks}, Max actors: {MAX_ACTORS}\")    \n",
    "    \n",
    "    # Stage 3: Collect & Report\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    total_pages = 0\n",
    "    total_docling_time = 0.0\n",
    "    start_time = time.time()\n",
    "    # Additional tracking variables\n",
    "    total_file_size_mb = 0.0\n",
    "    total_output_size_kb = 0.0\n",
    "    actor_distribution = {}\n",
    "    file_durations = []\n",
    "    errors_list = []\n",
    "    \n",
    "    for batch in results_ds.iter_batches(batch_size=10, prefetch_batches=2):\n",
    "        for result_list in batch.get(\"results\", []):\n",
    "            items = result_list if isinstance(result_list, list) else [result_list]\n",
    "            for item in items:\n",
    "                if item[\"status\"] == \"success\":\n",
    "                    success_count += 1\n",
    "                    file_durations.append((item[\"filename\"], item[\"docling_duration_s\"]))\n",
    "                else:\n",
    "                    error_count += 1\n",
    "                    errors_list.append((item[\"filename\"], item.get(\"error\", \"\")))\n",
    "                \n",
    "                total_pages += item[\"page_count\"]\n",
    "                total_docling_time += item[\"docling_duration_s\"]\n",
    "                total_file_size_mb += item.get(\"file_size_mb\", 0)\n",
    "                total_output_size_kb += item.get(\"output_size_kb\", 0)\n",
    "                \n",
    "                # Track actor distribution\n",
    "                actor = item.get(\"actor_hostname\", \"unknown\")\n",
    "                actor_distribution[actor] = actor_distribution.get(actor, 0) + 1\n",
    "\n",
    "    # Calculate derived metrics\n",
    "    wall_clock = time.time() - start_time\n",
    "    total_files = success_count + error_count\n",
    "    error_rate = (error_count / total_files * 100) if total_files > 0 else 0\n",
    "    avg_pages_per_file = total_pages / success_count if success_count > 0 else 0\n",
    "    parallelization_efficiency = (total_docling_time / wall_clock / MAX_ACTORS * 100) if wall_clock > 0 else 0\n",
    "    \n",
    "    # Sort for fastest/slowest\n",
    "    if file_durations:\n",
    "        file_durations.sort(key=lambda x: x[1])\n",
    "        fastest = file_durations[0]\n",
    "        slowest = file_durations[-1]\n",
    "    \n",
    "    # Enhanced Report\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PERFORMANCE REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n--- Results Summary ---\")\n",
    "    print(f\"Total Files:    {total_files}\")\n",
    "    print(f\"Success:        {success_count} ({100-error_rate:.1f}%)\")\n",
    "    print(f\"Errors:         {error_count} ({error_rate:.1f}%)\")\n",
    "    print(f\"Total Pages:    {total_pages}\")\n",
    "    print(f\"Avg Pages/File: {avg_pages_per_file:.1f}\")\n",
    "    \n",
    "    print(\"\\n--- Data Volume ---\")\n",
    "    print(f\"Input Size:     {total_file_size_mb:.2f} MB\")\n",
    "    print(f\"Output Size:    {total_output_size_kb/1024:.2f} MB\")\n",
    "    print(f\"Compression:    {total_output_size_kb*1024/total_file_size_mb/1024/1024:.1f}x\" if total_file_size_mb > 0 else \"N/A\")\n",
    "    \n",
    "    print(\"\\n--- Timing ---\")\n",
    "    print(f\"Wall Clock:     {wall_clock:.2f}s\")\n",
    "    print(f\"CPU Time (sum): {total_docling_time:.2f}s\")\n",
    "    \n",
    "    print(\"\\n--- Throughput ---\")\n",
    "    print(f\"Files/second:   {success_count / wall_clock:.2f}\")\n",
    "    print(f\"Pages/second:   {total_pages / wall_clock:.2f}\")\n",
    "    print(f\"MB/second:      {total_file_size_mb / wall_clock:.2f}\")\n",
    "    \n",
    "    print(\"\\n--- Efficiency ---\")\n",
    "    print(f\"Parallelization: {parallelization_efficiency:.1f}% of ideal\")\n",
    "    print(f\"Speedup:         {total_docling_time / wall_clock:.1f}x vs sequential\")\n",
    "    \n",
    "    if file_durations:\n",
    "        print(\"\\n--- Outliers ---\")\n",
    "        print(f\"Fastest: {fastest[0]} ({fastest[1]:.2f}s)\")\n",
    "        print(f\"Slowest: {slowest[0]} ({slowest[1]:.2f}s)\")\n",
    "    \n",
    "    print(\"\\n--- Actor Distribution ---\")\n",
    "    for actor, count in sorted(actor_distribution.items()):\n",
    "        pct = count / total_files * 100\n",
    "        print(f\"  {actor}: {count} files ({pct:.1f}%)\")\n",
    "    \n",
    "    if errors_list:\n",
    "        print(\"\\n--- Errors (first 5) ---\")\n",
    "        for fname, err in errors_list[:5]:\n",
    "            print(f\"  {fname}: {err[:60]}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    ray_data_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169cebb",
   "metadata": {},
   "source": [
    "## PVC requirements\n",
    "\n",
    "To use PVC storage with your RayCluster, you need to:\n",
    "\n",
    "1. **Create a PVC** with `ReadWriteMany` (RWX) access mode\n",
    "2. **Mount the PVC** on both the Ray head and worker pods\n",
    "3. **Upload your PDF files** to the PVC before running the job\n",
    "\n",
    "### Step 1: Create the PVC\n",
    "\n",
    "See the Setup section in the [README](./README.md) for the PVC YAML configuration.\n",
    "\n",
    "### Step 2: Verify the PVC\n",
    "\n",
    "Run the cell below to verify the PVC exists and has the correct access mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf84445-363e-48a0-9595-4ffe22ccfc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster and namespace configuration\n",
    "EXISTING_CLUSTER_NAME = \"rayjobtest\"\n",
    "NAMESPACE = \"ray-docling\"\n",
    "\n",
    "# PVC Configuration\n",
    "PVC_NAME = \"my-rwx-pvc2\"\n",
    "PVC_MOUNT_PATH = \"/mnt/data\"\n",
    "INPUT_PATH = \"input/pdfs/10000\"\n",
    "OUTPUT_PATH = \"output\"\n",
    "\n",
    "\n",
    "def verify_pvc():\n",
    "    # Load Kubernetes configuration\n",
    "    # This looks for the kubeconfig file (usually at ~/.kube/config)\n",
    "    # Your OpenShift Details\n",
    "\n",
    "    configuration = kclient.Configuration()\n",
    "    configuration.host = API_URL\n",
    "    configuration.verify_ssl = True\n",
    "    configuration.api_key[\"authorization\"] = f\"Bearer {TOKEN}\"\n",
    "\n",
    "    # Initialize client with manual config\n",
    "    v1 = kclient.CoreV1Api(kclient.ApiClient(configuration))\n",
    "\n",
    "    print(f\"üîç Checking PVC '{PVC_NAME}' in namespace '{NAMESPACE}'...\")\n",
    "\n",
    "    try:\n",
    "        # Fetch the PVC object\n",
    "        pvc = v1.read_namespaced_persistent_volume_claim(\n",
    "            name=PVC_NAME, namespace=NAMESPACE\n",
    "        )\n",
    "\n",
    "        # Check Status\n",
    "        pvc_status = pvc.status.phase\n",
    "        print(f\"‚úÖ PVC '{PVC_NAME}' found, status: {pvc_status}\")\n",
    "\n",
    "        # Check Access Modes\n",
    "        access_modes = pvc.spec.access_modes\n",
    "        if access_modes:\n",
    "            primary_mode = access_modes[0]\n",
    "            print(f\"   Access Mode: {primary_mode}\")\n",
    "\n",
    "            if primary_mode != \"ReadWriteMany\":\n",
    "                print(\n",
    "                    \"   ‚ö†Ô∏è  Warning: PVC should use 'ReadWriteMany' access mode for concurrent writes from multiple workers.\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Warning: No access modes defined for this PVC.\")\n",
    "\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"‚ùå PVC '{PVC_NAME}' not found in namespace '{NAMESPACE}'\")\n",
    "            print(\n",
    "                \"\\n   Create the PVC first using the YAML in the configuration section.\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"‚ùå An error occurred while fetching PVC: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_pvc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e71d02",
   "metadata": {},
   "source": [
    "## Configure the RayCluster\n",
    "\n",
    "Define the cluster configuration, prepare the PVC volume mount, and create the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382e830-ac58-47c3-8b21-d4a77ac889c8",
   "metadata": {},
   "source": [
    "### Prepare the PVC volume mount\n",
    "\n",
    "Define the volume mount and volume objects that will attach the PVC to the RayCluster pods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c73447-f460-4c1c-9fac-2250e5d7d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_mount = V1VolumeMount(\"/mnt/data\", name=\"shared-data\")\n",
    "data_volume = V1Volume(\n",
    "    name=\"shared-data\",\n",
    "    persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=PVC_NAME),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc6d73-a496-4695-a700-a12a902d37cb",
   "metadata": {},
   "source": [
    "### Define cluster resources\n",
    "\n",
    "Define the cluster resource specifications (CPU, memory, number of workers) for the RayCluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f96c5-5c3e-4ca1-95b2-686c185ce423",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config = ClusterConfiguration(\n",
    "    name=\"ray-data-processor\",\n",
    "    namespace=NAMESPACE,\n",
    "    num_workers=8,\n",
    "    worker_cpu_requests=8,\n",
    "    worker_cpu_limits=8,\n",
    "    worker_memory_requests=8,\n",
    "    worker_memory_limits=8,\n",
    "    volume_mounts=[shared_mount],\n",
    "    volumes=[data_volume],\n",
    "    image=\"quay.io/cathaloconnor/docling-ray:latest\",\n",
    "    envs={\"RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION\": \"0.3\"},\n",
    ")\n",
    "cluster = Cluster(config=cluster_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce02a90-43c1-4b95-9fab-697358a8d8ea",
   "metadata": {},
   "source": [
    "### Create the cluster\n",
    "\n",
    "Start the RayCluster and wait until it is ready to accept job submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c272bc5-f83e-4212-b30c-ee7669d04efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.apply()\n",
    "cluster.wait_ready(dashboard_check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e0119-a480-484f-830c-4b2791c9d57d",
   "metadata": {},
   "source": [
    "### Cluster details\n",
    "\n",
    "Fetch the cluster details, including the Ray dashboard URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e222a4-73ce-4313-bdf2-0f8c01ff4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883403b0-3b9e-4b6c-a37d-a6a569a9631a",
   "metadata": {},
   "source": [
    "## Submit a job to the cluster\n",
    "\n",
    "Initialize a job submission client from the cluster, then submit the processing job with performance tuning parameters:\n",
    "\n",
    "| Parameter | Description | Tuning guidance |\n",
    "|---|---|---|\n",
    "| `MAX_ACTORS` | Maximum parallel actors | Set to `total_cluster_cpus / CPUS_PER_ACTOR` |\n",
    "| `MIN_ACTORS` | Warm actors (avoids cold start) | 2-4 for steady workloads |\n",
    "| `CPUS_PER_ACTOR` | CPUs per Docling actor | 2 for most PDFs, 4 for complex documents |\n",
    "| `BATCH_SIZE` | PDFs per actor batch | 1 for large PDFs, 2-4 for small PDFs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66f25e-68aa-4a3a-80ba-b04c1c53c5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdbf9209",
   "metadata": {},
   "source": [
    "### Configure and submit the job\n",
    "\n",
    "Set the performance tuning parameters and submit the job with its runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b14d7-8df7-4406-a06d-fda3e1aa283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ‚ö° PERFORMANCE TUNING PARAMETERS\n",
    "# ============================================\n",
    "# Adjust these based on your cluster size for maximum throughput\n",
    "\n",
    "# How many files to process\n",
    "NUM_FILES = \"5000\"\n",
    "\n",
    "# Actor pool sizing (CRITICAL for throughput!)\n",
    "# Formula: MAX_ACTORS ‚âà (total_worker_cpus) / CPUS_PER_ACTOR\n",
    "# Example: 4 workers √ó 8 CPUs each = 32 CPUs ‚Üí MAX_ACTORS = 16 with 2 CPUs each\n",
    "MIN_ACTORS = \"16\"  # Keep minimum actors warm (avoids cold start)\n",
    "MAX_ACTORS = \"16\"  # Scale up based on cluster (increase for larger clusters!)\n",
    "\n",
    "# CPUs per Docling actor\n",
    "CPUS_PER_ACTOR = \"4\"\n",
    "\n",
    "# PDFs per batch (1 for large PDFs, 2-4 for small PDFs < 1MB)\n",
    "BATCH_SIZE = \"1\"\n",
    "\n",
    "# Initialize the Job Submission Client\n",
    "\n",
    "# The SDK will automatically gather the dashboard address and authenticate using the Ray Job Submission Client\n",
    "client = cluster.job_client\n",
    "\n",
    "# Submit an example mnist job using the Job Submission Client\n",
    "submission_id = client.submit_job(\n",
    "    entrypoint=\"python ray_data_process_async.py\",\n",
    "    runtime_env={\n",
    "        \"working_dir\": \".\",\n",
    "        \"pip\": [\"opencv-python-headless\", \"pypdfium2\", \"orjson\"],\n",
    "        \"env_vars\": {\n",
    "            # PVC configuration\n",
    "            \"PVC_MOUNT_PATH\": PVC_MOUNT_PATH,\n",
    "            \"INPUT_PATH\": INPUT_PATH,\n",
    "            \"OUTPUT_PATH\": OUTPUT_PATH,\n",
    "            # Performance tuning\n",
    "            \"NUM_FILES\": NUM_FILES,\n",
    "            \"MIN_ACTORS\": MIN_ACTORS,\n",
    "            \"MAX_ACTORS\": MAX_ACTORS,\n",
    "            \"CPUS_PER_ACTOR\": CPUS_PER_ACTOR,\n",
    "            \"BATCH_SIZE\": BATCH_SIZE,\n",
    "            # üí° Enable detailed progress monitoring\n",
    "            \"RAY_DATA_ENABLE_RICH_PROGRESS_BARS\": \"true\",\n",
    "            \"RAY_record_task_actor_creation_sites\": \"true\",\n",
    "            \"RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION\": \"0.5\",\n",
    "            # Cache directories\n",
    "            \"HF_HOME\": \"/tmp/huggingface\",\n",
    "            \"XDG_CACHE_HOME\": \"/tmp/cache\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(submission_id)\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Job Submitted with ID: {submission_id}\")\n",
    "print(f\"   ‚Üí PVC Mount: {PVC_MOUNT_PATH}\")\n",
    "print(f\"   ‚Üí Input Path: {PVC_MOUNT_PATH}/{INPUT_PATH}\")\n",
    "print(f\"   ‚Üí Output Path: {PVC_MOUNT_PATH}/{OUTPUT_PATH}\")\n",
    "print(f\"   ‚Üí Actors:  {MIN_ACTORS}-{MAX_ACTORS} √ó {CPUS_PER_ACTOR} CPUs each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2ec8d",
   "metadata": {},
   "source": [
    "## Monitor job status\n",
    "\n",
    "Check the status of the submitted job. Re-run these cells to see updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7151f-1844-4b3d-9cd4-e8ab73b4419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all existing jobs\n",
    "print(\"Job ID\", \" - \", \"Submission ID\", \" - \", \"Status\")\n",
    "for j in client.list_jobs():\n",
    "    print(j.job_id, \" - \", j.submission_id, \" - \", j.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8ccde-d773-4c34-8154-188db6da61c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.get_job_status(submission_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7c99af",
   "metadata": {},
   "source": [
    "## Retrieve job logs\n",
    "\n",
    "View or stream the job logs using the job submission client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a82c54-4509-4114-98b0-2e100488f0bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this could be a large object\n",
    "client.get_job_logs(submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c296751-9582-43f5-839e-c17967f36794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate through the logs of a job\n",
    "async for lines in client.tail_job_logs(submission_id):\n",
    "    print(lines, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344aa71-64ce-427a-a3d9-2f5f837aab0a",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Clean up resources when you are done processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f09fc-6b76-43c7-b11c-e332d1b0432c",
   "metadata": {},
   "source": [
    "### Remove the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e1f37-fb63-4c82-84ff-1ac32a0ccb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a job\n",
    "# Can run client.stop_job(submission_id) first if job is still running\n",
    "\n",
    "# client.delete_job(submission_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07f864-8015-4391-a761-1535ae86309a",
   "metadata": {},
   "source": [
    "### Remove the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e80a2-4292-48b4-ba65-adf5658fee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b124f9-354f-42ef-b7fe-bf4cd6c531e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
