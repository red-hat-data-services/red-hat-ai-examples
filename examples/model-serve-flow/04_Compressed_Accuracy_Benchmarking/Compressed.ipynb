{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dec15d-d789-4e22-b9ea-5eec04bf9230",
   "metadata": {},
   "source": [
    "## Evaluate Accuracy of the Compressed Model\n",
    "Now that the base model has been compressed and an accuracy baseline has been established, this step evaluates the **compressed model** on standard benchmarks. The goal is to quantify how model compression impacts accuracy and generative quality relative to the base model.\n",
    "\n",
    "**Goal**: Measure the accuracy of the compressed model and enable a direct comparison with the base model to assess the impact of compression.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- We will create a function called **evaluate** that uses `simple_evaluate` from LM Eval to test the compressed model.\n",
    "\n",
    "- Benchmark on multiple datasets:\n",
    "\n",
    "    - MMLU: General knowledge across subjects.\n",
    "\n",
    "    - IFeval: Instruction-following tasks.\n",
    "\n",
    "    - ARC: Logical and scientific reasoning.\n",
    "    \n",
    "    - HellaSwag: Commonsense completion.\n",
    "\n",
    "- Collect metrics like accuracy, accuracy_norm, and task-specific scores.\n",
    "\n",
    "- Save results as JSON for later comparison.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Quantitative metrics for the compressed model.\n",
    "\n",
    "- Confidence that the model is good enough in terms of accuracy.\n",
    "\n",
    "More details on evaluating LLMs is provided in [Accuracy_Evaluation.md](../docs/Accuracy_Evaluation.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18af3e",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d2bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to install dependencies if dependencies were not installed in 01_Base_Accuracy_Benchmarking/Base.ipynb\n",
    "# !pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f45e6-3f29-4922-b85f-2b77be643530",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from lm_eval.utils import make_table\n",
    "from utils import evaluate, load_pickle, save_pickle\n",
    "\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e3f9b-f437-4f3f-9391-c23b31fc5ef5",
   "metadata": {},
   "source": [
    "To make sure you have enough GPU memory to run this notebook, run the following command in terminal:\n",
    "\n",
    "`nvidia-smi`\n",
    "\n",
    "The output will look something like this:\n",
    "\n",
    "```text\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA L40S                    On  |   00000000:CA:00.0 Off |                    0 |\n",
    "| N/A   44C    P0             91W /  350W |   15753MiB /  46068MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A            8049      C   /opt/app-root/bin/python3             15744MiB |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "\n",
    "If there are any processes already running and using significant amount of GPU memory, note the PID and run the following command:\n",
    "\n",
    "```\n",
    "`kill -9 <pid>`\n",
    "\n",
    "\n",
    "Replace <pid> with the actual PID for example `8049` in this case. So the command will become `kill -9 8094`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e43e6d-4fbe-4bd4-91ae-156b6a1c3af5",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfb397-d989-4a4b-8636-511a25c12ed6",
   "metadata": {},
   "source": [
    "## Define evaluation benchmarking datasets\n",
    "We evaluate the compressed model on the same benchmarks as the base model to make the results comparable.\n",
    "The following benchmark datasets can be used for evaluating on multiple tasks:\n",
    "- MMLU: General knowledge across 57 subjects\n",
    "- IFeval: Instruction-following capability\n",
    "- ARC: Logical & scientific reasoning\n",
    "- HellaSwag: Commonsense completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383befc1-e7b7-4540-af75-a3d5ce28786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tasks you want to evaluate the model on\n",
    "tasks = [\"mmlu\", \"arc_easy\", \"hellaswag\", \"ifeval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddc52-4acd-4230-9681-f84201f19e33",
   "metadata": {},
   "source": [
    "### Evaluating the Compressed Model\n",
    "\n",
    "**NOTE**: \n",
    "1. Running the evaluation on the entire list of tasks can take long. So for testing, you can use a single task instead.\n",
    "\n",
    "2. The results will be stored as a **results.pkl** file in the directories defined by **compressed_results_dir**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b759ca-440b-4491-abba-486fe2c1afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting directories\n",
    "compressed_model_path = \"../Llama_3.1_8B_Instruct_int8_dynamic\"\n",
    "compressed_results_dir = \"../results/compressed_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c849dd-535e-442c-82e9-d1fbeed500d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the compressed model and save results in pkl format\n",
    "comp_acc = evaluate(\n",
    "    compressed_model_path,\n",
    "    tasks,\n",
    "    limit=None,\n",
    "    batch_size=16,\n",
    "    apply_chat_template=True,\n",
    "    verbosity=None,\n",
    ")\n",
    "save_pickle(compressed_results_dir, comp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e51ac-f4c3-4276-b2fe-df595a963443",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_results = load_pickle(compressed_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79a881-9bcc-4860-a3a1-7b7da82d0138",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# print results for the compressed model\n",
    "print(make_table(comp_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a4f4c",
   "metadata": {},
   "source": [
    "Accuracy results for the compressed model:\n",
    "\n",
    "```text\n",
    "|                 Tasks                 |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
    "|---------------------------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
    "|arc_easy                               |      1|none  |     0|acc                    |↑  |0.8106|±  |0.0080|\n",
    "|                                       |       |none  |     0|acc_norm               |↑  |0.7555|±  |0.0088|\n",
    "|hellaswag                              |      1|none  |     0|acc                    |↑  |0.5734|±  |0.0049|\n",
    "|                                       |       |none  |     0|acc_norm               |↑  |0.7277|±  |0.0044|\n",
    "|ifeval                                 |      4|none  |     0|inst_level_loose_acc   |↑  |0.8549|±  |   N/A|\n",
    "|                                       |       |none  |     0|inst_level_strict_acc  |↑  |0.8237|±  |   N/A|\n",
    "|                                       |       |none  |     0|prompt_level_loose_acc |↑  |0.7893|±  |0.0175|\n",
    "|                                       |       |none  |     0|prompt_level_strict_acc|↑  |0.7449|±  |0.0188|\n",
    "|mmlu                                   |      2|none  |      |acc                    |↑  |0.6311|±  |0.0038|\n",
    "| - humanities                          |      2|none  |      |acc                    |↑  |0.5911|±  |0.0068|\n",
    "|  - formal_logic                       |      1|none  |     0|acc                    |↑  |0.4921|±  |0.0447|\n",
    "|  - high_school_european_history       |      1|none  |     0|acc                    |↑  |0.7697|±  |0.0329|\n",
    "|  - high_school_us_history             |      1|none  |     0|acc                    |↑  |0.7990|±  |0.0281|\n",
    "|  - high_school_world_history          |      1|none  |     0|acc                    |↑  |0.8186|±  |0.0251|\n",
    "|  - international_law                  |      1|none  |     0|acc                    |↑  |0.7686|±  |0.0385|\n",
    "|  - jurisprudence                      |      1|none  |     0|acc                    |↑  |0.7500|±  |0.0419|\n",
    "|  - logical_fallacies                  |      1|none  |     0|acc                    |↑  |0.7669|±  |0.0332|\n",
    "|  - moral_disputes                     |      1|none  |     0|acc                    |↑  |0.6792|±  |0.0251|\n",
    "|  - moral_scenarios                    |      1|none  |     0|acc                    |↑  |0.4369|±  |0.0166|\n",
    "|  - philosophy                         |      1|none  |     0|acc                    |↑  |0.6913|±  |0.0262|\n",
    "|  - prehistory                         |      1|none  |     0|acc                    |↑  |0.7191|±  |0.0250|\n",
    "|  - professional_law                   |      1|none  |     0|acc                    |↑  |0.4687|±  |0.0127|\n",
    "|  - world_religions                    |      1|none  |     0|acc                    |↑  |0.8363|±  |0.0284|\n",
    "| - other                               |      2|none  |      |acc                    |↑  |0.7132|±  |0.0079|\n",
    "|  - business_ethics                    |      1|none  |     0|acc                    |↑  |0.6500|±  |0.0479|\n",
    "|  - clinical_knowledge                 |      1|none  |     0|acc                    |↑  |0.7019|±  |0.0282|\n",
    "|  - college_medicine                   |      1|none  |     0|acc                    |↑  |0.6474|±  |0.0364|\n",
    "|  - global_facts                       |      1|none  |     0|acc                    |↑  |0.4100|±  |0.0494|\n",
    "|  - human_aging                        |      1|none  |     0|acc                    |↑  |0.6861|±  |0.0311|\n",
    "|  - management                         |      1|none  |     0|acc                    |↑  |0.7864|±  |0.0406|\n",
    "|  - marketing                          |      1|none  |     0|acc                    |↑  |0.8462|±  |0.0236|\n",
    "|  - medical_genetics                   |      1|none  |     0|acc                    |↑  |0.7700|±  |0.0423|\n",
    "|  - miscellaneous                      |      1|none  |     0|acc                    |↑  |0.8059|±  |0.0141|\n",
    "|  - nutrition                          |      1|none  |     0|acc                    |↑  |0.7614|±  |0.0244|\n",
    "|  - professional_accounting            |      1|none  |     0|acc                    |↑  |0.4965|±  |0.0298|\n",
    "|  - professional_medicine              |      1|none  |     0|acc                    |↑  |0.7721|±  |0.0255|\n",
    "|  - virology                           |      1|none  |     0|acc                    |↑  |0.5361|±  |0.0388|\n",
    "| - social sciences                     |      2|none  |      |acc                    |↑  |0.7394|±  |0.0077|\n",
    "|  - econometrics                       |      1|none  |     0|acc                    |↑  |0.4474|±  |0.0468|\n",
    "|  - high_school_geography              |      1|none  |     0|acc                    |↑  |0.7778|±  |0.0296|\n",
    "|  - high_school_government_and_politics|      1|none  |     0|acc                    |↑  |0.8187|±  |0.0278|\n",
    "|  - high_school_macroeconomics         |      1|none  |     0|acc                    |↑  |0.6487|±  |0.0242|\n",
    "|  - high_school_microeconomics         |      1|none  |     0|acc                    |↑  |0.7437|±  |0.0284|\n",
    "|  - high_school_psychology             |      1|none  |     0|acc                    |↑  |0.8606|±  |0.0149|\n",
    "|  - human_sexuality                    |      1|none  |     0|acc                    |↑  |0.7634|±  |0.0373|\n",
    "|  - professional_psychology            |      1|none  |     0|acc                    |↑  |0.6814|±  |0.0189|\n",
    "|  - public_relations                   |      1|none  |     0|acc                    |↑  |0.6636|±  |0.0453|\n",
    "|  - security_studies                   |      1|none  |     0|acc                    |↑  |0.6857|±  |0.0297|\n",
    "|  - sociology                          |      1|none  |     0|acc                    |↑  |0.8408|±  |0.0259|\n",
    "|  - us_foreign_policy                  |      1|none  |     0|acc                    |↑  |0.8600|±  |0.0349|\n",
    "| - stem                                |      2|none  |      |acc                    |↑  |0.5043|±  |0.0084|\n",
    "|  - abstract_algebra                   |      1|none  |     0|acc                    |↑  |0.2500|±  |0.0435|\n",
    "|  - anatomy                            |      1|none  |     0|acc                    |↑  |0.6444|±  |0.0414|\n",
    "|  - astronomy                          |      1|none  |     0|acc                    |↑  |0.6842|±  |0.0378|\n",
    "|  - college_biology                    |      1|none  |     0|acc                    |↑  |0.7431|±  |0.0365|\n",
    "|  - college_chemistry                  |      1|none  |     0|acc                    |↑  |0.4500|±  |0.0500|\n",
    "|  - college_computer_science           |      1|none  |     0|acc                    |↑  |0.4200|±  |0.0496|\n",
    "|  - college_mathematics                |      1|none  |     0|acc                    |↑  |0.2700|±  |0.0446|\n",
    "|  - college_physics                    |      1|none  |     0|acc                    |↑  |0.3824|±  |0.0484|\n",
    "|  - computer_security                  |      1|none  |     0|acc                    |↑  |0.7300|±  |0.0446|\n",
    "|  - conceptual_physics                 |      1|none  |     0|acc                    |↑  |0.6000|±  |0.0320|\n",
    "|  - electrical_engineering             |      1|none  |     0|acc                    |↑  |0.6069|±  |0.0407|\n",
    "|  - elementary_mathematics             |      1|none  |     0|acc                    |↑  |0.4048|±  |0.0253|\n",
    "|  - high_school_biology                |      1|none  |     0|acc                    |↑  |0.7774|±  |0.0237|\n",
    "|  - high_school_chemistry              |      1|none  |     0|acc                    |↑  |0.4729|±  |0.0351|\n",
    "|  - high_school_computer_science       |      1|none  |     0|acc                    |↑  |0.5800|±  |0.0496|\n",
    "|  - high_school_mathematics            |      1|none  |     0|acc                    |↑  |0.2519|±  |0.0265|\n",
    "|  - high_school_physics                |      1|none  |     0|acc                    |↑  |0.3444|±  |0.0388|\n",
    "|  - high_school_statistics             |      1|none  |     0|acc                    |↑  |0.4213|±  |0.0337|\n",
    "|  - machine_learning                   |      1|none  |     0|acc                    |↑  |0.4732|±  |0.0474|\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
