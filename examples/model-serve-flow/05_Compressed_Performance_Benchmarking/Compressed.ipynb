{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4d68b1-dd85-4056-ac9a-752d04b49b54",
   "metadata": {},
   "source": [
    "## Compressed Model Performance Benchmarking Using GuideLLM\n",
    "\n",
    "This notebook focuses on evaluating the system-level performance of the **compressed model**. The results are used to understand how compression affects latency, throughput, and scalability compared to the uncompressed baseline.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Assess the performance efficiency of the compressed model and quantify the gains or trade-offs introduced by compression under realistic serving conditions.\n",
    "\n",
    "**GuideLLM Overview**\n",
    "\n",
    "GuideLLM is an open-source benchmarking tool used to measure the performance of large language models deployed with **vLLM**. It captures detailed system and inference-level metrics, including:\n",
    "\n",
    "- *Token throughput*\n",
    "- *Latency metrics*\n",
    "  - Time to First Token (TTFT)\n",
    "  - Inter-Token Latency (ITL)\n",
    "  - End-to-end request latency\n",
    "- *Concurrency behavior*\n",
    "- *Request-level diagnostics*\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "To run performance benchmarking using GuideLLM, we first need to start a vLLM server to host the base model.\n",
    "\n",
    "More details on system level performance benchmarking and GuideLLM are provided in [System_Level_Performance_Benchmarking.md](../docs/System_Level_Performance_Benchmarking.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3f366",
   "metadata": {},
   "source": [
    "### Install Depoendencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to install dependencies if dependencies were not installed in 02_Base_Performance_Benchmarking/Base.ipynb\n",
    "# !pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8d94d-10ad-4ed4-aba0-a3c71ca4254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from guidellm.benchmark import GenerativeBenchmarksReport\n",
    "from utils import generate, stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c27e36",
   "metadata": {},
   "source": [
    "### Launch an Inference Server (vLLM) for the compressed Model\n",
    "\n",
    "Set up a vLLM inference server to host the compressed model and expose an OpenAI-compatible API endpoint. This server is required so that GuideLLM can benchmark system-level performance like throughput, latency, and time-to-first-token. The performance benchmarks of the base and compressed models will be used later on to draw comparisons.\n",
    "\n",
    "The compressed model will be accessible via an API for performance evaluation.\n",
    "\n",
    "**Resources used** : 46GB L40S GPU x 1\n",
    "\n",
    "More details on vLLM are provided in [Model_Serving_vLLM.md](../docs/Vllm_Server_README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42169bb",
   "metadata": {},
   "source": [
    "####  Set up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the logging level for vLLM inference\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"DEBUG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b9619",
   "metadata": {},
   "source": [
    "**Before starting this notebook, use `nvidia-smi` and then `kill -9 <pid>` to kill any running processes that might be consuming GPU memory.**\n",
    "\n",
    "#### vLLM config for single node\n",
    "\n",
    "We will be using the configuration for a single-node, single-GPU set up to launch a vLLM server for the base model. \n",
    "\n",
    "Run the following command in terminal to serve the base model using vLLM\n",
    "\n",
    "- The configuration used to serve the compressed model and the base model (in the [Base.ipynb](../02_Base_Performance_Benchmarking/Base.ipynb) notebook) is the same other than the model name and port.\n",
    "- Make sure to run this command from the `05_Compressed_Performance_Benchmarking` directory\n",
    "\n",
    "  \n",
    "```bash\n",
    "vllm serve \\\n",
    "  \"../Llama_3.1_8B_Instruct_int8_dynamic\" \\\n",
    "  --host 127.0.0.1 \\\n",
    "  --port 8001 \\\n",
    "  --gpu-memory-utilization 0.6 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  --pipeline-parallel-size 1 \\\n",
    "  --max-model-len 2048\n",
    "```\n",
    "\n",
    "Once the server starts, you will see something like this:\n",
    "\n",
    "```INFO:     Started server process [166518]```\\\n",
    "```INFO:     Waiting for application startup.```\\\n",
    "```INFO:     Application startup complete.```\n",
    "\n",
    "\n",
    "**NOTE** You may encounter the following warning when executing the cell below:\n",
    "`The tokenizer you are loading from '../base_model' with an incorrect regex pattern... This will lead to incorrect tokenization.`\n",
    "This warning can be ignored safely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7bcbb",
   "metadata": {},
   "source": [
    "#### A test run to see if the vLLM server is accessible\n",
    "We use a helper function **generate** (defined in [utils.py](./utils.py)) to simplify sending requests to our locally-served vLLM model.\n",
    "\n",
    "This function wraps the OpenAI-compatible Chat Completions API exposed by vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe89e66",
   "metadata": {
    "tags": [
     "{   \"keep_output\": true }",
     "keep_output"
    ]
   },
   "outputs": [],
   "source": [
    "# For non streaming results\n",
    "response = generate(\n",
    "    model=\"../Llama_3.1_8B_Instruct_int8_dynamic\",\n",
    "    prompt=\"What is photosynthesis?\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8001,\n",
    "    api_key=\"empty\",\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212b644",
   "metadata": {
    "tags": [
     "{   \"keep_output\": true }"
    ]
   },
   "outputs": [],
   "source": [
    "# For streaming results\n",
    "res = \"\"\n",
    "for chunk in stream(\n",
    "    model=\"../Llama_3.1_8B_Instruct_int8_dynamic\",\n",
    "    prompt=\"What is photosynthesis?\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8001,\n",
    "    api_key=\"empty\",\n",
    "    max_tokens=512,\n",
    "):\n",
    "    res += chunk\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36dd2c8",
   "metadata": {},
   "source": [
    "## Checking GPU vRAM\n",
    "Loading the compressed model with the configuration defined in the above command will take approximately 28GB. It may seem surprising that a compressed 8.5 GB model consumes ~118 GB GPU memory. This is expected behavior in vLLM, due to how memory is allocated during inference. The main contributors are:\n",
    "\n",
    "1. **Model Weights (~8.5 GB)**\n",
    "\n",
    "    The size of your compressed model stored on disk (INT8, FP16, etc.). \n",
    "    Loaded once into GPU memory.\n",
    "   \n",
    "2. **Runtime GPU Memory (~6 GB)**\n",
    "\n",
    "- vLLM reserves extra memory for:\n",
    "\n",
    "- Parameter sharding\n",
    "\n",
    "- CUDA kernels\n",
    "\n",
    "- Attention buffers and temporary tensors\n",
    "\n",
    "- Weight adapters and padded tensors\n",
    "\n",
    "This adds ~4–8 GB depending on the model.\n",
    "\n",
    "3. **KV Cache (~14 GB)**\n",
    "\n",
    "- Stores key/value tensors for each generated token to avoid recomputation.\n",
    "\n",
    "- Memory grows with sequence length, model hidden size, and concurrency.\n",
    "\n",
    "- vLLM presets a large KV cache to support batching efficiently.\n",
    "\n",
    "\n",
    "4. **GPU Memory Utilization Flag (--gpu-memory-utilization)**\n",
    "\n",
    "``--gpu-memory-utilization`` is set to 0.6, meaning vLLM can utilize 60% of the total GPU memory. In this case, we have used one 46GB LS40 GPU, 60% of 46 is approx 28."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ed260-2a78-4d46-aa48-f763fa257bc7",
   "metadata": {},
   "source": [
    "### Run Performance Benchmarking\n",
    "\n",
    "Now that the **vLLM server for the compressed model** has been started, we can proceed with benchmarking its performance using **GuideLLM**.\n",
    "\n",
    "Identify the following parameters:\n",
    "\n",
    "- **target**: URL of the vLLM inference server started in the previous step  \n",
    "  (e.g., `http://127.0.0.1:8001`)\n",
    "\n",
    "- **output-path**: Path where benchmarking results will be saved\n",
    "\n",
    "If needed, adjust the `target`, `output-path`, or benchmarking profile in the command below, then run it in a terminal.\n",
    "\n",
    "**NOTES**:\n",
    "\n",
    "- Ensure the vLLM server for the compressed model is running before executing the benchmark.\n",
    "- If the vLLM server is running on a different port, update the `target` accordingly.\n",
    "- Make sure you run the following command from the `05_Compressed_Performance_Benchmarking` directory.\n",
    "- The same benchmarking configuration will be reused for evaluating the compressed model.\n",
    "\n",
    "```bash\n",
    "guidellm benchmark \\\n",
    "  --target \"http://127.0.0.1:8001\" \\\n",
    "  --profile sweep \\\n",
    "  --max-seconds 120 \\\n",
    "  --data \"prompt_tokens=1024,output_tokens=512\" \\\n",
    "  --output-path \"../results/compressed_performance_benchmarks.json\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f7854-3d9d-4124-8be2-31a2e4fad8ed",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The above command will result is multiple tables.\n",
    "\n",
    "1. **Request Latency Statistics (Completed Requests)**\n",
    "\n",
    "This table focuses on how **long** requests take and the latency characteristics of the server.\n",
    "\n",
    "```text\n",
    "ℹ Request Latency Statistics (Completed Requests)\n",
    "|=============|=========|========|=========|=========|======|======|=======|=======|\n",
    "| Benchmark   | Request Latency || TTFT             || ITL        || TPOT         ||\n",
    "| Strategy    | Sec             || ms               || ms         || ms           ||\n",
    "|             | Mdn     | p95    | Mdn     | p95     | Mdn  | p95  | Mdn   | p95   |\n",
    "|-------------|---------|--------|---------|---------|------|------|-------|-------|\n",
    "| synchronous | 7.6     | 7.9    | 87.9    | 445.7   | 14.7 | 14.7 | 14.8  | 15.5  |\n",
    "| throughput  | 70.4    | 74.8   | 36149.3 | 40360.5 | 63.9 | 99.4 | 137.4 | 146.1 |\n",
    "| constant    | 8.3     | 8.3    | 99.4    | 108.0   | 16.1 | 16.1 | 16.2  | 16.3  |\n",
    "| constant    | 8.9     | 8.9    | 99.1    | 107.2   | 17.2 | 17.3 | 17.4  | 17.4  |\n",
    "| constant    | 9.7     | 9.8    | 104.4   | 113.0   | 18.8 | 18.9 | 19.0  | 19.1  |\n",
    "| constant    | 10.5    | 10.6   | 104.9   | 114.6   | 20.4 | 20.5 | 20.6  | 20.6  |\n",
    "| constant    | 11.7    | 11.8   | 106.9   | 118.1   | 22.7 | 22.8 | 22.8  | 23.0  |\n",
    "| constant    | 12.7    | 12.8   | 108.3   | 119.3   | 24.7 | 24.8 | 24.9  | 24.9  |\n",
    "| constant    | 16.0    | 18.5   | 121.6   | 959.9   | 31.1 | 34.7 | 31.3  | 36.1  |\n",
    "| constant    | 17.8    | 18.1   | 119.7   | 136.0   | 34.5 | 35.2 | 34.7  | 35.4  |\n",
    "|=============|=========|========|=========|=========|======|======|=======|=======|\n",
    "\n",
    "```\n",
    "\n",
    "2.  **Server Throughput Statistics**\n",
    "\n",
    "This table focuses on how many requests a server can handle per second. Throughput can be thought of as the **rate** (or time required) of processing. \n",
    "```text\n",
    "Server Throughput Statistics\n",
    "|=============|=====|======|=======|=======|========|========|=======|========|=======|========|\n",
    "| Benchmark   | Requests                |||| Input Tokens   || Output Tokens || Total Tokens  ||\n",
    "| Strategy    | Per Sec   || Concurrency  || Per Sec        || Per Sec       || Per Sec       ||\n",
    "|             | Mdn | Mean | Mdn   | Mean  | Mdn    | Mean   | Mdn   | Mean   | Mdn   | Mean   |\n",
    "|-------------|-----|------|-------|-------|--------|--------|-------|--------|-------|--------|\n",
    "| synchronous | 0.1 | 0.1  | 1.0   | 1.0   | 139.6  | 148.9  | 68.2  | 67.6   | 68.2  | 207.3  |\n",
    "| throughput  | 0.6 | 2.6  | 194.0 | 152.8 | 123.1  | 4262.7 | 966.7 | 1369.8 | 971.8 | 4200.5 |\n",
    "| constant    | 0.4 | 0.4  | 4.0   | 3.3   | 456.2  | 465.9  | 217.6 | 209.7  | 217.8 | 643.0  |\n",
    "| constant    | 0.7 | 0.7  | 6.0   | 6.1   | 779.7  | 789.8  | 326.7 | 353.9  | 327.1 | 1085.1 |\n",
    "| constant    | 1.0 | 1.0  | 10.0  | 9.3   | 1103.9 | 1113.8 | 422.1 | 495.3  | 422.3 | 1518.7 |\n",
    "| constant    | 1.3 | 1.2  | 14.0  | 12.8  | 1426.6 | 1437.8 | 498.8 | 634.7  | 499.5 | 1946.3 |\n",
    "| constant    | 1.7 | 1.5  | 19.0  | 17.3  | 1753.6 | 1761.9 | 629.9 | 770.0  | 630.6 | 2361.2 |\n",
    "| constant    | 2.0 | 1.7  | 25.0  | 22.0  | 2078.6 | 2085.8 | 746.2 | 901.6  | 747.0 | 2764.8 |\n",
    "| constant    | 2.3 | 2.0  | 36.0  | 32.4  | 2401.2 | 2674.7 | 783.9 | 1110.9 | 786.0 | 3406.4 |\n",
    "| constant    | 2.5 | 2.2  | 44.0  | 37.5  | 2733.0 | 2733.5 | 829.7 | 1123.5 | 831.5 | 3445.2 |\n",
    "|=============|=====|======|=======|=======|========|========|=======|========|=======|========|\n",
    "\n",
    "\n",
    "```\n",
    "#### Compressed Model Performance Summary\n",
    "1. Max concurrency under load: 44.0 (Concurrency Mdn)\n",
    "2. Max output tokens per second under load: 829.7 (Output tokens per sec Mdn)\n",
    "3. Request latency under load: 17.8 (Request Latency in secs Mdn)\n",
    "4. Time to first token under load: 119.7 (TTFT ms Mdn)\n",
    "5. Inter token latency under load: 34.5 (ITL ms Mdn)\n",
    "\n",
    "\n",
    "#### SLO Analysis\n",
    "\n",
    "Assume the Service Level Objective (SLO) is:\n",
    "\n",
    "    TTFT ≤ 200 milliseconds for 95% of requests (p95) with optimal concurrency\n",
    "\n",
    "At the highest tested concurrency of **44 requests**, the compressed model achieves a **p95 TTFT of 136.0 ms**, which comfortably satisfies the SLO.\n",
    "\n",
    "This configuration meets the TTFT SLO of 200 ms for 95% of requests. Increasing concurrency beyond this point may push p95 TTFT above the SLO threshold and should be evaluated carefully in production scenarios.\n",
    "\n",
    "For a workload of **1024 input tokens and 512 output tokens**, the system can sustain approximately **37–44 concurrent requests** while remaining within the TTFT ≤ 200 ms SLO. Reducing input and output token lengths (e.g., 512/256) allows the system to support more concurrent requests while maintaining compliance with the SLO.\n",
    "\n",
    "#### Comparison with Base Model Performance\n",
    "\n",
    "| Metric | Base Model | Compressed Model |\n",
    "|------|-----------|------------------|\n",
    "| p95 TTFT (ms) | 162.4 ms | **136.0 ms** |\n",
    "| Max concurrency under SLO | 34 requests | **44 requests** |\n",
    "| SLO satisfied | Yes | Yes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0c53b-3f15-460a-a6ea-785e3eb92609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after the benchmarking process in the terminal completes\n",
    "report = GenerativeBenchmarksReport.load_file(\n",
    "    path=\"../results/compressed_performance_benchmarks.json\",\n",
    ")\n",
    "compressed_benchmarks = report.benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8ca06-42c8-4487-b0ac-de3a65449ad4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "compressed_benchmarks[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
