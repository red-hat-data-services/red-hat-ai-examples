{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7375209-d552-4543-9e7f-1c9a03117314",
   "metadata": {},
   "source": [
    "## Compress the Base LLM using LLM Compressor:\n",
    "Now that we have established **accuracy and performance baselines** for the base model (`Llama-3.1-8B-Instruct`), the next step is to apply **model compression**. Specifically, we will quantize the base model and later evaluate the compressed version for both accuracy and system-level performance.\n",
    "\n",
    "This will allow us to assess whether model compression improves deployment metrics—such as **Time to First Token (TTFT)** while maintaining accuracy comparable to the base model.\n",
    "\n",
    "\n",
    "This step focuses on **reducing the model’s memory footprint** through quantization, resulting in a smaller and more deployment-efficient model.\n",
    "\n",
    "**Note**: We use **data-aware quantization**, which relies on representative calibration data to preserve model quality.\n",
    "\n",
    "**Goal**: Reduce model size (e.g., FP16 → INT8 / INT4) while retaining accuracy and inference performance.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- Load the base model.\n",
    "\n",
    "- Measure its size and memory usage.\n",
    "\n",
    "- Use a calibration dataset (e.g., WikiText, UltraChat) to collect activation statistics.\n",
    "\n",
    "- Apply a quantization recipe (e.g., SmoothQuant + GPTQ modifier).\n",
    "\n",
    "- Save the compressed model and verify size reduction.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Compressed model saved on disk.\n",
    "\n",
    "- Model size reduced, typically by 50% (depending on quantization scheme)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69ebf9",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22405c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install .\n",
    "!pip install torch==2.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0aac4-4079-41af-a087-ac52602d1782",
   "metadata": {},
   "source": [
    "Running the above cell might return errors like `ERROR: pip's dependency resolver does not currently take into account... llmcompressor 0.8.1 requires torch<=2.8.0,>=2.7.0, but you have torch 2.9.0 which is incompatible.` which can be safely ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d76ec6-9bab-4a4b-bda8-0e5663510937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from utils import model_size_gb, tokenize_for_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb9620-ca7b-44e4-bc75-b2de8bd992c1",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6585cb-614e-403b-beb1-36a9c03f66b8",
   "metadata": {},
   "source": [
    "### Loading Base Model\n",
    "\n",
    "**Make sure to kill any running processes using `nvidia-smi` and `kill -9 <pid>` that might be consuming GPU memory.**\n",
    "\n",
    "While loading the model using **from_pretrained** using transformers' **AutoModelForCausalLM** class, we specify the data type using the **torch_dtype** parameter and set it to **auto** so the model is loaded in the data type specified in its config.\n",
    "Otherwise, PyTorch loads the weights in **full precision (fp32)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799031ae-0932-4924-86e6-68d7917c482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables\n",
    "base_model_path = \"../base_model\"\n",
    "compressed_model_path = \"../Llama_3.1_8B_Instruct_int8_dynamic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb655abf-5d22-49e4-b278-eb5d92fa26d5",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loading model and tokenizer from huggingfaceabs\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "print(\"Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725441f-ed72-44fd-8f6d-8db56b637049",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the base model is: 14.9575GB\n"
     ]
    }
   ],
   "source": [
    "# check model size\n",
    "# !du -sh {base_model_path}\n",
    "model_size = model_size_gb(model)\n",
    "print(f\"The size of the base model is: {model_size:.4f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b8093-b984-4d58-a552-65692dc7944c",
   "metadata": {},
   "source": [
    "### Preparing Calibration Dataset\n",
    "\n",
    "Since we are using data-aware quantization to compress the base model, we need a dataset to calibrate the model with real or representative inputs. For the sake of this example, we will use a small, general-purpose dataset for faster processing. Specifically, we use the `wikitext-2-raw-v1` version of the WikiText dataset which is the smaller version.  More information on why to use a calibration dataset is provided in [Compression.md](../docs/Compression.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6dd2e2-58d1-47e8-b17b-2a0d78627e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset to use for calibration\n",
    "dataset_id = \"wikitext\"\n",
    "\n",
    "# Specify the configuration / version of the dataset\n",
    "config = \"wikitext-2-raw-v1\"  # Small version (~2M tokens), raw text format\n",
    "\n",
    "# Set the number of calibration samples based on available device\n",
    "# - On GPU: use more samples to get more accurate activation statistics\n",
    "# - On CPU: reduce samples to prevent memory issues and keep demo fast\n",
    "num_calibration_samples = 512 if device == \"cuda\" else 16\n",
    "\n",
    "# Set the maximum sequence length for calibration\n",
    "max_sequence_length = 1024 if device == \"cuda\" else 16\n",
    "\n",
    "# Load the dataset using Hugging Face Datasets API\n",
    "# This downloads train split of the dataset\n",
    "ds = load_dataset(dataset_id, config, split=\"train\")\n",
    "# Shuffle and grab only the number of samples we need\n",
    "ds = ds.shuffle(seed=42).select(range(num_calibration_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b24ce-6980-4aff-937c-e0276eb58a6a",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in the wikitext: ['text']\n",
      "\n",
      "{'text': ' Continuous , short @-@ arc , high pressure xenon arc lamps have a color temperature closely approximating noon sunlight and are used in solar simulators . That is , the chromaticity of these lamps closely approximates a heated black body radiator that has a temperature close to that observed from the Sun . After they were first introduced during the 1940s , these lamps began replacing the shorter @-@ lived carbon arc lamps in movie projectors . They are employed in typical 35mm , IMAX and the new digital projectors film projection systems , automotive HID headlights , high @-@ end \" tactical \" flashlights and other specialized uses . These arc lamps are an excellent source of short wavelength ultraviolet radiation and they have intense emissions in the near infrared , which is used in some night vision systems . \\n'}\n"
     ]
    }
   ],
   "source": [
    "# inspect the dataset\n",
    "print(f\"columns in the {dataset_id}: {ds.column_names}\\n\")\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e60f8-6420-41aa-91da-266b2ce828c1",
   "metadata": {},
   "source": [
    "**Datset inspection shows the we need to extract column ```text``` and pass it as input to the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf853df6-2fbd-4a95-ae92-52c7b11577a4",
   "metadata": {},
   "source": [
    "### When to Use a Custom Template for Calibration\n",
    "\n",
    "Use a **custom template** when you want the calibration text to closely mimic the input format your model will see in production.  \n",
    "\n",
    "For example, if your model is **instruction-following** or **chat-based**, providing the template the model was originally trained on or the template that will be used during inference ensures that the activation statistics collected during calibration reflect realistic usage patterns. \n",
    "\n",
    "This can improve the accuracy of quantization and compression.\n",
    "\n",
    "If your model can handle raw text and doesn’t require a specific format, you can rely on the default template instead.\n",
    "\n",
    "A custom template can be provided to the `tokenize_for_calibration` function using the `custom_template` argument. It accepts the following format:\n",
    "\n",
    "```python\n",
    "custom_template = {\n",
    " \"template_text\": \"Instruction: {content}\\nOutput:\", \n",
    " \"placeholder\": \"content\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54934efc-9564-446c-a37b-daf58dcb5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get activations for the calibration dataset, we need to:\n",
    "# 1. extract the samples from the dataset\n",
    "# 2. tokenize samples in the dataset\n",
    "input_column = \"text\"\n",
    "\n",
    "# Call tokenize_for_calibration using dataset.map\n",
    "tokenized_dataset = ds.map(\n",
    "    lambda batch: tokenize_for_calibration(\n",
    "        examples=batch,  # batch from Hugging Face dataset\n",
    "        input_column=input_column,  # the column containing text to calibrate\n",
    "        tokenizer=tokenizer,  # your Hugging Face tokenizer\n",
    "        max_length=max_sequence_length,  # maximum sequence length\n",
    "        model_type=\"chat\",  # use chat template if no custom template\n",
    "        custom_template=None,  # optional, provide a dict if you want a custom template\n",
    "    ),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea72af-71a5-4cc8-91fc-69bf7b642b45",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 512\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5357d71-6eed-40fa-972b-d7e428422414",
   "metadata": {},
   "source": [
    "### Quantizing/Compressing Base Model to INT8\n",
    "After preparing the dataset for calibration, we define a recipe for quantization. For quantization scheme `W8A8-INT8`, we use `SmoothQuantModifier` followed by `GPTQModifier`.\n",
    "\n",
    "More details on what SmoothQUant and GPTQ algorithms are provided in [Compression.md](Compression.md).\n",
    "\n",
    "Running the cell below to compress the model may take a significant amount of time (approximately one hour), as the model is executed on a calibration dataset to collect and calibrate activation statistics prior to applying quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b5686-60b4-42f1-991a-21260466797c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the quantization scheme\n",
    "scheme = \"W8A8\"  # W8A8 means 8-bit weights and 8-bit activations\n",
    "\n",
    "# Strength for SmoothQuant smoothing\n",
    "# This controls how much the activation values are smoothed to reduce outliers\n",
    "smoothing_strength = 0.8\n",
    "\n",
    "# Create SmoothQuant modifier\n",
    "# - smooths activations before quantization to improve stability and reduce degradation\n",
    "smooth_quant = SmoothQuantModifier(smoothing_strength=smoothing_strength)\n",
    "\n",
    "# Create GPTQ modifier\n",
    "# - targets=\"Linear\" quantizes only Linear layers (e.g., feedforward layers)\n",
    "# - scheme=scheme uses the W8A8 quantization scheme\n",
    "# - ignore=[\"lm_head\"] preserves the LM head to avoid generation quality loss\n",
    "quantizer = GPTQModifier(targets=\"Linear\", scheme=scheme, ignore=[\"lm_head\"])\n",
    "\n",
    "# Combine the modifiers into a recipe list\n",
    "# The order matters: first apply SmoothQuant, then GPTQ\n",
    "recipe = [smooth_quant, quantizer]\n",
    "\n",
    "# Perform quantization\n",
    "oneshot(\n",
    "    model=base_model_path,  # Model to quantize\n",
    "    dataset=tokenized_dataset,  # Calibration dataset, used for both SmoothQuant & GPTQ\n",
    "    recipe=recipe,  # List of quantization modifiers to apply\n",
    "    output_dir=compressed_model_path,  # Directory to save the quantized model\n",
    "    max_seq_length=2048,  # Maximum sequence length for calibration\n",
    "    num_calibration_samples=512,  # Number of samples used for calibration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf956831",
   "metadata": {},
   "source": [
    "### Checking model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56836404-daab-4638-8abf-957b71f82507",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 224it [00:00, 1292.67it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.45it/s]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (GB): 8.460090637207031\n"
     ]
    }
   ],
   "source": [
    "# Load quantized model\n",
    "model_quant = AutoModelForCausalLM.from_pretrained(compressed_model_path)\n",
    "model_quant.config.dtype = model.config.torch_dtype\n",
    "model_quant.save_pretrained(compressed_model_path)\n",
    "model_size = model_size_gb(model_quant)\n",
    "print(f\"Model size (GB): {model_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882026b-2c4c-4e1f-b20f-75fdaf71b7f3",
   "metadata": {},
   "source": [
    "### Observation\n",
    "After quantizing the model, the size has clearly reduced from 14.9GB to 8GB. \n",
    "\n",
    "\n",
    "**ALTERNATIVELY**, llm-compressor also supports FP8 quantization. This conversion does not require any calibration dataset. Uncomment the below cell to quantize the model to FP8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c84a3-83b0-4d28-9665-13e4f1ef5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe = QuantizationModifier(\n",
    "#   targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"])\n",
    "\n",
    "# oneshot(model=model_name, recipe=recipe, output_dir=compressed_model_path)\n",
    "\n",
    "# # Load quantized model\n",
    "# model_quant = AutoModelForCausalLM.from_pretrained(compressed_model_path)\n",
    "# model_quant.config.dtype = model.config.torch_dtype\n",
    "# model_quant.save_pretrained(compressed_model_path)\n",
    "# model_size = model_size_gb(model_quant)\n",
    "# print(f\"Model size (GB): {model_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94794d74",
   "metadata": {},
   "source": [
    "Now that we have reduced the model size, the next step is to evaluate this compressed model to make sure the accuracy has retained after compression. This is done in step `04_Compressed_Accuracy_Benchmarking`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
