{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4d68b1-dd85-4056-ac9a-752d04b49b54",
   "metadata": {},
   "source": [
    "## Base Model Performance Benchmarking Using GuideLLM\n",
    "\n",
    "This notebook evaluates the system-level performance of the **base model**. The results from this notebook serve as a baseline for comparing performance against the compressed model deployment.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Establish baseline performance metrics for the base model to enable a direct comparison with the compressed model under identical serving conditions.\n",
    "\n",
    "**GuideLLM Overview**\n",
    "\n",
    "GuideLLM is an open-source benchmarking tool designed to evaluate the performance of LLMs served through **vLLM**. It provides fine-grained system and inference metrics, including:\n",
    "\n",
    "- *Token throughput*\n",
    "- *Latency metrics*\n",
    "  - Time to First Token (TTFT)\n",
    "  - Inter-Token Latency (ITL)\n",
    "  - End-to-end request latency\n",
    "- *Concurrency scaling*\n",
    "- *Request-level diagnostics*\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "To run performance benchmarking using GuideLLM, we first need to start a vLLM server to host the base model.\n",
    "\n",
    "More details on system level performance benchmarking and GuideLLM are provided in [System_Level_Performance_Benchmarking.md](../docs/System_Level_Performance_Benchmarking.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2c741",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8d94d-10ad-4ed4-aba0-a3c71ca4254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from guidellm.benchmark import GenerativeBenchmarksReport\n",
    "from utils import generate, stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9effa5",
   "metadata": {},
   "source": [
    "### Launch an Inference Server (vLLM) for the base Model\n",
    "\n",
    "Set up a vLLM inference server to host your base model and expose an OpenAI-compatible API endpoint. This server is required so that GuideLLM can benchmark system-level performance like throughput, latency, and time-to-first-token. The performance benchmarks of the base and compressed models will be used later on to draw comparisons.\n",
    "\n",
    "The base model will be accessible via an API for performance evaluation.\n",
    "\n",
    "**Resources used** : 46GB L40S GPU x 1\n",
    "\n",
    "More details on vLLM are provided in [Model_Serving_vLLM.md](../docs/Model_Serving_vLLM.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaa0bf",
   "metadata": {},
   "source": [
    "####  Set up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the logging level for vLLM inference\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"DEBUG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4a108",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Before starting this notebook, use `nvidia-smi` and then `kill -9 <pid>` to kill any running processes that might be consuming GPU memory.**\n",
    "\n",
    "#### vLLM config for single node\n",
    "\n",
    "We will be using the configuration for a single-node, single-GPU set up to launch a vLLM server for the base model. \n",
    "\n",
    "Run the following command in terminal to serve the base model using vLLM\n",
    "\n",
    "- The configuration used to serve the base model and the compressed model (in the [Compressed.ipynb](../05_Compressed_Performance_Benchmarking/Compressed.ipynb) notebook) is the same other than the model name and port.\n",
    "- Make sure to run this command from the `02_Base_Performance_Benchmarking` directory. If running from a different directory, make sure to provide the correct model path.\n",
    "\n",
    "```bash\n",
    "vllm serve \\\n",
    "  \"../base_model/RedHatAI-Llama-3.1-8B-Instruct\" \\\n",
    "  --host 127.0.0.1 \\\n",
    "  --port 8000 \\\n",
    "  --gpu-memory-utilization 0.6 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  --pipeline-parallel-size 1 \\\n",
    "  --max-model-len 2048\n",
    "```\n",
    "\n",
    "Once the server starts, you will see something like this:\n",
    "\n",
    "```INFO:     Started server process [166518]```\\\n",
    "```INFO:     Waiting for application startup.```\\\n",
    "```INFO:     Application startup complete.```\n",
    "\n",
    "**NOTE** You may encounter the following warning when serving the model:\n",
    "`The tokenizer you are loading from '../base_model/RedHatAI-Llama-3.1-8B-Instruct' with an incorrect regex pattern... This will lead to incorrect tokenization.`\n",
    "This warning can be ignored safely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1d4e9",
   "metadata": {},
   "source": [
    "#### A test run to see if the vLLM server is accessible\n",
    "We use a helper function **generate** (defined in [utils.py](./utils.py)) to simplify sending requests to our locally-served vLLM model.\n",
    "\n",
    "This function wraps the OpenAI-compatible Chat Completions API exposed by vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd308e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"../base_model/RedHatAI-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8becdf",
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is a vital process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose. This process is essential for life on Earth, as it provides the primary source of energy and organic compounds for nearly all living organisms.\n",
      "\n",
      "The word \"photosynthesis\" comes from the Greek words \"photo\" (light) and \"synthesis\" (putting together). During photosynthesis, plants use energy from sunlight, water, and carbon dioxide to produce glucose and oxygen. The overall equation for photosynthesis is:\n",
      "\n",
      "6 CO2 (carbon dioxide) + 6 H2O (water) + light energy → C6H12O6 (glucose) + 6 O2 (oxygen)\n",
      "\n",
      "Here's a simplified overview of the process:\n",
      "\n",
      "1. **Light absorption**: Plants absorb light energy from the sun through specialized pigments such as chlorophyll.\n",
      "2. **Water absorption**: Plants absorb water from the soil through their roots.\n",
      "3. **Carbon dioxide absorption**: Plants absorb carbon dioxide from the atmosphere through small openings on their leaves called stomata.\n",
      "4. **Light-dependent reactions**: Light energy is absorbed and used to generate ATP and NADPH, which are energy-rich molecules.\n",
      "5. **Light-independent reactions**: The energy from ATP and NADPH is used to convert carbon dioxide into glucose through a series of chemical reactions.\n",
      "6. **Glucose production**: Glucose is produced and stored in the plant's cells, providing energy for growth and development.\n",
      "7. **Oxygen release**: Oxygen is released into the atmosphere as a byproduct of photosynthesis.\n",
      "\n",
      "Photosynthesis is essential for life on Earth, as it:\n",
      "\n",
      "* Provides energy for plants to grow and develop\n",
      "* Produces oxygen for animals to breathe\n",
      "* Supports the food chain by providing energy for herbivores and carnivores\n",
      "* Regulates the Earth's climate by removing carbon dioxide from the atmosphere\n",
      "\n",
      "In summary, photosynthesis is the process by which plants convert light energy into chemical energy, producing glucose and oxygen, and supporting life on Earth.\n"
     ]
    }
   ],
   "source": [
    "# For non streaming results\n",
    "response = generate(\n",
    "    model=base_model_path,\n",
    "    prompt=\"What is photosynthesis?\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8000,\n",
    "    api_key=\"empty\",\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb30bb",
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is a vital process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose. This process is essential for life on Earth, as it provides the primary source of energy and organic compounds for nearly all living organisms.\n",
      "\n",
      "During photosynthesis, plants use energy from sunlight, water, and carbon dioxide to produce glucose (a type of sugar) and oxygen. The overall equation for photosynthesis is:\n",
      "\n",
      "6 CO2 (carbon dioxide) + 6 H2O (water) + light energy → C6H12O6 (glucose) + 6 O2 (oxygen)\n",
      "\n",
      "Here's a simplified overview of the process:\n",
      "\n",
      "1. **Light absorption**: Chlorophyll, a green pigment found in plant cells, absorbs light energy from the sun.\n",
      "2. **Water absorption**: Plants absorb water from the soil through their roots.\n",
      "3. **Carbon dioxide absorption**: Plants absorb carbon dioxide from the air through small openings on their leaves called stomata.\n",
      "4. **Light-dependent reactions**: Light energy is used to convert water and carbon dioxide into ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate).\n",
      "5. **Calvin cycle**: The ATP and NADPH produced in the light-dependent reactions are used to convert carbon dioxide into glucose through a series of chemical reactions.\n",
      "\n",
      "Photosynthesis is a complex process that involves many biochemical reactions and requires specific conditions, such as:\n",
      "\n",
      "* Light: Photosynthesis occurs in the presence of light, typically from the sun.\n",
      "* Water: Plants need water to absorb carbon dioxide and produce glucose.\n",
      "* Carbon dioxide: Plants need carbon dioxide to produce glucose.\n",
      "* Temperature: Photosynthesis occurs optimally between 20°C and 30°C (68°F to 86°F).\n",
      "* Chlorophyll: Plants need chlorophyll to absorb light energy.\n",
      "\n",
      "In summary, photosynthesis is the process by which plants convert light energy into chemical energy, producing glucose and oxygen as byproducts, which are essential for life on Earth."
     ]
    }
   ],
   "source": [
    "# For streaming results\n",
    "res = \"\"\n",
    "for chunk in stream(\n",
    "    model=base_model_path,\n",
    "    prompt=\"What is photosynthesis?\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8000,\n",
    "    api_key=\"empty\",\n",
    "    max_tokens=512,\n",
    "):\n",
    "    res += chunk\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0f061-08cd-4bbc-95e2-1dbb3456b543",
   "metadata": {},
   "source": [
    "### Run Performance Benchmarking\n",
    "\n",
    "Now that the **vLLM server for the base model** has been started, we can proceed with benchmarking its performance using **GuideLLM**.\n",
    "\n",
    "Identify the following parameters:\n",
    "\n",
    "- **target**: URL of the vLLM inference server started in the previous step  \n",
    "  (e.g., `http://127.0.0.1:8000`)\n",
    "\n",
    "- **output-path**: Path where benchmarking results will be saved\n",
    "\n",
    "If needed, adjust the `target`, `output-path`, or benchmarking profile in the command below, then run it in a terminal.\n",
    "\n",
    "**NOTES**:\n",
    "\n",
    "- Ensure the vLLM server for the base model is running before executing the benchmark.\n",
    "- If the vLLM server is running on a different port, update the `target` accordingly.\n",
    "- Make sure you run the following command from the `02_Base_Performance_Benchmarking` directory. If ran from a different directory, the results might not be saved in `model-serve-flow/results/`\n",
    "- The same benchmarking configuration will be reused for evaluating the compressed model.\n",
    "\n",
    "```bash\n",
    "guidellm benchmark \\\n",
    "  --target \"http://127.0.0.1:8000\" \\\n",
    "  --profile sweep \\\n",
    "  --max-seconds 120 \\\n",
    "  --data \"prompt_tokens=1024,output_tokens=512\" \\\n",
    "  --output-path \"../results/base_performance_benchmarks.json\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f7854-3d9d-4124-8be2-31a2e4fad8ed",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The above command will result in multiple tables. The results will be displayed on the terminal and will be saved in the path defined by `output-path`.\n",
    "\n",
    "1. **Request Latency Statistics (Completed Requests)**\n",
    "\n",
    "This table focuses on how **long** requests take and the latency characteristics of the server.\n",
    "\n",
    "```text\n",
    "\n",
    "ℹ Request Latency Statistics (Completed Requests)\n",
    "|=============|=========|========|=========|=========|======|======|=======|=======|\n",
    "| Benchmark   | Request Latency || TTFT             || ITL        || TPOT         ||\n",
    "| Strategy    | Sec             || ms               || ms         || ms           ||\n",
    "|             | Mdn     | p95    | Mdn     | p95     | Mdn  | p95  | Mdn   | p95   |\n",
    "|-------------|---------|--------|---------|---------|------|------|-------|-------|\n",
    "| synchronous | 11.4    | 11.4   | 115.9   | 124.1   | 22.2 | 22.2 | 22.3  | 22.4  |\n",
    "| throughput  | 62.3    | 92.4   | 33854.1 | 60812.2 | 55.7 | 92.4 | 121.6 | 180.5 |\n",
    "| constant    | 12.5    | 12.6   | 130.4   | 143.1   | 24.3 | 24.3 | 24.5  | 24.5  |\n",
    "| constant    | 13.2    | 13.2   | 133.6   | 144.9   | 25.6 | 25.6 | 25.8  | 25.8  |\n",
    "| constant    | 14.0    | 14.1   | 133.5   | 144.9   | 27.1 | 27.2 | 27.3  | 27.5  |\n",
    "| constant    | 14.7    | 14.8   | 138.7   | 151.9   | 28.6 | 28.6 | 28.8  | 28.9  |\n",
    "| constant    | 16.5    | 16.6   | 140.2   | 156.9   | 32.0 | 32.2 | 32.3  | 32.4  |\n",
    "| constant    | 17.5    | 17.5   | 143.2   | 157.9   | 33.9 | 34.0 | 34.1  | 34.3  |\n",
    "| constant    | 18.5    | 18.7   | 146.8   | 161.8   | 36.0 | 36.3 | 36.2  | 36.5  |\n",
    "| constant    | 20.4    | 20.4   | 147.0   | 162.4   | 39.6 | 39.7 | 39.8  | 39.9  |\n",
    "|=============|=========|========|=========|=========|======|======|=======|=======|\n",
    "```\n",
    "\n",
    "2.  **Server Throughput Statistics**\n",
    "\n",
    "This table focuses on how many requests a server can handle per second. Throughput can be thought of as the **rate** (or time required) of processing. \n",
    "```text\n",
    "ℹ Server Throughput Statistics\n",
    "|=============|=====|======|=======|=======|========|========|========|=======|=======|========|\n",
    "| Benchmark   | Requests                |||| Input Tokens   || Output Tokens || Total Tokens  ||\n",
    "| Strategy    | Per Sec   || Concurrency  || Per Sec        || Per Sec       || Per Sec       ||\n",
    "|             | Mdn | Mean | Mdn   | Mean  | Mdn    | Mean   | Mdn    | Mean  | Mdn   | Mean   |\n",
    "|-------------|-----|------|-------|-------|--------|--------|--------|-------|-------|--------|\n",
    "| synchronous | 0.1 | 0.1  | 1.0   | 1.0   | 92.5   | 101.8  | 45.1   | 44.8  | 45.1  | 137.4  |\n",
    "| throughput  | 0.5 | 1.7  | 125.0 | 100.5 | 132.2  | 3094.4 | 604.5  | 885.0 | 607.5 | 2713.7 |\n",
    "| constant    | 0.3 | 0.2  | 3.0   | 3.2   | 303.9  | 314.3  | 138.2  | 135.9 | 138.3 | 416.8  |\n",
    "| constant    | 0.5 | 0.4  | 6.0   | 5.7   | 519.6  | 530.3  | 204.8  | 227.9 | 205.2 | 698.9  |\n",
    "| constant    | 0.7 | 0.6  | 10.0  | 8.6   | 735.7  | 746.1  | 262.5  | 318.9 | 262.7 | 977.8  |\n",
    "| constant    | 0.9 | 0.8  | 13.0  | 11.5  | 951.1  | 962.4  | 344.7  | 408.1 | 344.8 | 1251.4 |\n",
    "| constant    | 1.1 | 0.9  | 18.0  | 15.5  | 1169.7 | 1178.3 | 422.8  | 491.6 | 423.4 | 1507.6 |\n",
    "| constant    | 1.3 | 1.1  | 23.0  | 19.3  | 1383.0 | 1394.3 | 464.6  | 576.5 | 465.1 | 1767.8 |\n",
    "| constant    | 1.5 | 1.3  | 28.0  | 23.4  | 1598.5 | 1610.6 | 497.3  | 658.7 | 498.2 | 2019.9 |\n",
    "| constant    | 1.7 | 1.4  | 34.0  | 28.5  | 1827.6 | 1826.5 | 576.5  | 734.0 | 577.6 | 2250.6 |\n",
    "|=============|=====|======|=======|=======|========|========|========|=======|=======|========|\n",
    "\n",
    "```\n",
    "#### Base Model Performance Summary\n",
    "1. Max concurrency under load: 34.0 (Concurrency Mdn)\n",
    "2. Max output tokens per second under load: 576.5 (Output tokens per sec Mdn)\n",
    "3. Request latency under load: 20.4 (Request Latency in secs Mdn)\n",
    "4. Time to first token under load: 147.0 (TTFT ms Mdn)\n",
    "5. Inter token latency under load: 39.6 (ITL ms Mdn)\n",
    "\n",
    "#### SLO Analysis\n",
    "\n",
    "Assume the Service Level Objective (SLO) is:\n",
    "\n",
    "    TTFT ≤ 200 milliseconds for 95% of requests (p95) with optimal concurrency\n",
    "\n",
    "Given the SLO of TTFT ≤ 200 ms for 95% of requests (p95) at optimal concurrency, the base model meets this requirement. At the highest tested concurrency of 34 requests, the p95 TTFT is 162.4 ms, which satisfies the SLO.\n",
    "\n",
    "These results establish the performance baseline for the base model. In the next step, the quantized (compressed) model will be benchmarked under the same conditions to determine whether model compression leads to improved TTFT while continuing to meet the SLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0c53b-3f15-460a-a6ea-785e3eb92609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after the benchmarking process in the terminal completes\n",
    "report = GenerativeBenchmarksReport.load_file(\n",
    "    path=\"../results/base_performance_benchmarks.json\",\n",
    ")\n",
    "base_benchmarks = report.benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8ca06-42c8-4487-b0ac-de3a65449ad4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_benchmarks[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
