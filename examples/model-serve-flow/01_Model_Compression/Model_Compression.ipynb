{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7375209-d552-4543-9e7f-1c9a03117314",
   "metadata": {},
   "source": [
    "## Compress the Base LLM using LLM Compressor:\n",
    "In this step, a Large Language Model(which we refere to as the base model for this use case) is compressed to reduce its memory footprint and improve inference efficiency without significantly impacting accuracy. We will be using data-aware quantization.\n",
    "\n",
    "**Goal**: Reduce model size (e.g., FP16 → INT8/INT4) while retaining performance.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- Load the base model.\n",
    "\n",
    "- Measure its size and memory usage.\n",
    "\n",
    "- Use a calibration dataset (e.g., WikiText, UltraChat) to collect activation statistics.\n",
    "\n",
    "- Apply a quantization recipe (e.g., SmoothQuant + GPTQ modifier).\n",
    "\n",
    "- Save the compressed model and verify size reduction.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Compressed model saved on disk.\n",
    "\n",
    "- Model size reduced, typically by 50% (depending on quantization scheme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d76ec6-9bab-4a4b-bda8-0e5663510937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from utils import model_size_gb, tokenize_for_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb9620-ca7b-44e4-bc75-b2de8bd992c1",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6585cb-614e-403b-beb1-36a9c03f66b8",
   "metadata": {},
   "source": [
    "### Loading Base Model\n",
    "You can use the model of your choice by modifying the `model_name` variable.\n",
    "While loading the model using **from_pretrained** using transformers' **AutoModelForCausalLM** class, we specify the data type using the **torch_dtype** parameter and set it to **auto** so the model is loaded in the data type specified in its config.\n",
    "Otherwise, PyTorch loads the weights in **full precision (fp32)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799031ae-0932-4924-86e6-68d7917c482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables\n",
    "model_name = \"RedHatAI/Llama-3.1-8B-Instruct\"\n",
    "base_model_path = \"./base_model\"\n",
    "compressed_model_path = \"Llama_3.1_8B_Instruct_int8_dynamic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb655abf-5d22-49e4-b278-eb5d92fa26d5",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 127.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model saved at: ./base_model\n"
     ]
    }
   ],
   "source": [
    "# loading model and tokenizer from huggingfaceabs\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "model.config.dtype = \"bfloat16\"\n",
    "# saving model and tokenizer\n",
    "model.save_pretrained(base_model_path)\n",
    "tokenizer.save_pretrained(base_model_path)\n",
    "\n",
    "print(\"Base model saved at:\", base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725441f-ed72-44fd-8f6d-8db56b637049",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the base model is: 14.9575GB\n"
     ]
    }
   ],
   "source": [
    "# check model size\n",
    "# !du -sh {base_model_path}\n",
    "model_size = model_size_gb(model)\n",
    "print(f\"The size of the base model is: {model_size:.4f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b8093-b984-4d58-a552-65692dc7944c",
   "metadata": {},
   "source": [
    "### Preparing Calibration Dataset\n",
    "\n",
    "Since we are using data-aware quantization to compress the base model, we need a dataset to calibrate the model with real or representative inputs. For the sake of this example, we will use a small, general-purpose dataset for faster processing. Specifically, we use the `wikitext-2-raw-v1` version of the WikiText dataset which is the smaller version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6dd2e2-58d1-47e8-b17b-2a0d78627e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset to use for calibration\n",
    "dataset_id = \"wikitext\"\n",
    "\n",
    "# Specify the configuration / version of the dataset\n",
    "config = \"wikitext-2-raw-v1\"  # Small version (~2M tokens), raw text format\n",
    "\n",
    "# Set the number of calibration samples based on available device\n",
    "# - On GPU: use more samples to get more accurate activation statistics\n",
    "# - On CPU: reduce samples to prevent memory issues and keep demo fast\n",
    "num_calibration_samples = 512 if device == \"cuda\" else 16\n",
    "\n",
    "# Set the maximum sequence length for calibration\n",
    "max_sequence_length = 1024 if device == \"cuda\" else 16\n",
    "\n",
    "# Load the dataset using Hugging Face Datasets API\n",
    "# This downloads train split of the dataset\n",
    "ds = load_dataset(dataset_id, config, split=\"train\")\n",
    "# Shuffle and grab only the number of samples we need\n",
    "ds = ds.shuffle(seed=42).select(range(num_calibration_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b24ce-6980-4aff-937c-e0276eb58a6a",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in the wikitext: ['text']\n",
      "\n",
      "{'text': ' Continuous , short @-@ arc , high pressure xenon arc lamps have a color temperature closely approximating noon sunlight and are used in solar simulators . That is , the chromaticity of these lamps closely approximates a heated black body radiator that has a temperature close to that observed from the Sun . After they were first introduced during the 1940s , these lamps began replacing the shorter @-@ lived carbon arc lamps in movie projectors . They are employed in typical 35mm , IMAX and the new digital projectors film projection systems , automotive HID headlights , high @-@ end \" tactical \" flashlights and other specialized uses . These arc lamps are an excellent source of short wavelength ultraviolet radiation and they have intense emissions in the near infrared , which is used in some night vision systems . \\n'}\n"
     ]
    }
   ],
   "source": [
    "# inspect the dataset\n",
    "print(f\"columns in the {dataset_id}: {ds.column_names}\\n\")\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e60f8-6420-41aa-91da-266b2ce828c1",
   "metadata": {},
   "source": [
    "**Datset inspection shows the we need to extract column ```text``` and pass it as input to the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf853df6-2fbd-4a95-ae92-52c7b11577a4",
   "metadata": {},
   "source": [
    "### When to Use a Custom Template for Calibration\n",
    "\n",
    "Use a **custom template** when you want the calibration text to closely mimic the input format your model will see in production.  \n",
    "\n",
    "For example, if your model is **instruction-following** or **chat-based**, providing the template the model was originally trained on or the template that will be used during inference ensures that the activation statistics collected during calibration reflect realistic usage patterns. \n",
    "\n",
    "This can improve the accuracy of quantization and compression.\n",
    "\n",
    "If your model can handle raw text and doesn’t require a specific format, you can rely on the default template instead.\n",
    "\n",
    "A custom template can be provided to the `tokenize_for_calibration` function using the `custom_template` argument. It accepts the following format:\n",
    "\n",
    "```python\n",
    "custom_template = {\n",
    " \"template_text\": \"Instruction: {content}\\nOutput:\", \n",
    " \"placeholder\": \"content\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54934efc-9564-446c-a37b-daf58dcb5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get activations for the calibration dataset, we need to:\n",
    "# 1. extract the samples from the dataset\n",
    "# 2. tokenize samples in the dataset\n",
    "input_column = \"text\"\n",
    "\n",
    "# Call tokenize_for_calibration using dataset.map\n",
    "tokenized_dataset = ds.map(\n",
    "    lambda batch: tokenize_for_calibration(\n",
    "        examples=batch,  # batch from Hugging Face dataset\n",
    "        input_column=input_column,  # the column containing text to calibrate\n",
    "        tokenizer=tokenizer,  # your Hugging Face tokenizer\n",
    "        max_length=max_sequence_length,  # maximum sequence length\n",
    "        model_type=\"chat\",  # use chat template if no custom template\n",
    "        custom_template=None,  # optional, provide a dict if you want a custom template\n",
    "    ),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea72af-71a5-4cc8-91fc-69bf7b642b45",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 512\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5357d71-6eed-40fa-972b-d7e428422414",
   "metadata": {},
   "source": [
    "### Quantizing/Compressing Base Model to INT8\n",
    "After perparing the dataset calibration, we define a recipe for quantization. For quantization scheme `W8A8-INT8`, we use `SmoothQuantModifier` followed by `GPTQModifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b5686-60b4-42f1-991a-21260466797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quantization scheme\n",
    "scheme = \"W8A8\"  # W8A8 means 8-bit weights and 8-bit activations\n",
    "\n",
    "# Strength for SmoothQuant smoothing\n",
    "# This controls how much the activation values are smoothed to reduce outliers\n",
    "smoothing_strength = 0.8\n",
    "\n",
    "# Create SmoothQuant modifier\n",
    "# - smooths activations before quantization to improve stability and reduce degradation\n",
    "smooth_quant = SmoothQuantModifier(smoothing_strength=smoothing_strength)\n",
    "\n",
    "# Create GPTQ modifier\n",
    "# - targets=\"Linear\" quantizes only Linear layers (e.g., feedforward layers)\n",
    "# - scheme=scheme uses the W8A8 quantization scheme\n",
    "# - ignore=[\"lm_head\"] preserves the LM head to avoid generation quality loss\n",
    "quantizer = GPTQModifier(targets=\"Linear\", scheme=scheme, ignore=[\"lm_head\"])\n",
    "\n",
    "# Combine the modifiers into a recipe list\n",
    "# The order matters: first apply SmoothQuant, then GPTQ\n",
    "recipe = [smooth_quant, quantizer]\n",
    "\n",
    "# Perform quantization\n",
    "oneshot(\n",
    "    model=model_name,  # Model to quantize\n",
    "    dataset=tokenized_dataset,  # Calibration dataset, used for both SmoothQuant & GPTQ\n",
    "    recipe=recipe,  # List of quantization modifiers to apply\n",
    "    output_dir=compressed_model_path,  # Directory to save the quantized model\n",
    "    max_seq_length=2048,  # Maximum sequence length for calibration\n",
    "    num_calibration_samples=512,  # Number of samples used for calibration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf956831",
   "metadata": {},
   "source": [
    "### Checking model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56836404-daab-4638-8abf-957b71f82507",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (GB): 8.460090637207031\n"
     ]
    }
   ],
   "source": [
    "# Load quantized model\n",
    "model_quant = AutoModelForCausalLM.from_pretrained(compressed_model_path)\n",
    "model_quant.config.dtype = model.config.torch_dtype\n",
    "model_quant.save_pretrained(compressed_model_path)\n",
    "model_size = model_size_gb(model_quant)\n",
    "print(f\"Model size (GB): {model_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882026b-2c4c-4e1f-b20f-75fdaf71b7f3",
   "metadata": {},
   "source": [
    "### Observation\n",
    "After quantizing the model, the size has clearly reduced from 14.9GB to 8GB. Now that we have reduced the model size, the next step is to evaluate this compressed model to make sure the accuracy has retained after compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bb261-81be-4324-aa28-241650c9634f",
   "metadata": {},
   "source": [
    "\n",
    "**ALTERNATIVELY**, llm-compressor also supports FP8 quantization. This conversion foes not require any calibration dataset. Uncomment the below cell to quantize the model to FP8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c84a3-83b0-4d28-9665-13e4f1ef5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe = QuantizationModifier(\n",
    "#   targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"])\n",
    "\n",
    "# oneshot(model=model_name, recipe=recipe, output_dir=compressed_model_path)\n",
    "\n",
    "# # Load quantized model\n",
    "# model_quant = AutoModelForCausalLM.from_pretrained(compressed_model_path)\n",
    "# model_quant.config.dtype = model.config.torch_dtype\n",
    "# model_quant.save_pretrained(compressed_model_path)\n",
    "# model_size = model_size_gb(model_quant)\n",
    "# print(f\"Model size (GB): {model_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-serve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
