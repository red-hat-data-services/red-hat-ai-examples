{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7375209-d552-4543-9e7f-1c9a03117314",
   "metadata": {},
   "source": [
    "## Compress the Base LLM using LLM Compressor:\n",
    "In this step, a Large Language Model(which we refere to as the base model for this use case) is compressed to reduce its memory footprint and improve inference efficiency without significantly impacting accuracy.\n",
    "\n",
    "**Goal**: Reduce model size (e.g., FP16 → INT8/INT4) while retaining performance.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- Load the base model.\n",
    "\n",
    "- Measure its size and memory usage.\n",
    "\n",
    "- Use a calibration dataset (e.g., WikiText, UltraChat) to collect activation statistics.\n",
    "\n",
    "- Apply a quantization recipe (e.g., SmoothQuant + GPTQ modifier).\n",
    "\n",
    "- Save the compressed model and verify size reduction.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Compressed model saved on disk.\n",
    "\n",
    "- Model size reduced, typically by 50% (depending on quantization scheme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d76ec6-9bab-4a4b-bda8-0e5663510937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd49f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_gb(model):\n",
    "    \"\"\"\n",
    "    Compute the model's size in gigabytes and return its parameter dtype.\n",
    "\n",
    "    Returns:\n",
    "        (float): Total size of all model parameters in GB.\n",
    "        (torch.dtype | None): Data type of the model parameters, or None if empty.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for param in model.parameters():\n",
    "        total += param.nelement() * param.element_size()\n",
    "    size_gb = total / (1024 * 1024 * 1024)  # GB\n",
    "\n",
    "    return size_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_calibration(\n",
    "    examples,\n",
    "    input_column,\n",
    "    tokenizer,\n",
    "    max_length,\n",
    "    model_type=\"general\",\n",
    "    custom_template=None,  # dictionary with 'text' and 'mapping'\n",
    "):\n",
    "    \"\"\"\n",
    "    Tokenize dataset text examples for use in GPTQ / LM Compressor calibration.\n",
    "\n",
    "    This function prepares text inputs according to the expected prompting format\n",
    "    of different model types (general, chat, instruction, code). Calibration text\n",
    "    should resemble the inputs the model will see in real usage so that activation\n",
    "    statistics are accurate for quantization.\n",
    "\n",
    "    Behavior:\n",
    "    - If a custom template dictionary (`custom_template`) is provided, the template text\n",
    "      is applied to each example using the specified placeholder.\n",
    "    - If `custom_template` is not provided, a default template is selected based on\n",
    "      `model_type` using predefined mappings.\n",
    "    - For general-purpose models, the default template is raw text (`\"{text}\"`).\n",
    "    - For chat, instruction, or code models, structured templates are applied\n",
    "      (e.g., \"User: ...\\nAssistant:\", instruction headers, or code docstring format).\n",
    "    - Only a single column from the dataset is required for calibration; the placeholder\n",
    "      in the template is filled with values from this column.\n",
    "\n",
    "    Args:\n",
    "        examples (dict):\n",
    "            A batch from a Hugging Face dataset.\n",
    "        input_column (str):\n",
    "            Name of the column that contains text to be used for calibration.\n",
    "        tokenizer (transformers.PreTrainedTokenizerBase):\n",
    "            The tokenizer associated with the model being calibrated.\n",
    "        max_length (int):\n",
    "            Maximum sequence length for truncation/padding during tokenization.\n",
    "        model_type (str, optional):\n",
    "            Type of model input format to use when no custom template is provided. One of:\n",
    "                - \"general\": raw text (default)\n",
    "                - \"chat\": conversational prompt format\n",
    "                - \"instruction\": instruction-following format\n",
    "                - \"code\": code generation format\n",
    "        custom_template (dict, optional):\n",
    "            A dictionary specifying a custom template for calibration. Must contain:\n",
    "                - 'template_text': the template string containing a placeholder\n",
    "                - 'placeholder': the name of the placeholder in the template string\n",
    "            Example:\n",
    "                custom_template = {\n",
    "                    \"template_text\": \"Instruction: {content}\\nOutput:\",\n",
    "                    \"placeholder\": \"content\"\n",
    "                }\n",
    "            If provided, this template is used instead of the default template.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            A dictionary containing tokenized fields (e.g., \"input_ids\",\n",
    "            \"attention_mask\") compatible with LM Compressor / GPTQ calibration.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_TEMPLATES = {\n",
    "        \"general\": \"{text}\",\n",
    "        \"chat\": \"User: {text}\\nAssistant:\",\n",
    "        \"instruction\": (\"Instruction: {text}\\nInput:\\nOutput:\"),\n",
    "        \"code\": (\"# Task description:\\n{text}\\n# Solution:\"),\n",
    "    }\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    try:\n",
    "        texts = examples[input_column]\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]  # huggingface tokenizer expects a list\n",
    "    except (KeyError, TypeError) as err:\n",
    "        raise ValueError(\n",
    "            f\"Expected `examples` to contain a {input_column} field. \"\n",
    "            f\"Please ensure your dataset has a {input_column} column.\"\n",
    "        ) from err\n",
    "\n",
    "    # Choose template: user-defined or default\n",
    "    if custom_template is None:\n",
    "        # Use default template\n",
    "        template = DEFAULT_TEMPLATES.get(model_type, \"{text}\")\n",
    "        placeholder = \"text\"\n",
    "    else:\n",
    "        # use custom template\n",
    "        if (\n",
    "            not isinstance(custom_template, dict)\n",
    "            or \"template_text\" not in custom_template\n",
    "            or \"placeholder\" not in custom_template\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"custom_template must be a dict containing keys 'template_text' and 'placeholder'.\"\n",
    "            )\n",
    "        template = custom_template[\"template_text\"]\n",
    "        placeholder = custom_template.get(\"placeholder\")\n",
    "\n",
    "    if custom_template is not None:\n",
    "        # check if the provided place holder exists in the custom template\n",
    "        if \"{\" + placeholder + \"}\" not in template:\n",
    "            raise ValueError(\n",
    "                f\"Custom template does not contain placeholder {{{placeholder}}}\"\n",
    "            )\n",
    "    # apply template\n",
    "    texts = [template.format(**{placeholder: text}) for text in texts]\n",
    "\n",
    "    # Tokenize\n",
    "    return tokenizer(\n",
    "        texts, truncation=True, max_length=max_length, padding=\"max_length\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb9620-ca7b-44e4-bc75-b2de8bd992c1",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6585cb-614e-403b-beb1-36a9c03f66b8",
   "metadata": {},
   "source": [
    "## Loading Base Model\n",
    "You can use the model of your choice by modifying the \"model_name\" variable.\n",
    "While loading the model using **from_pretrained** using transformers' **AutoModelForCausalLM** class, we specify the data type using the **torch_dtype** parameter and set it to **auto** so the model is loaded in the data type specified in its config.\n",
    "Otherwise, PyTorch loads the weights in **full precision (fp32)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799031ae-0932-4924-86e6-68d7917c482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables\n",
    "model_name = \"RedHatAI/Llama-3.1-8B-Instruct\"\n",
    "base_model_path = \"./base_model\"\n",
    "compressed_model_path = \"Llama_3.1_8B_Instruct_int8_dynamic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb655abf-5d22-49e4-b278-eb5d92fa26d5",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 127.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model saved at: ./base_model\n"
     ]
    }
   ],
   "source": [
    "# loading model and tokenizer from huggingfaceabs\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "model.config.dtype = \"bfloat16\"\n",
    "# saving model and tokenizer\n",
    "model.save_pretrained(base_model_path)\n",
    "tokenizer.save_pretrained(base_model_path)\n",
    "\n",
    "print(\"Base model saved at:\", base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725441f-ed72-44fd-8f6d-8db56b637049",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the base model is: 14.9575GB\n"
     ]
    }
   ],
   "source": [
    "# check model size\n",
    "# !du -sh {base_model_path}\n",
    "model_size = model_size_gb(model)\n",
    "print(f\"The size of the base model is: {model_size:.4f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f28eb-1122-416d-bb31-40ee900b1ddc",
   "metadata": {},
   "source": [
    "## Data Aware Weight+Activation Quantization\n",
    "\n",
    "- Use a calibration dataset to avoid data distribution drift after quantization\n",
    "- Scheme for quantization is \"W8A8\"; convert both weights and activations to INT8 (can be W4A4 as well)\n",
    "- Activations are quantized on the fly during inference. (dynamic activation)\n",
    "\n",
    "### Things to keep in mind for data aware quantization\n",
    "1. **Choice of Calibration Dataset:** GPTQ quantization estimates activation ranges from calibration data. If the calibration data is very different from what the model will see in production, these ranges may be inaccurate, leading to higher quantization errors and degraded model outputs.\n",
    "\n",
    "   For production, use a dataset that closely resembles the expected domain(e.g finance, medicine etc), task(Q/A,    summarization etc), and style of your inputs to ensure quantization preserves quality.\n",
    "\n",
    "    For the sake of this demo, we can use a small, general-purpose dataset for faster processing. Specifically, we use the `wikitext-2-raw-v1` version of the WikiText dataset which is the smaller version.\n",
    "\n",
    "2. **Number of Calibration Samples Used for Quantization**\n",
    "\n",
    "     More samples give better and stable statistics on the range and distribution of activations, which reduces quantization noise. Small calibration sets, on the other hand, are quicker but noisier.\n",
    "    \n",
    "    For the sake of this demo, we use a small number of samples (e.g., 16–512) is enough to show the process.\n",
    "    \n",
    "    For production, use a larger sample set (hundreds to thousands) to stabilize ranges and minimize error.\n",
    "\n",
    "4. **Sequence Length**\n",
    "\n",
    "    Longer input sequences generate larger activations because each token’s representation depends on all previous tokens and layers. These bigger values can exceed the quantization range (e.g., -128 to 127 for 8-bit quantization), causing rounding errors or clipping, which reduces accuracy.\n",
    "    \n",
    "    For this demo, shorter sequences are sufficient to illustrate quantization.\n",
    "    \n",
    "    For production, use sequences that reflect maximum expected lengths in your application to prevent errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b8093-b984-4d58-a552-65692dc7944c",
   "metadata": {},
   "source": [
    "### Preparing Calibration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6dd2e2-58d1-47e8-b17b-2a0d78627e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset to use for calibration\n",
    "dataset_id = \"wikitext\"\n",
    "\n",
    "# Specify the configuration / version of the dataset\n",
    "config = \"wikitext-2-raw-v1\"  # Small version (~2M tokens), raw text format\n",
    "\n",
    "# Set the number of calibration samples based on available device\n",
    "# - On GPU: use more samples to get more accurate activation statistics\n",
    "# - On CPU: reduce samples to prevent memory issues and keep demo fast\n",
    "num_calibration_samples = 512 if device == \"cuda\" else 16\n",
    "\n",
    "# Set the maximum sequence length for calibration\n",
    "max_sequence_length = 1024 if device == \"cuda\" else 16\n",
    "\n",
    "# Load the dataset using Hugging Face Datasets API\n",
    "# This downloads train split of the dataset\n",
    "ds = load_dataset(dataset_id, config, split=\"train\")\n",
    "# Shuffle and grab only the number of samples we need\n",
    "ds = ds.shuffle(seed=42).select(range(num_calibration_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b24ce-6980-4aff-937c-e0276eb58a6a",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in the wikitext: ['text']\n",
      "\n",
      "{'text': ' Continuous , short @-@ arc , high pressure xenon arc lamps have a color temperature closely approximating noon sunlight and are used in solar simulators . That is , the chromaticity of these lamps closely approximates a heated black body radiator that has a temperature close to that observed from the Sun . After they were first introduced during the 1940s , these lamps began replacing the shorter @-@ lived carbon arc lamps in movie projectors . They are employed in typical 35mm , IMAX and the new digital projectors film projection systems , automotive HID headlights , high @-@ end \" tactical \" flashlights and other specialized uses . These arc lamps are an excellent source of short wavelength ultraviolet radiation and they have intense emissions in the near infrared , which is used in some night vision systems . \\n'}\n"
     ]
    }
   ],
   "source": [
    "# inspect the dataset\n",
    "print(f\"columns in the {dataset_id}: {ds.column_names}\\n\")\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e60f8-6420-41aa-91da-266b2ce828c1",
   "metadata": {},
   "source": [
    "**Datset inspection shows the we need to extract column ```text``` and pass it as input to the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf853df6-2fbd-4a95-ae92-52c7b11577a4",
   "metadata": {},
   "source": [
    "### When to Use a Custom Template for Calibration\n",
    "\n",
    "Use a **custom template** when you want the calibration text to closely mimic the input format your model will see in production.  \n",
    "\n",
    "For example, if your model is **instruction-following** or **chat-based**, providing the template the model was originally trained on or the template that will be used during inference ensures that the activation statistics collected during calibration reflect realistic usage patterns. \n",
    "\n",
    "This can improve the accuracy of quantization and compression.\n",
    "\n",
    "If your model can handle raw text and doesn’t require a specific format, you can rely on the default template instead.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54934efc-9564-446c-a37b-daf58dcb5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get activations for the calibration dataset, we need to:\n",
    "# 1. extract the samples from the dataset\n",
    "# 2. tokenize samples in the dataset\n",
    "input_column = \"text\"\n",
    "\n",
    "# Call tokenize_for_calibration using dataset.map\n",
    "tokenized_dataset = ds.map(\n",
    "    lambda batch: tokenize_for_calibration(\n",
    "        examples=batch,  # batch from Hugging Face dataset\n",
    "        input_column=input_column,  # the column containing text to calibrate\n",
    "        tokenizer=tokenizer,  # your Hugging Face tokenizer\n",
    "        max_length=max_sequence_length,  # maximum sequence length\n",
    "        model_type=\"chat\",  # use chat template if no custom template\n",
    "        custom_template=None,  # optional, provide a dict if you want a custom template\n",
    "    ),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea72af-71a5-4cc8-91fc-69bf7b642b45",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 512\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5357d71-6eed-40fa-972b-d7e428422414",
   "metadata": {},
   "source": [
    "### Quantizing/Compressing Base Model to INT8\n",
    "\n",
    "**SmoothQuant** SmoothQuant operates on the activations (outputs of intermediate layers that become inputs to the next layer) produced by the base model. These activations can sometimes have extreme values (outliers). SmoothQuant scales the activations to reduce these outliers so that most values fall within a reasonable range, e.g., [-4, 4].\n",
    "\n",
    "To ensure that the overall layer output remains unchanged (Y = W * A), SmoothQuant also scales the corresponding weights by multiplying them with the same factor.\n",
    "\n",
    "Activations are scaled as:\n",
    "$A^*=A/s$\n",
    "\n",
    "Weights are scaled as:\n",
    "$W^*=W∗s$\n",
    "\n",
    "This way, the layer output remains approximately the same, but the activations are now suitable for stable low-bit quantization.\n",
    "\n",
    "**GPTQModifier** GPTQ takes the smoothed activations and weights produced by SmoothQuant and computes a quantization scale for each weight matrix. This scale determines how weights will be mapped into low-bit integers (e.g., int8).\n",
    "\n",
    "GPTQ then:\n",
    "\n",
    "1. Quantizes the weights using these scales\n",
    "\n",
    "    $Wquant=round(W/s)$\n",
    "\n",
    "2. Computes the model outputs using:\n",
    "\n",
    "    full-precision weights → Y\n",
    "\n",
    "   \n",
    "    quantized weights → Yquant\n",
    "\n",
    "3. Adjusts the quantization error so that\n",
    "\n",
    "    $Yquant≈Y$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b5686-60b4-42f1-991a-21260466797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quantization scheme\n",
    "scheme = \"W8A8\"  # W8A8 means 8-bit weights and 8-bit activations\n",
    "\n",
    "# Strength for SmoothQuant smoothing\n",
    "# This controls how much the activation values are smoothed to reduce outliers\n",
    "smoothing_strength = 0.8\n",
    "\n",
    "# Create SmoothQuant modifier\n",
    "# - smooths activations before quantization to improve stability and reduce degradation\n",
    "smooth_quant = SmoothQuantModifier(smoothing_strength=smoothing_strength)\n",
    "\n",
    "# Create GPTQ modifier\n",
    "# - targets=\"Linear\" quantizes only Linear layers (e.g., feedforward layers)\n",
    "# - scheme=scheme uses the W8A8 quantization scheme\n",
    "# - ignore=[\"lm_head\"] preserves the LM head to avoid generation quality loss\n",
    "quantizer = GPTQModifier(targets=\"Linear\", scheme=scheme, ignore=[\"lm_head\"])\n",
    "\n",
    "# Combine the modifiers into a recipe list\n",
    "# The order matters: first apply SmoothQuant, then GPTQ\n",
    "recipe = [smooth_quant, quantizer]\n",
    "\n",
    "# Perform quantization\n",
    "oneshot(\n",
    "    model=model_name,  # Model to quantize\n",
    "    dataset=ds,  # Calibration dataset, used for both SmoothQuant & GPTQ\n",
    "    recipe=recipe,  # List of quantization modifiers to apply\n",
    "    output_dir=compressed_model_path,  # Directory to save the quantized model\n",
    "    max_seq_length=2048,  # Maximum sequence length for calibration\n",
    "    num_calibration_samples=512,  # Number of samples used for calibration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56836404-daab-4638-8abf-957b71f82507",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (GB): 8.460090637207031\n"
     ]
    }
   ],
   "source": [
    "# Load quantized model\n",
    "model_quant = AutoModelForCausalLM.from_pretrained(compressed_model_path)\n",
    "model_quant.config.dtype = model.config.torch_dtype\n",
    "model_quant.save_pretrained(compressed_model_path)\n",
    "model_size = model_size_gb(model_quant)\n",
    "print(f\"Model size (GB): {model_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882026b-2c4c-4e1f-b20f-75fdaf71b7f3",
   "metadata": {},
   "source": [
    "### Observation\n",
    "After quantizing the model, the size has clearly reduced from 14.9GB to 8GB. Now that we have reduced the model size, the next step is to evaluate this compressed model to make sure the accuracy has retained after compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bb261-81be-4324-aa28-241650c9634f",
   "metadata": {},
   "source": [
    "\n",
    "**ALTERNATIVELY**, llm-compressor also supports FP8 quantization. This conversion foes not require any calibration dataset. Uncomment the below cell to quantize the model to FP8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c84a3-83b0-4d28-9665-13e4f1ef5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe = QuantizationModifier(\n",
    "#   targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"])\n",
    "\n",
    "# oneshot(model=model_name, recipe=recipe, output_dir=compressed_model_path)\n",
    "\n",
    "# # Load quantized model\n",
    "# model_quant = AutoModelForCausalLM.from_pretrained(compressed_model_path)\n",
    "# model_quant.config.dtype = model.config.torch_dtype\n",
    "# model_quant.save_pretrained(compressed_model_path)\n",
    "# model_size = model_size_gb(model_quant)\n",
    "# print(f\"Model size (GB): {model_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-serve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
