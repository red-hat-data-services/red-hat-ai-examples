{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dec15d-d789-4e22-b9ea-5eec04bf9230",
   "metadata": {},
   "source": [
    "## Evaluate Accuracy of the Base Model\n",
    "After compression, it is important to ensure that the model maintains its generative capabilities. In this step, the base model is evaluated on standard benchmarks to establish a performance baseline, which is later used to compare against the compressed model.\n",
    "\n",
    "**Goal**: Evaluate the base (uncompressed) model on standard benchmarks to establish a performance baseline for later comparison with the compressed model.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- We will create a function called **evaluate** that uses the `simple_evaluate` from LM Eval to test the compressed model.\n",
    "\n",
    "- Benchmark on multiple datasets:\n",
    "\n",
    "    - MMLU: General knowledge across subjects.\n",
    "\n",
    "    - IFeval: Instruction-following tasks.\n",
    "\n",
    "    - ARC: Logical and scientific reasoning.\n",
    "    \n",
    "    - HellaSwag: Commonsense completion.\n",
    "\n",
    "- Collect metrics like accuracy, accuracy_norm, and task-specific scores.\n",
    "\n",
    "- Save results as JSON.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Quantitative metrics for the base model.\n",
    "\n",
    "- A baseline to compare the compressed model's accuracy agaisnt.\n",
    "\n",
    "More details on evaluating LLMs is provided in [Accuracy_Evaluation.md](Accuracy_Evaluation.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f45e6-3f29-4922-b85f-2b77be643530",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lm_eval.utils import make_table\n",
    "from utils import evaluate, load_pickle, save_pickle\n",
    "\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab231b-c501-4151-ad16-0d123d114226",
   "metadata": {},
   "source": [
    "To make sure you have enough GPU memory to run this notebook, run the following command in terminal:\n",
    "\n",
    "`nvidia-smi`\n",
    "\n",
    "The output will look something like this:\n",
    "\n",
    "```text\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA L40S                    On  |   00000000:CA:00.0 Off |                    0 |\n",
    "| N/A   44C    P0             91W /  350W |   15753MiB /  46068MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A            8049      C   /opt/app-root/bin/python3             15744MiB |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "\n",
    "Note the PID and run the following command:\n",
    "```\n",
    "`kill -9 <pid>`\n",
    "\n",
    "\n",
    "Replace <pid> with the actual PID for example `8049` in this case. So the command will become `kill -9 8094`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e43e6d-4fbe-4bd4-91ae-156b6a1c3af5",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfb397-d989-4a4b-8636-511a25c12ed6",
   "metadata": {},
   "source": [
    "### Define evaluation benchmarking datasets\n",
    "The following benchmark datasets can be used for evaluating on multiple tasks:\n",
    "- MMLU: General knowledge across 57 subjects\n",
    "- IFeval: Instruction-following capability\n",
    "- ARC: Logical & scientific reasoning\n",
    "- HellaSwag: Commonsense completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383befc1-e7b7-4540-af75-a3d5ce28786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tasks you want to evaluate the model on\n",
    "tasks = [\"mmlu\", \"arc_easy\", \"hellaswag\", \"ifeval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddc52-4acd-4230-9681-f84201f19e33",
   "metadata": {},
   "source": [
    "### Evaluating the Base Model\n",
    "\n",
    "**NOTE**: \n",
    "1. Running the evaluation on the entire list of tasks can take long. So for testing, you can use a single task instead.\n",
    "\n",
    "2. The results will be stored as a **results.pkl** files in the directories defined by **base_results_dir**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b759ca-440b-4491-abba-486fe2c1afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting directories\n",
    "base_model_path = \"../base_model\"\n",
    "base_results_dir = \"results/base_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff8288-c452-4d04-89cc-ddf57339469b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the base model and save results in pkl format\n",
    "base_acc = evaluate(\n",
    "    base_model_path,\n",
    "    tasks,\n",
    "    limit=None,\n",
    "    batch_size=\"auto\",\n",
    "    apply_chat_template=True,\n",
    "    verbosity=None,\n",
    ")\n",
    "save_pickle(base_results_dir, base_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e51ac-f4c3-4276-b2fe-df595a963443",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results = load_pickle(base_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36511cfb-fac8-4f8c-be17-c2d98ffb8560",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 Tasks                 |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
      "|arc_easy                               |      1|none  |     0|acc                    |↑  |0.8136|±  |0.0080|\n",
      "|                                       |       |none  |     0|acc_norm               |↑  |0.7588|±  |0.0088|\n",
      "|hellaswag                              |      1|none  |     0|acc                    |↑  |0.5741|±  |0.0049|\n",
      "|                                       |       |none  |     0|acc_norm               |↑  |0.7251|±  |0.0045|\n",
      "|ifeval                                 |      4|none  |     0|inst_level_loose_acc   |↑  |0.8513|±  |   N/A|\n",
      "|                                       |       |none  |     0|inst_level_strict_acc  |↑  |0.8189|±  |   N/A|\n",
      "|                                       |       |none  |     0|prompt_level_loose_acc |↑  |0.7874|±  |0.0176|\n",
      "|                                       |       |none  |     0|prompt_level_strict_acc|↑  |0.7449|±  |0.0188|\n",
      "|mmlu                                   |      2|none  |      |acc                    |↑  |0.6322|±  |0.0038|\n",
      "| - humanities                          |      2|none  |      |acc                    |↑  |0.5864|±  |0.0068|\n",
      "|  - formal_logic                       |      1|none  |     0|acc                    |↑  |0.4921|±  |0.0447|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc                    |↑  |0.7455|±  |0.0340|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc                    |↑  |0.7892|±  |0.0286|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc                    |↑  |0.8186|±  |0.0251|\n",
      "|  - international_law                  |      1|none  |     0|acc                    |↑  |0.7686|±  |0.0385|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc                    |↑  |0.7315|±  |0.0428|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc                    |↑  |0.7730|±  |0.0329|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc                    |↑  |0.6792|±  |0.0251|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc                    |↑  |0.4179|±  |0.0165|\n",
      "|  - philosophy                         |      1|none  |     0|acc                    |↑  |0.6977|±  |0.0261|\n",
      "|  - prehistory                         |      1|none  |     0|acc                    |↑  |0.7130|±  |0.0252|\n",
      "|  - professional_law                   |      1|none  |     0|acc                    |↑  |0.4687|±  |0.0127|\n",
      "|  - world_religions                    |      1|none  |     0|acc                    |↑  |0.8480|±  |0.0275|\n",
      "| - other                               |      2|none  |      |acc                    |↑  |0.7184|±  |0.0078|\n",
      "|  - business_ethics                    |      1|none  |     0|acc                    |↑  |0.6500|±  |0.0479|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc                    |↑  |0.7094|±  |0.0279|\n",
      "|  - college_medicine                   |      1|none  |     0|acc                    |↑  |0.6416|±  |0.0366|\n",
      "|  - global_facts                       |      1|none  |     0|acc                    |↑  |0.4200|±  |0.0496|\n",
      "|  - human_aging                        |      1|none  |     0|acc                    |↑  |0.6771|±  |0.0314|\n",
      "|  - management                         |      1|none  |     0|acc                    |↑  |0.8058|±  |0.0392|\n",
      "|  - marketing                          |      1|none  |     0|acc                    |↑  |0.8547|±  |0.0231|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc                    |↑  |0.8100|±  |0.0394|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc                    |↑  |0.8084|±  |0.0141|\n",
      "|  - nutrition                          |      1|none  |     0|acc                    |↑  |0.7549|±  |0.0246|\n",
      "|  - professional_accounting            |      1|none  |     0|acc                    |↑  |0.5177|±  |0.0298|\n",
      "|  - professional_medicine              |      1|none  |     0|acc                    |↑  |0.7610|±  |0.0259|\n",
      "|  - virology                           |      1|none  |     0|acc                    |↑  |0.5663|±  |0.0386|\n",
      "| - social sciences                     |      2|none  |      |acc                    |↑  |0.7442|±  |0.0077|\n",
      "|  - econometrics                       |      1|none  |     0|acc                    |↑  |0.4386|±  |0.0467|\n",
      "|  - high_school_geography              |      1|none  |     0|acc                    |↑  |0.7929|±  |0.0289|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc                    |↑  |0.8497|±  |0.0258|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc                    |↑  |0.6564|±  |0.0241|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc                    |↑  |0.7479|±  |0.0282|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc                    |↑  |0.8642|±  |0.0147|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc                    |↑  |0.7634|±  |0.0373|\n",
      "|  - professional_psychology            |      1|none  |     0|acc                    |↑  |0.6797|±  |0.0189|\n",
      "|  - public_relations                   |      1|none  |     0|acc                    |↑  |0.6909|±  |0.0443|\n",
      "|  - security_studies                   |      1|none  |     0|acc                    |↑  |0.6898|±  |0.0296|\n",
      "|  - sociology                          |      1|none  |     0|acc                    |↑  |0.8308|±  |0.0265|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc                    |↑  |0.8600|±  |0.0349|\n",
      "| - stem                                |      2|none  |      |acc                    |↑  |0.5062|±  |0.0084|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc                    |↑  |0.2700|±  |0.0446|\n",
      "|  - anatomy                            |      1|none  |     0|acc                    |↑  |0.6222|±  |0.0419|\n",
      "|  - astronomy                          |      1|none  |     0|acc                    |↑  |0.6974|±  |0.0374|\n",
      "|  - college_biology                    |      1|none  |     0|acc                    |↑  |0.7708|±  |0.0351|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc                    |↑  |0.4700|±  |0.0502|\n",
      "|  - college_computer_science           |      1|none  |     0|acc                    |↑  |0.4100|±  |0.0494|\n",
      "|  - college_mathematics                |      1|none  |     0|acc                    |↑  |0.2800|±  |0.0451|\n",
      "|  - college_physics                    |      1|none  |     0|acc                    |↑  |0.3922|±  |0.0486|\n",
      "|  - computer_security                  |      1|none  |     0|acc                    |↑  |0.7200|±  |0.0451|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc                    |↑  |0.5915|±  |0.0321|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc                    |↑  |0.5724|±  |0.0412|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc                    |↑  |0.3942|±  |0.0252|\n",
      "|  - high_school_biology                |      1|none  |     0|acc                    |↑  |0.7581|±  |0.0244|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc                    |↑  |0.5123|±  |0.0352|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc                    |↑  |0.6100|±  |0.0490|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc                    |↑  |0.2481|±  |0.0263|\n",
      "|  - high_school_physics                |      1|none  |     0|acc                    |↑  |0.3709|±  |0.0394|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc                    |↑  |0.4213|±  |0.0337|\n",
      "|  - machine_learning                   |      1|none  |     0|acc                    |↑  |0.4911|±  |0.0475|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print results for the base model\n",
    "print(make_table(base_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
