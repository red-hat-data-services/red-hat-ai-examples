{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dec15d-d789-4e22-b9ea-5eec04bf9230",
   "metadata": {},
   "source": [
    "## Evaluate Accuracy of the Base Model\n",
    "After compression, it’s important to ensure the model maintains its generative capabilities. This step evaluates the compressed model on standard benchmarks.\n",
    "\n",
    "**Goal**: Verify that compression does not degrade model quality.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- We will create a function called **evaluate** that uses simple_evaluate from LM Eval to test the compressed model.\n",
    "\n",
    "- Benchmark on multiple datasets:\n",
    "\n",
    "    - MMLU: General knowledge across subjects.\n",
    "\n",
    "    - IFeval: Instruction-following tasks.\n",
    "\n",
    "    - ARC: Logical and scientific reasoning.\n",
    "    \n",
    "    - HellaSwag: Commonsense completion.\n",
    "\n",
    "- Collect metrics like accuracy, accuracy_norm, and task-specific scores.\n",
    "\n",
    "- Save results as JSON for later comparison.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Quantitative metrics for the compressed model.\n",
    "\n",
    "- Confidence that the model is good enough in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f45e6-3f29-4922-b85f-2b77be643530",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-10 13:22:55,324\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 13:22:55 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lm_eval.utils import make_table\n",
    "from utils import evaluate, load_pickle, save_pickle\n",
    "\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e43e6d-4fbe-4bd4-91ae-156b6a1c3af5",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfb397-d989-4a4b-8636-511a25c12ed6",
   "metadata": {},
   "source": [
    "## Define evaluation benchmarking datasets\n",
    "The following benchmark datasets can be used for evaluating on multiple tasks:\n",
    "- MMLU: General knowledge across 57 subjects\n",
    "- IFeval: Instruction-following capability\n",
    "- ARC: Logical & scientific reasoning\n",
    "- HellaSwag: Commonsense completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383befc1-e7b7-4540-af75-a3d5ce28786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tasks you want to evaluate the model on\n",
    "tasks = [\"mmlu\", \"arc_easy\", \"hellaswag\", \"ifeval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddc52-4acd-4230-9681-f84201f19e33",
   "metadata": {},
   "source": [
    "### Evaluating the Base Model with `simple_evaluate`\n",
    "\n",
    "`simple_evaluate` is the **main entry point** in LM Evaluation Harness to evaluate a model across one or multiple benchmark datasets. It handles:\n",
    "\n",
    "1. Wrapping your model (or creating an LM object) to provide a **standardized interface**.\n",
    "2. Preparing inputs and optionally applying **few-shot examples** or **chat/instruction templates**.\n",
    "3. Running the model on benchmark tasks and collecting outputs.\n",
    "4. Computing **evaluation metrics** (accuracy, accuracy_norm, etc.) for each task.\n",
    "5. Returning a **results dictionary** that includes task-level metrics and model configuration info.\n",
    "\n",
    "We have wrapped **simple_evaluate** in a helper function **evaluate** which can be found in [utils.py](utils.py).\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "- **LM object**:  \n",
    "  LM Evaluation Harness wraps all models (Hugging Face, custom, or preloaded) in an `LM` object. This object provides a consistent interface (`loglikelihood`, `generate`, etc.) regardless of model backend.\n",
    "\n",
    "- **model_args**:  \n",
    "  Optional dictionary or string containing model-specific arguments (e.g., temperature, top-k, top-p). Ignored if passing a pre-wrapped LM object.\n",
    "\n",
    "- **apply_chat_template**:  \n",
    "  If your model is chat-based or instruction-following, this parameter allows you to prepend a prompt template to match the model's training format.  \n",
    "  \n",
    "**Parameters used here:**\n",
    "- `model`: Path or name of the model to evaluate (can be a string or an LM object).\n",
    "- `model_args`: Optional dictionary to provide model-specific arguments (e.g., batch size, device).\n",
    "- `tasks`: List of task names or objects to evaluate.\n",
    "- `num_fewshot`: Number of examples in the few-shot context (set to 0 for zero-shot).\n",
    "- `batch_size`: Number of samples to process per batch.\n",
    "- `device`: Device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
    "- `apply_chat_template`: Whether to wrap inputs in a chat-style template; useful for chat or instruction-tuned models.\n",
    "- `verbosity`: Set logging level; use `\"DEBUG\"` to inspect inputs/outputs for debugging. Default is None.\n",
    "- `log_samples`: Whether to log per-sample outputs for inspection.\n",
    "\n",
    "\n",
    "**NOTE**: \n",
    "1. Running the evaluation on the entire list of tasks can take long. So for testing, you can use a single task instead.\n",
    "\n",
    "2. The results will be stored as a **results.pkl** files in the directories defined by **compressed_results_dir** and **base_results_dir** paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b759ca-440b-4491-abba-486fe2c1afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting directories\n",
    "base_model_path = \"./base_model\"\n",
    "base_results_dir = \"results/base_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff8288-c452-4d04-89cc-ddf57339469b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the base model and save results in pkl format\n",
    "base_acc = evaluate(\n",
    "    base_model_path,\n",
    "    tasks,\n",
    "    limit=None,\n",
    "    batch_size=\"auto\",\n",
    "    apply_chat_template=True,\n",
    "    verbosity=None,\n",
    ")\n",
    "save_pickle(base_results_dir, base_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e51ac-f4c3-4276-b2fe-df595a963443",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results = load_pickle(base_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36511cfb-fac8-4f8c-be17-c2d98ffb8560",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 Tasks                 |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
      "|arc_easy                               |      1|none  |     0|acc                    |↑  |0.8127|±  |0.0080|\n",
      "|                                       |       |none  |     0|acc_norm               |↑  |0.7588|±  |0.0088|\n",
      "|hellaswag                              |      1|none  |     0|acc                    |↑  |0.5742|±  |0.0049|\n",
      "|                                       |       |none  |     0|acc_norm               |↑  |0.7254|±  |0.0045|\n",
      "|ifeval                                 |      4|none  |     0|inst_level_loose_acc   |↑  |0.8513|±  |   N/A|\n",
      "|                                       |       |none  |     0|inst_level_strict_acc  |↑  |0.8189|±  |   N/A|\n",
      "|                                       |       |none  |     0|prompt_level_loose_acc |↑  |0.7874|±  |0.0176|\n",
      "|                                       |       |none  |     0|prompt_level_strict_acc|↑  |0.7449|±  |0.0188|\n",
      "|mmlu                                   |      2|none  |      |acc                    |↑  |0.6321|±  |0.0038|\n",
      "| - humanities                          |      2|none  |      |acc                    |↑  |0.5868|±  |0.0068|\n",
      "|  - formal_logic                       |      1|none  |     0|acc                    |↑  |0.5000|±  |0.0447|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc                    |↑  |0.7455|±  |0.0340|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc                    |↑  |0.7892|±  |0.0286|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc                    |↑  |0.8186|±  |0.0251|\n",
      "|  - international_law                  |      1|none  |     0|acc                    |↑  |0.7686|±  |0.0385|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc                    |↑  |0.7315|±  |0.0428|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc                    |↑  |0.7730|±  |0.0329|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc                    |↑  |0.6792|±  |0.0251|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc                    |↑  |0.4190|±  |0.0165|\n",
      "|  - philosophy                         |      1|none  |     0|acc                    |↑  |0.6977|±  |0.0261|\n",
      "|  - prehistory                         |      1|none  |     0|acc                    |↑  |0.7130|±  |0.0252|\n",
      "|  - professional_law                   |      1|none  |     0|acc                    |↑  |0.4687|±  |0.0127|\n",
      "|  - world_religions                    |      1|none  |     0|acc                    |↑  |0.8480|±  |0.0275|\n",
      "| - other                               |      2|none  |      |acc                    |↑  |0.7177|±  |0.0078|\n",
      "|  - business_ethics                    |      1|none  |     0|acc                    |↑  |0.6500|±  |0.0479|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc                    |↑  |0.7094|±  |0.0279|\n",
      "|  - college_medicine                   |      1|none  |     0|acc                    |↑  |0.6416|±  |0.0366|\n",
      "|  - global_facts                       |      1|none  |     0|acc                    |↑  |0.4200|±  |0.0496|\n",
      "|  - human_aging                        |      1|none  |     0|acc                    |↑  |0.6771|±  |0.0314|\n",
      "|  - management                         |      1|none  |     0|acc                    |↑  |0.8058|±  |0.0392|\n",
      "|  - marketing                          |      1|none  |     0|acc                    |↑  |0.8547|±  |0.0231|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc                    |↑  |0.8000|±  |0.0402|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc                    |↑  |0.8084|±  |0.0141|\n",
      "|  - nutrition                          |      1|none  |     0|acc                    |↑  |0.7516|±  |0.0247|\n",
      "|  - professional_accounting            |      1|none  |     0|acc                    |↑  |0.5177|±  |0.0298|\n",
      "|  - professional_medicine              |      1|none  |     0|acc                    |↑  |0.7610|±  |0.0259|\n",
      "|  - virology                           |      1|none  |     0|acc                    |↑  |0.5663|±  |0.0386|\n",
      "| - social sciences                     |      2|none  |      |acc                    |↑  |0.7442|±  |0.0077|\n",
      "|  - econometrics                       |      1|none  |     0|acc                    |↑  |0.4386|±  |0.0467|\n",
      "|  - high_school_geography              |      1|none  |     0|acc                    |↑  |0.7929|±  |0.0289|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc                    |↑  |0.8497|±  |0.0258|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc                    |↑  |0.6564|±  |0.0241|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc                    |↑  |0.7479|±  |0.0282|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc                    |↑  |0.8642|±  |0.0147|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc                    |↑  |0.7634|±  |0.0373|\n",
      "|  - professional_psychology            |      1|none  |     0|acc                    |↑  |0.6797|±  |0.0189|\n",
      "|  - public_relations                   |      1|none  |     0|acc                    |↑  |0.6909|±  |0.0443|\n",
      "|  - security_studies                   |      1|none  |     0|acc                    |↑  |0.6898|±  |0.0296|\n",
      "|  - sociology                          |      1|none  |     0|acc                    |↑  |0.8308|±  |0.0265|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc                    |↑  |0.8600|±  |0.0349|\n",
      "| - stem                                |      2|none  |      |acc                    |↑  |0.5059|±  |0.0084|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc                    |↑  |0.2700|±  |0.0446|\n",
      "|  - anatomy                            |      1|none  |     0|acc                    |↑  |0.6222|±  |0.0419|\n",
      "|  - astronomy                          |      1|none  |     0|acc                    |↑  |0.7039|±  |0.0372|\n",
      "|  - college_biology                    |      1|none  |     0|acc                    |↑  |0.7639|±  |0.0355|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc                    |↑  |0.4700|±  |0.0502|\n",
      "|  - college_computer_science           |      1|none  |     0|acc                    |↑  |0.4100|±  |0.0494|\n",
      "|  - college_mathematics                |      1|none  |     0|acc                    |↑  |0.2800|±  |0.0451|\n",
      "|  - college_physics                    |      1|none  |     0|acc                    |↑  |0.3824|±  |0.0484|\n",
      "|  - computer_security                  |      1|none  |     0|acc                    |↑  |0.7200|±  |0.0451|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc                    |↑  |0.5915|±  |0.0321|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc                    |↑  |0.5724|±  |0.0412|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc                    |↑  |0.3942|±  |0.0252|\n",
      "|  - high_school_biology                |      1|none  |     0|acc                    |↑  |0.7581|±  |0.0244|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc                    |↑  |0.5123|±  |0.0352|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc                    |↑  |0.6100|±  |0.0490|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc                    |↑  |0.2481|±  |0.0263|\n",
      "|  - high_school_physics                |      1|none  |     0|acc                    |↑  |0.3709|±  |0.0394|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc                    |↑  |0.4213|±  |0.0337|\n",
      "|  - machine_learning                   |      1|none  |     0|acc                    |↑  |0.4911|±  |0.0475|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print results for the base model\n",
    "print(make_table(base_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
