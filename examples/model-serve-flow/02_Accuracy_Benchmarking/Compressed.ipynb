{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dec15d-d789-4e22-b9ea-5eec04bf9230",
   "metadata": {},
   "source": [
    "## Evaluate Accuracy of the Compressed Model\n",
    "After compression, this step evaluates the compressed model on standard benchmarks to determine how compression affects its accuracy and generative quality relative to the base model.\n",
    "\n",
    "**Goal**: Establish the performance and accuracy of the compressed model and compare it later against the baseline to understand the impact of compression.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- We will create a function called **evaluate** that uses `simple_evaluate` from LM Eval to test the compressed model.\n",
    "\n",
    "- Benchmark on multiple datasets:\n",
    "\n",
    "    - MMLU: General knowledge across subjects.\n",
    "\n",
    "    - IFeval: Instruction-following tasks.\n",
    "\n",
    "    - ARC: Logical and scientific reasoning.\n",
    "    \n",
    "    - HellaSwag: Commonsense completion.\n",
    "\n",
    "- Collect metrics like accuracy, accuracy_norm, and task-specific scores.\n",
    "\n",
    "- Save results as JSON for later comparison.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Quantitative metrics for the compressed model.\n",
    "\n",
    "- Confidence that the model is good enough in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1f45e6-3f29-4922-b85f-2b77be643530",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from lm_eval.utils import make_table\n",
    "from utils import evaluate, load_pickle, save_pickle\n",
    "\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e3f9b-f437-4f3f-9391-c23b31fc5ef5",
   "metadata": {},
   "source": [
    "To make sure you have enough GPU memory to run this notebook, run the following command in terminal:\n",
    "\n",
    "`nvidia-smi`\n",
    "\n",
    "The output will look something like this:\n",
    "\n",
    "```text\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA L40S                    On  |   00000000:CA:00.0 Off |                    0 |\n",
    "| N/A   44C    P0             91W /  350W |   15753MiB /  46068MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A            8049      C   /opt/app-root/bin/python3             15744MiB |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "\n",
    "Note the PID and run the following command:\n",
    "```\n",
    "`kill -9 <pid>`\n",
    "\n",
    "\n",
    "Replace <pid> with the actual PID for example `8049` in this case. So the command will become `kill -9 8094`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e43e6d-4fbe-4bd4-91ae-156b6a1c3af5",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfb397-d989-4a4b-8636-511a25c12ed6",
   "metadata": {},
   "source": [
    "## Define evaluation benchmarking datasets\n",
    "We evaluate the compressed model on the same benchmarks as the base model to make the results comparable.\n",
    "The following benchmark datasets can be used for evaluating on multiple tasks:\n",
    "- MMLU: General knowledge across 57 subjects\n",
    "- IFeval: Instruction-following capability\n",
    "- ARC: Logical & scientific reasoning\n",
    "- HellaSwag: Commonsense completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383befc1-e7b7-4540-af75-a3d5ce28786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tasks you want to evaluate the model on\n",
    "tasks = [\"mmlu\", \"arc_easy\", \"hellaswag\", \"ifeval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddc52-4acd-4230-9681-f84201f19e33",
   "metadata": {},
   "source": [
    "### Evaluating the Compressed Model\n",
    "\n",
    "**NOTE**: \n",
    "1. Running the evaluation on the entire list of tasks can take long. So for testing, you can use a single task instead.\n",
    "\n",
    "2. The results will be stored as a **results.pkl** files in the directories defined by **compressed_results_dir**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b759ca-440b-4491-abba-486fe2c1afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting directories\n",
    "compressed_model_path = \"../Llama_3.1_8B_Instruct_int8_dynamic\"\n",
    "compressed_results_dir = \"results/compressed_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c849dd-535e-442c-82e9-d1fbeed500d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/transformers/quantizers/auto.py:231: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['run_compressed']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7833f6e1-4675-4100-b0d2-9bf05d5490ff)')' thrown while requesting HEAD https://huggingface.co/datasets/cais/mmlu/resolve/c30699e8356da336a370243923dbaf21066bb9fe/.huggingface.yaml\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_abstract_algebra from None to 0\n",
      "Overwriting default num_fewshot of mmlu_anatomy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_astronomy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_biology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_chemistry from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_computer_science from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_mathematics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_physics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_computer_security from None to 0\n",
      "Overwriting default num_fewshot of mmlu_conceptual_physics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_electrical_engineering from None to 0\n",
      "Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_biology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_physics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_statistics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_machine_learning from None to 0\n",
      "Overwriting default num_fewshot of mmlu_business_ethics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 0\n",
      "Overwriting default num_fewshot of mmlu_college_medicine from None to 0\n",
      "Overwriting default num_fewshot of mmlu_global_facts from None to 0\n",
      "Overwriting default num_fewshot of mmlu_human_aging from None to 0\n",
      "Overwriting default num_fewshot of mmlu_management from None to 0\n",
      "Overwriting default num_fewshot of mmlu_marketing from None to 0\n",
      "Overwriting default num_fewshot of mmlu_medical_genetics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_miscellaneous from None to 0\n",
      "Overwriting default num_fewshot of mmlu_nutrition from None to 0\n",
      "Overwriting default num_fewshot of mmlu_professional_accounting from None to 0\n",
      "Overwriting default num_fewshot of mmlu_professional_medicine from None to 0\n",
      "Overwriting default num_fewshot of mmlu_virology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_econometrics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_geography from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_psychology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_human_sexuality from None to 0\n",
      "Overwriting default num_fewshot of mmlu_professional_psychology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_public_relations from None to 0\n",
      "Overwriting default num_fewshot of mmlu_security_studies from None to 0\n",
      "Overwriting default num_fewshot of mmlu_sociology from None to 0\n",
      "Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_formal_logic from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_european_history from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_us_history from None to 0\n",
      "Overwriting default num_fewshot of mmlu_high_school_world_history from None to 0\n",
      "Overwriting default num_fewshot of mmlu_international_law from None to 0\n",
      "Overwriting default num_fewshot of mmlu_jurisprudence from None to 0\n",
      "Overwriting default num_fewshot of mmlu_logical_fallacies from None to 0\n",
      "Overwriting default num_fewshot of mmlu_moral_disputes from None to 0\n",
      "Overwriting default num_fewshot of mmlu_moral_scenarios from None to 0\n",
      "Overwriting default num_fewshot of mmlu_philosophy from None to 0\n",
      "Overwriting default num_fewshot of mmlu_prehistory from None to 0\n",
      "Overwriting default num_fewshot of mmlu_professional_law from None to 0\n",
      "Overwriting default num_fewshot of mmlu_world_religions from None to 0\n",
      "Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "100%|██████████| 541/541 [00:00<00:00, 12764.35it/s]\n",
      "100%|██████████| 10042/10042 [00:03<00:00, 2667.81it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1395.85it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 812.97it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 818.78it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 821.41it/s]\n",
      "100%|██████████| 144/144 [00:00<00:00, 824.00it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 818.06it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 812.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 814.79it/s]\n",
      "100%|██████████| 102/102 [00:00<00:00, 816.81it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 813.96it/s]\n",
      "100%|██████████| 235/235 [00:00<00:00, 820.71it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 814.95it/s]\n",
      "100%|██████████| 378/378 [00:00<00:00, 822.20it/s]\n",
      "100%|██████████| 310/310 [00:00<00:00, 820.57it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 817.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 822.54it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 822.63it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 820.17it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 823.30it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 820.39it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 820.10it/s]\n",
      "100%|██████████| 265/265 [00:00<00:00, 822.23it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 819.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 820.52it/s]\n",
      "100%|██████████| 223/223 [00:00<00:00, 820.36it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 820.22it/s]\n",
      "100%|██████████| 234/234 [00:00<00:00, 823.39it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 819.86it/s]\n",
      "100%|██████████| 783/783 [00:00<00:00, 822.77it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 820.90it/s]\n",
      "100%|██████████| 282/282 [00:00<00:00, 825.89it/s]\n",
      "100%|██████████| 272/272 [00:00<00:00, 823.14it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 819.34it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 816.59it/s]\n",
      "100%|██████████| 198/198 [00:00<00:00, 819.52it/s]\n",
      "100%|██████████| 193/193 [00:00<00:00, 820.98it/s]\n",
      "100%|██████████| 390/390 [00:00<00:00, 830.96it/s]\n",
      "100%|██████████| 238/238 [00:00<00:00, 821.95it/s]\n",
      "100%|██████████| 545/545 [00:00<00:00, 825.11it/s]\n",
      "100%|██████████| 131/131 [00:00<00:00, 825.04it/s]\n",
      "100%|██████████| 612/612 [00:00<00:00, 828.49it/s]\n",
      "100%|██████████| 110/110 [00:00<00:00, 813.12it/s]\n",
      "100%|██████████| 245/245 [00:00<00:00, 823.13it/s]\n",
      "100%|██████████| 201/201 [00:00<00:00, 827.82it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 824.92it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 811.35it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 818.16it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 819.07it/s]\n",
      "100%|██████████| 237/237 [00:00<00:00, 820.87it/s]\n",
      "100%|██████████| 121/121 [00:00<00:00, 825.48it/s]\n",
      "100%|██████████| 108/108 [00:00<00:00, 822.85it/s]\n",
      "100%|██████████| 163/163 [00:00<00:00, 824.13it/s]\n",
      "100%|██████████| 346/346 [00:00<00:00, 818.55it/s]\n",
      "100%|██████████| 895/895 [00:01<00:00, 828.98it/s]\n",
      "100%|██████████| 311/311 [00:00<00:00, 833.30it/s]\n",
      "100%|██████████| 324/324 [00:00<00:00, 832.79it/s]\n",
      "100%|██████████| 1534/1534 [00:01<00:00, 831.72it/s]\n",
      "100%|██████████| 171/171 [00:00<00:00, 833.32it/s]\n",
      "Running generate_until requests:   0%|          | 0/541 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:   0%|          | 1/541 [04:07<37:07:19, 247.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:   3%|▎         | 17/541 [07:41<3:22:43, 23.21s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:   6%|▌         | 33/541 [15:24<3:44:16, 26.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:   9%|▉         | 49/541 [18:59<2:48:56, 20.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  12%|█▏        | 65/541 [22:55<2:25:08, 18.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  15%|█▍        | 81/541 [30:35<2:49:03, 22.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  18%|█▊        | 97/541 [38:14<2:59:51, 24.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  21%|██        | 113/541 [42:30<2:34:01, 21.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  24%|██▍       | 129/541 [50:07<2:43:33, 23.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  27%|██▋       | 145/541 [54:46<2:23:57, 21.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  30%|██▉       | 161/541 [1:02:24<2:31:34, 23.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  33%|███▎      | 177/541 [1:10:06<2:34:16, 25.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  36%|███▌      | 193/541 [1:13:09<2:02:51, 21.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  39%|███▊      | 209/541 [1:17:49<1:51:02, 20.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  42%|████▏     | 225/541 [1:25:27<1:59:16, 22.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  45%|████▍     | 241/541 [1:30:00<1:44:51, 20.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  48%|████▊     | 257/541 [1:37:40<1:50:18, 23.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  50%|█████     | 273/541 [1:42:34<1:37:26, 21.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  53%|█████▎    | 289/541 [1:50:11<1:40:11, 23.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  56%|█████▋    | 305/541 [1:53:31<1:20:22, 20.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  59%|█████▉    | 321/541 [2:01:09<1:23:57, 22.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  62%|██████▏   | 337/541 [2:07:32<1:18:54, 23.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  68%|██████▊   | 369/541 [2:16:44<57:30, 20.06s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  71%|███████   | 385/541 [2:24:21<58:49, 22.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  74%|███████▍  | 401/541 [2:32:00<57:01, 24.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  77%|███████▋  | 417/541 [2:35:43<43:58, 21.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  80%|████████  | 433/541 [2:43:19<42:12, 23.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  83%|████████▎ | 449/541 [2:50:58<38:21, 25.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  86%|████████▌ | 465/541 [2:55:20<28:24, 22.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  89%|████████▉ | 481/541 [3:03:00<24:19, 24.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  92%|█████████▏| 497/541 [3:07:20<16:03, 21.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests:  95%|█████████▍| 513/541 [3:14:35<10:57, 23.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests: 100%|██████████| 541/541 [3:17:52<00:00, 21.95s/it]\n",
      "Running loglikelihood requests:  84%|████████▍ | 89365/105837 [25:59<07:22, 37.20it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the compressed model and save results in pkl format\n",
    "comp_acc = evaluate(\n",
    "    compressed_model_path,\n",
    "    tasks,\n",
    "    limit=None,\n",
    "    batch_size=16,\n",
    "    apply_chat_template=True,\n",
    "    verbosity=None,\n",
    ")\n",
    "save_pickle(compressed_results_dir, comp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c20e51ac-f4c3-4276-b2fe-df595a963443",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_results = load_pickle(compressed_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd79a881-9bcc-4860-a3a1-7b7da82d0138",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 Tasks                 |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
      "|arc_easy                               |      1|none  |     0|acc                    |↑  |0.8106|±  |0.0080|\n",
      "|                                       |       |none  |     0|acc_norm               |↑  |0.7555|±  |0.0088|\n",
      "|hellaswag                              |      1|none  |     0|acc                    |↑  |0.5734|±  |0.0049|\n",
      "|                                       |       |none  |     0|acc_norm               |↑  |0.7277|±  |0.0044|\n",
      "|ifeval                                 |      4|none  |     0|inst_level_loose_acc   |↑  |0.8549|±  |   N/A|\n",
      "|                                       |       |none  |     0|inst_level_strict_acc  |↑  |0.8237|±  |   N/A|\n",
      "|                                       |       |none  |     0|prompt_level_loose_acc |↑  |0.7893|±  |0.0175|\n",
      "|                                       |       |none  |     0|prompt_level_strict_acc|↑  |0.7449|±  |0.0188|\n",
      "|mmlu                                   |      2|none  |      |acc                    |↑  |0.6311|±  |0.0038|\n",
      "| - humanities                          |      2|none  |      |acc                    |↑  |0.5911|±  |0.0068|\n",
      "|  - formal_logic                       |      1|none  |     0|acc                    |↑  |0.4921|±  |0.0447|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc                    |↑  |0.7697|±  |0.0329|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc                    |↑  |0.7990|±  |0.0281|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc                    |↑  |0.8186|±  |0.0251|\n",
      "|  - international_law                  |      1|none  |     0|acc                    |↑  |0.7686|±  |0.0385|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc                    |↑  |0.7500|±  |0.0419|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc                    |↑  |0.7669|±  |0.0332|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc                    |↑  |0.6792|±  |0.0251|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc                    |↑  |0.4369|±  |0.0166|\n",
      "|  - philosophy                         |      1|none  |     0|acc                    |↑  |0.6913|±  |0.0262|\n",
      "|  - prehistory                         |      1|none  |     0|acc                    |↑  |0.7191|±  |0.0250|\n",
      "|  - professional_law                   |      1|none  |     0|acc                    |↑  |0.4687|±  |0.0127|\n",
      "|  - world_religions                    |      1|none  |     0|acc                    |↑  |0.8363|±  |0.0284|\n",
      "| - other                               |      2|none  |      |acc                    |↑  |0.7132|±  |0.0079|\n",
      "|  - business_ethics                    |      1|none  |     0|acc                    |↑  |0.6500|±  |0.0479|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc                    |↑  |0.7019|±  |0.0282|\n",
      "|  - college_medicine                   |      1|none  |     0|acc                    |↑  |0.6474|±  |0.0364|\n",
      "|  - global_facts                       |      1|none  |     0|acc                    |↑  |0.4100|±  |0.0494|\n",
      "|  - human_aging                        |      1|none  |     0|acc                    |↑  |0.6861|±  |0.0311|\n",
      "|  - management                         |      1|none  |     0|acc                    |↑  |0.7864|±  |0.0406|\n",
      "|  - marketing                          |      1|none  |     0|acc                    |↑  |0.8462|±  |0.0236|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc                    |↑  |0.7700|±  |0.0423|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc                    |↑  |0.8059|±  |0.0141|\n",
      "|  - nutrition                          |      1|none  |     0|acc                    |↑  |0.7614|±  |0.0244|\n",
      "|  - professional_accounting            |      1|none  |     0|acc                    |↑  |0.4965|±  |0.0298|\n",
      "|  - professional_medicine              |      1|none  |     0|acc                    |↑  |0.7721|±  |0.0255|\n",
      "|  - virology                           |      1|none  |     0|acc                    |↑  |0.5361|±  |0.0388|\n",
      "| - social sciences                     |      2|none  |      |acc                    |↑  |0.7394|±  |0.0077|\n",
      "|  - econometrics                       |      1|none  |     0|acc                    |↑  |0.4474|±  |0.0468|\n",
      "|  - high_school_geography              |      1|none  |     0|acc                    |↑  |0.7778|±  |0.0296|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc                    |↑  |0.8187|±  |0.0278|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc                    |↑  |0.6487|±  |0.0242|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc                    |↑  |0.7437|±  |0.0284|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc                    |↑  |0.8606|±  |0.0149|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc                    |↑  |0.7634|±  |0.0373|\n",
      "|  - professional_psychology            |      1|none  |     0|acc                    |↑  |0.6814|±  |0.0189|\n",
      "|  - public_relations                   |      1|none  |     0|acc                    |↑  |0.6636|±  |0.0453|\n",
      "|  - security_studies                   |      1|none  |     0|acc                    |↑  |0.6857|±  |0.0297|\n",
      "|  - sociology                          |      1|none  |     0|acc                    |↑  |0.8408|±  |0.0259|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc                    |↑  |0.8600|±  |0.0349|\n",
      "| - stem                                |      2|none  |      |acc                    |↑  |0.5043|±  |0.0084|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc                    |↑  |0.2500|±  |0.0435|\n",
      "|  - anatomy                            |      1|none  |     0|acc                    |↑  |0.6444|±  |0.0414|\n",
      "|  - astronomy                          |      1|none  |     0|acc                    |↑  |0.6842|±  |0.0378|\n",
      "|  - college_biology                    |      1|none  |     0|acc                    |↑  |0.7431|±  |0.0365|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc                    |↑  |0.4500|±  |0.0500|\n",
      "|  - college_computer_science           |      1|none  |     0|acc                    |↑  |0.4200|±  |0.0496|\n",
      "|  - college_mathematics                |      1|none  |     0|acc                    |↑  |0.2700|±  |0.0446|\n",
      "|  - college_physics                    |      1|none  |     0|acc                    |↑  |0.3824|±  |0.0484|\n",
      "|  - computer_security                  |      1|none  |     0|acc                    |↑  |0.7300|±  |0.0446|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc                    |↑  |0.6000|±  |0.0320|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc                    |↑  |0.6069|±  |0.0407|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc                    |↑  |0.4048|±  |0.0253|\n",
      "|  - high_school_biology                |      1|none  |     0|acc                    |↑  |0.7774|±  |0.0237|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc                    |↑  |0.4729|±  |0.0351|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc                    |↑  |0.5800|±  |0.0496|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc                    |↑  |0.2519|±  |0.0265|\n",
      "|  - high_school_physics                |      1|none  |     0|acc                    |↑  |0.3444|±  |0.0388|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc                    |↑  |0.4213|±  |0.0337|\n",
      "|  - machine_learning                   |      1|none  |     0|acc                    |↑  |0.4732|±  |0.0474|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print results for the compressed model\n",
    "print(make_table(comp_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
