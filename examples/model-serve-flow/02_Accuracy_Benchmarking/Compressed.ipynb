{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dec15d-d789-4e22-b9ea-5eec04bf9230",
   "metadata": {},
   "source": [
    "## Evaluate Accuracy of the Compressed Model\n",
    "After compression, it’s important to ensure the model maintains its generative capabilities. This step evaluates the compressed model on standard benchmarks.\n",
    "\n",
    "**Goal**: Verify that compression does not degrade model quality.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- We will create a function called **evaluate** that uses simple_evaluate from LM Eval to test the compressed model.\n",
    "\n",
    "- Benchmark on multiple datasets:\n",
    "\n",
    "    - MMLU: General knowledge across subjects.\n",
    "\n",
    "    - IFeval: Instruction-following tasks.\n",
    "\n",
    "    - ARC: Logical and scientific reasoning.\n",
    "    \n",
    "    - HellaSwag: Commonsense completion.\n",
    "\n",
    "- Collect metrics like accuracy, accuracy_norm, and task-specific scores.\n",
    "\n",
    "- Save results as JSON for later comparison.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Quantitative metrics for the compressed model.\n",
    "\n",
    "- Confidence that the model is good enough in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f45e6-3f29-4922-b85f-2b77be643530",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-10 13:22:55,324\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 13:22:55 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lm_eval.utils import make_table\n",
    "from utils import evaluate, load_pickle, save_pickle\n",
    "\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e43e6d-4fbe-4bd4-91ae-156b6a1c3af5",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfb397-d989-4a4b-8636-511a25c12ed6",
   "metadata": {},
   "source": [
    "## Define evaluation benchmarking datasets\n",
    "The following benchmark datasets can be used for evaluating on multiple tasks:\n",
    "- MMLU: General knowledge across 57 subjects\n",
    "- IFeval: Instruction-following capability\n",
    "- ARC: Logical & scientific reasoning\n",
    "- HellaSwag: Commonsense completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383befc1-e7b7-4540-af75-a3d5ce28786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tasks you want to evaluate the model on\n",
    "tasks = [\"mmlu\", \"arc_easy\", \"hellaswag\", \"ifeval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddc52-4acd-4230-9681-f84201f19e33",
   "metadata": {},
   "source": [
    "### Evaluating the Compressed Model with `simple_evaluate`\n",
    "\n",
    "`simple_evaluate` is the **main entry point** in LM Evaluation Harness to evaluate a model across one or multiple benchmark datasets. It handles:\n",
    "\n",
    "1. Wrapping your model (or creating an LM object) to provide a **standardized interface**.\n",
    "2. Preparing inputs and optionally applying **few-shot examples** or **chat/instruction templates**.\n",
    "3. Running the model on benchmark tasks and collecting outputs.\n",
    "4. Computing **evaluation metrics** (accuracy, accuracy_norm, etc.) for each task.\n",
    "5. Returning a **results dictionary** that includes task-level metrics and model configuration info.\n",
    "\n",
    "We have wrapped **simple_evaluate** in a helper function **evaluate** which can be found in [utils.py](utils.py).\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "- **LM object**:  \n",
    "  LM Evaluation Harness wraps all models (Hugging Face, custom, or preloaded) in an `LM` object. This object provides a consistent interface (`loglikelihood`, `generate`, etc.) regardless of model backend.\n",
    "\n",
    "- **model_args**:  \n",
    "  Optional dictionary or string containing model-specific arguments (e.g., temperature, top-k, top-p). Ignored if passing a pre-wrapped LM object.\n",
    "\n",
    "- **apply_chat_template**:  \n",
    "  If your model is chat-based or instruction-following, this parameter allows you to prepend a prompt template to match the model's training format.  \n",
    "  \n",
    "**Parameters used here:**\n",
    "- `model`: Path or name of the model to evaluate (can be a string or an LM object).\n",
    "- `model_args`: Optional dictionary to provide model-specific arguments (e.g., batch size, device).\n",
    "- `tasks`: List of task names or objects to evaluate.\n",
    "- `num_fewshot`: Number of examples in the few-shot context (set to 0 for zero-shot).\n",
    "- `batch_size`: Number of samples to process per batch.\n",
    "- `device`: Device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
    "- `apply_chat_template`: Whether to wrap inputs in a chat-style template; useful for chat or instruction-tuned models.\n",
    "- `verbosity`: Set logging level; use `\"DEBUG\"` to inspect inputs/outputs for debugging. Default is None.\n",
    "- `log_samples`: Whether to log per-sample outputs for inspection.\n",
    "\n",
    "\n",
    "**NOTE**: \n",
    "1. Running the evaluation on the entire list of tasks can take long. So for testing, you can use a single task instead.\n",
    "\n",
    "2. The results will be stored as a **results.pkl** files in the directories defined by **compressed_results_dir** and **base_results_dir** paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b759ca-440b-4491-abba-486fe2c1afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting directories\n",
    "compressed_model_path = \"Llama_3.1_8B_Instruct_int8_dynamic\"\n",
    "compressed_results_dir = \"results/compressed_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c849dd-535e-442c-82e9-d1fbeed500d1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the compressed model and save results in pkl format\n",
    "comp_acc = evaluate(\n",
    "    compressed_model_path,\n",
    "    tasks,\n",
    "    limit=None,\n",
    "    batch_size=16,\n",
    "    apply_chat_template=True,\n",
    "    verbosity=None,\n",
    ")\n",
    "save_pickle(compressed_results_dir, comp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e51ac-f4c3-4276-b2fe-df595a963443",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_results = load_pickle(compressed_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79a881-9bcc-4860-a3a1-7b7da82d0138",
   "metadata": {
    "tags": [
     "parameters",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 Tasks                 |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
      "|arc_easy                               |      1|none  |     0|acc                    |↑  |0.8114|±  |0.0080|\n",
      "|                                       |       |none  |     0|acc_norm               |↑  |0.7584|±  |0.0088|\n",
      "|hellaswag                              |      1|none  |     0|acc                    |↑  |0.5756|±  |0.0049|\n",
      "|                                       |       |none  |     0|acc_norm               |↑  |0.7261|±  |0.0045|\n",
      "|ifeval                                 |      4|none  |     0|inst_level_loose_acc   |↑  |0.8609|±  |   N/A|\n",
      "|                                       |       |none  |     0|inst_level_strict_acc  |↑  |0.8225|±  |   N/A|\n",
      "|                                       |       |none  |     0|prompt_level_loose_acc |↑  |0.8004|±  |0.0172|\n",
      "|                                       |       |none  |     0|prompt_level_strict_acc|↑  |0.7468|±  |0.0187|\n",
      "|mmlu                                   |      2|none  |      |acc                    |↑  |0.6322|±  |0.0038|\n",
      "| - humanities                          |      2|none  |      |acc                    |↑  |0.5853|±  |0.0068|\n",
      "|  - formal_logic                       |      1|none  |     0|acc                    |↑  |0.4683|±  |0.0446|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc                    |↑  |0.7455|±  |0.0340|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc                    |↑  |0.7892|±  |0.0286|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc                    |↑  |0.8101|±  |0.0255|\n",
      "|  - international_law                  |      1|none  |     0|acc                    |↑  |0.7686|±  |0.0385|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc                    |↑  |0.7315|±  |0.0428|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc                    |↑  |0.7730|±  |0.0329|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc                    |↑  |0.6792|±  |0.0251|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc                    |↑  |0.4145|±  |0.0165|\n",
      "|  - philosophy                         |      1|none  |     0|acc                    |↑  |0.6945|±  |0.0262|\n",
      "|  - prehistory                         |      1|none  |     0|acc                    |↑  |0.7130|±  |0.0252|\n",
      "|  - professional_law                   |      1|none  |     0|acc                    |↑  |0.4713|±  |0.0127|\n",
      "|  - world_religions                    |      1|none  |     0|acc                    |↑  |0.8480|±  |0.0275|\n",
      "| - other                               |      2|none  |      |acc                    |↑  |0.7181|±  |0.0078|\n",
      "|  - business_ethics                    |      1|none  |     0|acc                    |↑  |0.6600|±  |0.0476|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc                    |↑  |0.7019|±  |0.0282|\n",
      "|  - college_medicine                   |      1|none  |     0|acc                    |↑  |0.6358|±  |0.0367|\n",
      "|  - global_facts                       |      1|none  |     0|acc                    |↑  |0.4200|±  |0.0496|\n",
      "|  - human_aging                        |      1|none  |     0|acc                    |↑  |0.6771|±  |0.0314|\n",
      "|  - management                         |      1|none  |     0|acc                    |↑  |0.7961|±  |0.0399|\n",
      "|  - marketing                          |      1|none  |     0|acc                    |↑  |0.8547|±  |0.0231|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc                    |↑  |0.7800|±  |0.0416|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc                    |↑  |0.8110|±  |0.0140|\n",
      "|  - nutrition                          |      1|none  |     0|acc                    |↑  |0.7614|±  |0.0244|\n",
      "|  - professional_accounting            |      1|none  |     0|acc                    |↑  |0.5284|±  |0.0298|\n",
      "|  - professional_medicine              |      1|none  |     0|acc                    |↑  |0.7463|±  |0.0264|\n",
      "|  - virology                           |      1|none  |     0|acc                    |↑  |0.5783|±  |0.0384|\n",
      "| - social sciences                     |      2|none  |      |acc                    |↑  |0.7452|±  |0.0077|\n",
      "|  - econometrics                       |      1|none  |     0|acc                    |↑  |0.4561|±  |0.0469|\n",
      "|  - high_school_geography              |      1|none  |     0|acc                    |↑  |0.7980|±  |0.0286|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc                    |↑  |0.8549|±  |0.0254|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc                    |↑  |0.6538|±  |0.0241|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc                    |↑  |0.7395|±  |0.0285|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc                    |↑  |0.8569|±  |0.0150|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc                    |↑  |0.7557|±  |0.0377|\n",
      "|  - professional_psychology            |      1|none  |     0|acc                    |↑  |0.6846|±  |0.0188|\n",
      "|  - public_relations                   |      1|none  |     0|acc                    |↑  |0.7000|±  |0.0439|\n",
      "|  - security_studies                   |      1|none  |     0|acc                    |↑  |0.6939|±  |0.0295|\n",
      "|  - sociology                          |      1|none  |     0|acc                    |↑  |0.8358|±  |0.0262|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc                    |↑  |0.8700|±  |0.0338|\n",
      "| - stem                                |      2|none  |      |acc                    |↑  |0.5075|±  |0.0084|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc                    |↑  |0.2700|±  |0.0446|\n",
      "|  - anatomy                            |      1|none  |     0|acc                    |↑  |0.6222|±  |0.0419|\n",
      "|  - astronomy                          |      1|none  |     0|acc                    |↑  |0.6974|±  |0.0374|\n",
      "|  - college_biology                    |      1|none  |     0|acc                    |↑  |0.7639|±  |0.0355|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc                    |↑  |0.4600|±  |0.0501|\n",
      "|  - college_computer_science           |      1|none  |     0|acc                    |↑  |0.4100|±  |0.0494|\n",
      "|  - college_mathematics                |      1|none  |     0|acc                    |↑  |0.2600|±  |0.0441|\n",
      "|  - college_physics                    |      1|none  |     0|acc                    |↑  |0.4118|±  |0.0490|\n",
      "|  - computer_security                  |      1|none  |     0|acc                    |↑  |0.7200|±  |0.0451|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc                    |↑  |0.6043|±  |0.0320|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc                    |↑  |0.5793|±  |0.0411|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc                    |↑  |0.3968|±  |0.0252|\n",
      "|  - high_school_biology                |      1|none  |     0|acc                    |↑  |0.7645|±  |0.0241|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc                    |↑  |0.5074|±  |0.0352|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc                    |↑  |0.6200|±  |0.0488|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc                    |↑  |0.2519|±  |0.0265|\n",
      "|  - high_school_physics                |      1|none  |     0|acc                    |↑  |0.3841|±  |0.0397|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc                    |↑  |0.4167|±  |0.0336|\n",
      "|  - machine_learning                   |      1|none  |     0|acc                    |↑  |0.4643|±  |0.0473|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print results for the compressed model\n",
    "print(make_table(comp_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
