{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training with TransformersTrainer\n",
    "\n",
    "This notebook demonstrates how to use `TransformersTrainer` from the Kubeflow SDK to run **distributed fine-tuning** of a HuggingFace model on **Red Hat OpenShift AI**.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this example, we fine-tune **DistilBERT** on the **IMDB sentiment classification** dataset using **2 nodes**.\n",
    "\n",
    "| Feature | Description |\n",
    "| --- | --- |\n",
    "| **Distributed training** | Run the same `transformers.Trainer` code across multiple nodes without manual DDP setup |\n",
    "| **Progress tracking** | View training progress in the OpenShift AI Dashboard (Training Jobs) |\n",
    "| **Checkpointing (optional)** | Persist checkpoints to a shared RWX PVC via `output_dir=\"pvc://...\"` |\n",
    "\n",
    "### Model details\n",
    "\n",
    "- **Model**: `distilbert-base-uncased`\n",
    "- **Task**: binary sentiment classification\n",
    "\n",
    "### Dataset details\n",
    "\n",
    "- **Dataset**: `stanfordnlp/imdb` (train split; we use a 1,000-sample subset)\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- How to define a `train_func()` that uses `transformers.Trainer`\n",
    "- How to configure and submit a multi-node training job with `TransformersTrainer`\n",
    "- Where to monitor training progress in the OpenShift AI Dashboard\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- OpenShift AI (RHOAI) 3.2+ with Kubeflow Trainer v2 enabled\n",
    "- A workbench with Python and access to submit TrainJobs\n",
    "- A shared PVC named `shared` with **ReadWriteMany (RWX)** access mode\n",
    "  - **Suggested size**: 20Gi (model weights + dataset + checkpoints)\n",
    "  - **Workbench mount**: `/opt/app-root/src/shared`\n",
    "  - See `README.md` in this folder for step-by-step PVC setup instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Install the Kubeflow SDK and required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install datasets transformers accelerate huggingface_hub\n",
    "!python3 -m pip install --force-reinstall --no-cache-dir -U \"kubeflow @ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0\"\n",
    "!python3 -m pip install --force-reinstall --no-cache-dir -U ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kubeflow\n",
    "import torch\n",
    "from kubeflow.common.types import KubernetesBackendConfig\n",
    "from kubeflow.trainer import TrainerClient\n",
    "from kubeflow.trainer.rhai import TransformersTrainer\n",
    "from kubernetes import client as k8s\n",
    "\n",
    "print(f\"Kubeflow SDK version: {kubeflow.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This notebook needs:\n",
    "\n",
    "- **Authentication** to talk to the OpenShift/Kubernetes API\n",
    "- **A shared RWX PVC** (model + data + checkpoints)\n",
    "- **Distributed training settings** (`num_nodes`, `resources_per_node`)\n",
    "\n",
    "### Environment variables\n",
    "\n",
    "The following environment variables are required for API authentication:\n",
    "\n",
    "- `OPENSHIFT_API_URL` ‚Äî your cluster API URL (e.g. `https://api.cluster.example.com:6443`)\n",
    "- `NOTEBOOK_USER_TOKEN` ‚Äî an access token for API calls\n",
    "\n",
    "In OpenShift AI workbenches, these are often auto-set.\n",
    "\n",
    "If they are not set in your environment, uncomment and populate the values in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AUTHENTICATION\n",
    "# ============================================================================\n",
    "# If your workbench does not auto-populate these env vars, uncomment and fill them in:\n",
    "#\n",
    "# api_server = \"https://api.your-cluster.example.com:6443\"\n",
    "# token = \"sha256~your-token-here\"\n",
    "\n",
    "api_server = os.getenv(\"OPENSHIFT_API_URL\")\n",
    "token = os.getenv(\"NOTEBOOK_USER_TOKEN\")\n",
    "\n",
    "if not api_server or not token:\n",
    "    raise RuntimeError(\n",
    "        \"OPENSHIFT_API_URL and NOTEBOOK_USER_TOKEN must be set. \"\n",
    "        \"Either set them in your environment or uncomment the values above.\"\n",
    "    )\n",
    "\n",
    "# Configure Kubernetes client\n",
    "configuration = k8s.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.verify_ssl = False  # Set to True if using trusted certificates\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# ============================================================================\n",
    "# PVC MOUNT PATHS\n",
    "# ============================================================================\n",
    "# Workbench mount path\n",
    "#\n",
    "# NOTEBOOK_SHARED_PATH is where *your workbench* sees the PVC.\n",
    "# This depends on which PVC you attached when you created the workbench.\n",
    "# OpenShift AI typically mounts PVCs under:\n",
    "#   /opt/app-root/src/<pvc-name>\n",
    "PVC_NAME = \"shared\"\n",
    "NOTEBOOK_SHARED_PATH = f\"/opt/app-root/src/{PVC_NAME}\"\n",
    "\n",
    "# Training pod mount path\n",
    "#\n",
    "# SDK_MOUNT_PATH is a fixed path used by the Kubeflow SDK when you set:\n",
    "#   TransformersTrainer(output_dir=\"pvc://<pvc-name>/<path>\")\n",
    "# The SDK mounts that PVC at this location inside the training pods.\n",
    "SDK_MOUNT_PATH = \"/mnt/kubeflow-checkpoints\"\n",
    "\n",
    "if not os.path.exists(NOTEBOOK_SHARED_PATH):\n",
    "    print(\n",
    "        \"‚ö†Ô∏è  Expected workbench PVC mount not found at: \"\n",
    "        f\"{NOTEBOOK_SHARED_PATH}\\n\"\n",
    "        \"If your PVC has a different name/mount, update PVC_NAME/NOTEBOOK_SHARED_PATH.\\n\"\n",
    "        \"Tip: in a workbench, PVCs are typically under /opt/app-root/src/.\"\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL + DATASET\n",
    "# ============================================================================\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "# Use the canonical Hub repo ID to avoid brittle revision lookups\n",
    "DATASET_NAME = \"stanfordnlp/imdb\"\n",
    "DATASET_REVISION = \"main\"\n",
    "\n",
    "MODEL_PATH = f\"{NOTEBOOK_SHARED_PATH}/models/{MODEL_NAME}\"\n",
    "DATA_PATH = f\"{NOTEBOOK_SHARED_PATH}/data/imdb_train_1000\"\n",
    "CHECKPOINTS_PATH = f\"{NOTEBOOK_SHARED_PATH}/checkpoints/transformer-trainer\"\n",
    "\n",
    "TRAINING_MODEL_PATH = f\"{SDK_MOUNT_PATH}/models/{MODEL_NAME}\"\n",
    "TRAINING_DATA_PATH = f\"{SDK_MOUNT_PATH}/data/imdb_train_1000\"\n",
    "TRAINING_CHECKPOINTS_PATH = f\"{SDK_MOUNT_PATH}/checkpoints/transformer-trainer\"\n",
    "\n",
    "# ============================================================================\n",
    "# DISTRIBUTED TRAINING\n",
    "# ============================================================================\n",
    "NUM_NODES = 2\n",
    "GPUS_PER_NODE = 1\n",
    "\n",
    "print(f\"API Server: {api_server}\")\n",
    "print(f\"PVC name: {PVC_NAME}\")\n",
    "print(f\"Workbench PVC mount: {NOTEBOOK_SHARED_PATH}\")\n",
    "print(f\"Training pod PVC mount (SDK): {SDK_MOUNT_PATH}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Nodes: {NUM_NODES}\")\n",
    "print(f\"GPUs per node: {GPUS_PER_NODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training function\n",
    "\n",
    "The training function runs inside each training pod as a distributed PyTorch process. `TransformersTrainer` serializes this function and executes it via `torchrun` on each node.\n",
    "\n",
    "### Key points\n",
    "\n",
    "- **All imports must be inside the function** ‚Äî the function is serialized and executed in training pods\n",
    "- **Use `transformers.Trainer` or `trl.SFTTrainer`** ‚Äî both are automatically instrumented\n",
    "- **PyTorch env vars are set for you** ‚Äî `RANK`, `WORLD_SIZE`, `LOCAL_RANK` are set automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func():\n",
    "    \"\"\"Distributed training function for IMDB sentiment classification.\"\"\"\n",
    "    import os\n",
    "\n",
    "    import torch\n",
    "    from datasets import load_from_disk\n",
    "    from transformers import (\n",
    "        AutoModelForSequenceClassification,\n",
    "        AutoTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "\n",
    "    rank = int(os.environ.get(\"RANK\", 0))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "\n",
    "    # The SDK mounts the PVC at a fixed path inside training pods\n",
    "    model_path = \"/mnt/kubeflow-checkpoints/models/distilbert-base-uncased\"\n",
    "    data_path = \"/mnt/kubeflow-checkpoints/data/imdb_train_1000\"\n",
    "\n",
    "    print(f\"üöÄ Starting training on rank {rank}/{world_size}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        print(f\"üîß GPU: {torch.cuda.get_device_name(local_rank)}\")\n",
    "\n",
    "    # Load model + tokenizer from the PVC (downloaded by the workbench)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=2,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "\n",
    "    # Load dataset from the PVC (saved by the workbench)\n",
    "    dataset = load_from_disk(data_path)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "    )\n",
    "\n",
    "    # TransformersTrainer will patch output_dir + checkpoint settings when output_dir=\"pvc://...\" is set\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/tmp/output\",  # placeholder; overridden by the SDK\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "    )\n",
    "\n",
    "    print(f\"üíæ Trainer output_dir: {trainer.args.output_dir}\")\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save final model on rank 0\n",
    "    if rank == 0:\n",
    "        final_path = os.path.join(trainer.args.output_dir, \"final\")\n",
    "        os.makedirs(final_path, exist_ok=True)\n",
    "        trainer.save_model(final_path)\n",
    "        tokenizer.save_pretrained(final_path)\n",
    "        print(f\"‚úÖ Final model saved to {final_path}\")\n",
    "\n",
    "    print(f\"‚úÖ Training complete on rank {rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model and dataset to the shared PVC (recommended)\n",
    "\n",
    "To make training more reliable, we download the model and dataset to the shared PVC from the workbench first.\n",
    "\n",
    "This avoids repeated downloads inside training pods and lets the training job run without direct internet access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Download model to PVC\n",
    "if os.path.exists(MODEL_PATH) and os.listdir(MODEL_PATH):\n",
    "    print(f\"‚úÖ Model already exists at {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"üîÑ Downloading model {MODEL_NAME} to {MODEL_PATH}...\")\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    model.save_pretrained(MODEL_PATH, safe_serialization=True)\n",
    "\n",
    "    print(f\"‚úÖ Model saved to {MODEL_PATH}\")\n",
    "\n",
    "# Download dataset subset to PVC\n",
    "if os.path.exists(DATA_PATH) and os.listdir(DATA_PATH):\n",
    "    print(f\"‚úÖ Dataset already exists at {DATA_PATH}\")\n",
    "else:\n",
    "    print(f\"üîÑ Downloading dataset {DATASET_NAME} (train[:1000]) to {DATA_PATH}...\")\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        DATASET_NAME, split=\"train[:1000]\", revision=DATASET_REVISION\n",
    "    )\n",
    "    dataset.save_to_disk(DATA_PATH)\n",
    "\n",
    "    print(f\"‚úÖ Dataset saved to {DATA_PATH}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model and dataset ready on PVC!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and submit the training job\n",
    "\n",
    "Create a `TransformersTrainer` and submit the job using the `TrainerClient`.\n",
    "\n",
    "We also set `output_dir=\"pvc://...\"` so checkpoints are written to the shared PVC and can be inspected from the workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer.rhai.transformers import PeriodicCheckpointConfig\n",
    "\n",
    "checkpoint_config = PeriodicCheckpointConfig(\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = TransformersTrainer(\n",
    "    func=train_func,\n",
    "    num_nodes=NUM_NODES,\n",
    "    resources_per_node={\n",
    "        \"nvidia.com/gpu\": GPUS_PER_NODE,\n",
    "        \"cpu\": \"4\",\n",
    "        \"memory\": \"16Gi\",\n",
    "    },\n",
    "    # Keep train_func focused; set offline mode at the pod level\n",
    "    env={\n",
    "        \"HF_HUB_OFFLINE\": \"1\",\n",
    "        \"TRANSFORMERS_OFFLINE\": \"1\",\n",
    "    },\n",
    "    # Persist checkpoints on the shared PVC\n",
    "    output_dir=f\"pvc://{PVC_NAME}/checkpoints/transformer-trainer\",\n",
    "    periodic_checkpoint_config=checkpoint_config,\n",
    "    enable_jit_checkpoint=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TransformersTrainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TrainerClient using the explicit API server + token\n",
    "api_client = k8s.ApiClient(configuration)\n",
    "backend_config = KubernetesBackendConfig(\n",
    "    client_configuration=api_client.configuration,\n",
    ")\n",
    "client = TrainerClient(backend_config)\n",
    "\n",
    "runtime = client.backend.get_runtime(\"torch-distributed\")\n",
    "print(f\"‚úÖ Using runtime: {runtime.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the training job\n",
    "JOB_NAME = client.train(trainer=trainer, runtime=runtime)\n",
    "print(f\"Job submitted: {JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the training job\n",
    "\n",
    "Navigate to **Training Jobs** in the OpenShift AI Dashboard to see training progress and job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check job status\n",
    "job = client.get_job(name=JOB_NAME)\n",
    "print(f\"Job: {job.name}\")\n",
    "print(f\"Status: {job.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for job to complete\n",
    "import time\n",
    "\n",
    "print(\"Waiting for job to complete...\")\n",
    "while True:\n",
    "    job = client.get_job(name=JOB_NAME)\n",
    "    print(f\"Status: {job.status}\")\n",
    "    if job.status in [\"Complete\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(15)\n",
    "\n",
    "print(f\"Job finished: {job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the training job to free cluster resources.\n",
    "\n",
    "**Optional:** If you want to clean up all artifacts from the PVC (model, dataset, checkpoints), you can delete these directories from the workbench:\n",
    "\n",
    "```bash\n",
    "rm -rf /opt/app-root/src/shared/models/distilbert-base-uncased\n",
    "rm -rf /opt/app-root/src/shared/data/imdb_train_1000\n",
    "rm -rf /opt/app-root/src/shared/checkpoints/transformer-trainer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the training job\n",
    "client.delete_job(name=JOB_NAME)\n",
    "print(f\"Job {JOB_NAME} deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully run a distributed fine-tuning job with `TransformersTrainer`.\n",
    "\n",
    "### What you accomplished\n",
    "\n",
    "| Step | Description |\n",
    "| --- | --- |\n",
    "| ‚úÖ Model + dataset staging | Downloaded DistilBERT + IMDB subset to a shared PVC from the workbench |\n",
    "| ‚úÖ Distributed training | Ran 2-node training with PyTorch distributed via `torchrun` |\n",
    "| ‚úÖ Monitoring | Viewed progress in the OpenShift AI Dashboard (Training Jobs) |\n",
    "\n",
    "### Why this pattern works well on OpenShift AI\n",
    "\n",
    "- **Repeatable**: training pods load model/data from shared storage\n",
    "- **Scalable**: change `num_nodes` / `resources_per_node` without changing training code\n",
    "- **Observable**: progress tracking is enabled by default for `TransformersTrainer`\n",
    "\n",
    "### How progress tracking works\n",
    "\n",
    "When you use `TransformersTrainer` (default `enable_progression_tracking=True`):\n",
    "\n",
    "1. **Automatic instrumentation**: a `KubeflowProgressCallback` is injected into your Hugging Face `Trainer`\n",
    "2. **Metrics endpoint**: a lightweight HTTP server exposes progress metrics during training\n",
    "3. **Dashboard integration**: the OpenShift AI Dashboard polls and displays progress\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- Increase `NUM_NODES` or adjust `resources_per_node` for larger workloads\n",
    "- Swap in a different model/dataset (as long as you use `transformers.Trainer` or `trl.SFTTrainer`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
