{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "155e8874",
      "metadata": {},
      "source": [
        "# JIT Checkpointing with TransformersTrainer\n",
        "\n",
        "This notebook demonstrates how to enable **Just-In-Time (JIT) checkpointing** for distributed training using `TransformersTrainer` on Red Hat OpenShift AI.\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this example, we fine-tune the **Qwen 2.5 1.5B Instruct** model on the **Stanford Alpaca** instruction-following dataset. The training runs on 2 GPU nodes with **JIT checkpointing enabled**, which automatically saves training state when pods receive a SIGTERM signal (e.g., during preemption, scaling, or graceful shutdown).\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| **JIT Checkpointing** | Automatically save training state on SIGTERM signal (preemption-safe) |\n",
        "| **Periodic Checkpointing** | Configure regular checkpoint saves using `PeriodicCheckpointConfig` |\n",
        "| **Auto-Resume** | Automatically resume training from the latest checkpoint on restart |\n",
        "| **PVC-Based Storage** | Save checkpoints to shared PersistentVolumeClaim for durability |\n",
        "\n",
        "### Why JIT Checkpointing?\n",
        "\n",
        "In cloud and Kubernetes environments, training pods can be preempted or terminated for various reasons:\n",
        "- **Spot/Preemptible instances** - Cost-effective but can be reclaimed\n",
        "- **Kueue preemption** - Higher-priority workloads may preempt lower-priority jobs\n",
        "- **Node maintenance** - Cluster upgrades or node drains\n",
        "- **Resource pressure** - Pods evicted due to memory or resource limits\n",
        "\n",
        "JIT checkpointing ensures that when a pod receives SIGTERM:\n",
        "1. Training pauses safely after the current optimizer step\n",
        "2. Model state, optimizer state, and training progress are saved\n",
        "3. When the job restarts, training automatically resumes from the checkpoint\n",
        "\n",
        "### Model Details\n",
        "\n",
        "**Qwen 2.5 1.5B Instruct** is a compact instruction-tuned language model from the Qwen family:\n",
        "- **Parameters:** 1.5 billion\n",
        "- **Context Length:** 32K tokens\n",
        "- **Languages:** Multilingual with strong English and Chinese support\n",
        "- **Use Case:** Ideal for instruction-following, chat, and text generation tasks\n",
        "- **Why this model?** Small enough to train quickly for demonstration, yet powerful enough for real-world tasks\n",
        "\n",
        "### Dataset Details\n",
        "\n",
        "We use the **Stanford Alpaca** dataset (`tatsu-lab/alpaca`), a widely-used instruction-following dataset:\n",
        "\n",
        "| Property | Value |\n",
        "|----------|-------|\n",
        "| **Source** | Stanford University |\n",
        "| **Size** | 52,000 instruction-response pairs (we use 500 samples for this demo) |\n",
        "| **Format** | Instruction, optional input, and response |\n",
        "| **Use Case** | Instruction-tuning language models |\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "\n",
        "1. **OpenShift AI Cluster** with Kubeflow Trainer v2 enabled\n",
        "2. **Workbench** running Python 3.12+ with GPU access\n",
        "3. **Shared PVC** named `shared` with **ReadWriteMany (RWX)** access mode\n",
        "   - **Recommended Size:** 20Gi (for model weights, dataset, and checkpoints)\n",
        "   - **Mount Path:** `/opt/app-root/src/shared` in the workbench\n",
        "   - See the [README](./README.md) for detailed PVC setup instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1306a076",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Install the Kubeflow SDK and required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c80093",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install kubeflow --no-cache-dir --index-url https://console.redhat.com/api/pypi/public-rhai/rhoai/3.3/cuda12.9-ubi9/simple/\n",
        "!python3 -m pip install datasets transformers accelerate huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b68cb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import kubeflow\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from kubeflow.common.types import KubernetesBackendConfig\n",
        "from kubeflow.trainer import TrainerClient\n",
        "from kubeflow.trainer.rhai import TransformersTrainer\n",
        "from kubeflow.trainer.rhai.transformers import PeriodicCheckpointConfig\n",
        "from kubernetes import client as k8s\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f\"Kubeflow SDK version: {kubeflow.__version__}\")\n",
        "print(\"‚úÖ All imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59e13d5f",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Configure authentication and paths.\n",
        "\n",
        "### Environment Variables\n",
        "\n",
        "The following environment variables are required for API authentication:\n",
        "- `OPENSHIFT_API_URL` - Your OpenShift API server URL (e.g., `https://api.cluster.example.com:6443`)\n",
        "- `NOTEBOOK_USER_TOKEN` - Authentication token for API access\n",
        "\n",
        "**Note:** These are typically auto-set in OpenShift AI workbenches. If not set, uncomment and fill in the values in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e31b9158",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# AUTHENTICATION CONFIGURATION\n",
        "# ============================================================================\n",
        "# These values are typically auto-set in OpenShift AI workbenches.\n",
        "# If not set, uncomment and fill in your values below:\n",
        "\n",
        "# api_server = \"https://api.your-cluster.example.com:6443\"\n",
        "# token = \"sha256~your-token-here\"\n",
        "\n",
        "# Try to get from environment variables first\n",
        "api_server = os.getenv(\"OPENSHIFT_API_URL\")\n",
        "token = os.getenv(\"NOTEBOOK_USER_TOKEN\")\n",
        "\n",
        "if not api_server or not token:\n",
        "    raise RuntimeError(\n",
        "        \"OPENSHIFT_API_URL and NOTEBOOK_USER_TOKEN environment variables are required.\\n\"\n",
        "        \"Either set them in your environment, or uncomment and fill in the values above.\"\n",
        "    )\n",
        "\n",
        "# Configure Kubernetes client\n",
        "configuration = k8s.Configuration()\n",
        "configuration.host = api_server\n",
        "configuration.verify_ssl = False  # Set to True if using trusted certificates\n",
        "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "# ============================================================================\n",
        "# PVC MOUNT PATHS\n",
        "# ============================================================================\n",
        "# Workbench (notebook) mount path\n",
        "#\n",
        "# NOTEBOOK_SHARED_PATH is where *your workbench* sees the PVC.\n",
        "# This depends on what PVC you attached when you created the workbench.\n",
        "# On OpenShift AI, the default mount convention is:\n",
        "#   /opt/app-root/src/<pvc-name>\n",
        "#\n",
        "# If your PVC is not named \"shared\", set PVC_NAME accordingly.\n",
        "PVC_NAME = \"shared\"\n",
        "NOTEBOOK_SHARED_PATH = f\"/opt/app-root/src/{PVC_NAME}\"\n",
        "\n",
        "# Training pods mount path\n",
        "#\n",
        "# SDK_MOUNT_PATH is a fixed path used by the Kubeflow SDK when you set:\n",
        "#   TransformersTrainer(output_dir=\"pvc://<pvc-name>/<path>\")\n",
        "# The SDK mounts the PVC at this location inside the training pods.\n",
        "# (This comes from the SDK constant CHECKPOINT_MOUNT_PATH.)\n",
        "SDK_MOUNT_PATH = \"/mnt/kubeflow-checkpoints\"\n",
        "\n",
        "# Quick sanity check to help users discover the right workbench mount\n",
        "if not os.path.exists(NOTEBOOK_SHARED_PATH):\n",
        "    print(\n",
        "        \"‚ö†Ô∏è  Expected workbench PVC mount not found at: \"\n",
        "        f\"{NOTEBOOK_SHARED_PATH}\\n\"\n",
        "        \"If your PVC has a different name or mount, update PVC_NAME/NOTEBOOK_SHARED_PATH.\\n\"\n",
        "        \"Tip: in a workbench, PVCs are typically under /opt/app-root/src/.\"\n",
        "    )\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "# Paths for notebook operations only (downloading model/data, reading checkpoints)\n",
        "# Note: Training pods use SDK mount convention (/mnt/kubeflow-checkpoints/...)\n",
        "#       which is handled automatically by the pvc:// URI in TransformersTrainer\n",
        "NOTEBOOK_MODEL_PATH = f\"{NOTEBOOK_SHARED_PATH}/models/qwen2.5-1.5b-instruct\"\n",
        "NOTEBOOK_DATA_PATH = f\"{NOTEBOOK_SHARED_PATH}/data/alpaca_processed\"\n",
        "NOTEBOOK_CHECKPOINTS_PATH = f\"{NOTEBOOK_SHARED_PATH}/checkpoints/jit-checkpointing\"\n",
        "\n",
        "print(f\"API Server: {api_server}\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"PVC name: {PVC_NAME}\")\n",
        "print(f\"Workbench PVC mount: {NOTEBOOK_SHARED_PATH}\")\n",
        "print(f\"Training pod PVC mount (SDK): {SDK_MOUNT_PATH}\")\n",
        "print(f\"Notebook Model Path: {NOTEBOOK_MODEL_PATH}\")\n",
        "print(f\"Notebook Data Path: {NOTEBOOK_DATA_PATH}\")\n",
        "print(f\"Notebook Checkpoints Path: {NOTEBOOK_CHECKPOINTS_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d71d2ba",
      "metadata": {},
      "source": [
        "## Download Model and Dataset to Shared PVC\n",
        "\n",
        "Before submitting the training job, we pre-download the model and dataset to the shared PVC. This ensures:\n",
        "- **Offline Training:** Training pods don't need internet access during training\n",
        "- **Faster Startup:** No download delays when training pods start\n",
        "- **Consistency:** All nodes use the same model weights and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a3e3b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model to PVC\n",
        "if os.path.exists(NOTEBOOK_MODEL_PATH) and os.listdir(NOTEBOOK_MODEL_PATH):\n",
        "    print(f\"‚úÖ Model already exists at {NOTEBOOK_MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"üîÑ Downloading model {MODEL_NAME} to {NOTEBOOK_MODEL_PATH}...\")\n",
        "    os.makedirs(NOTEBOOK_MODEL_PATH, exist_ok=True)\n",
        "\n",
        "    # Use fast tokenizer for compatibility\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        MODEL_NAME, use_fast=True, trust_remote_code=True\n",
        "    )\n",
        "    tokenizer.save_pretrained(NOTEBOOK_MODEL_PATH)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model.save_pretrained(NOTEBOOK_MODEL_PATH, safe_serialization=True)\n",
        "    print(f\"‚úÖ Model saved to {NOTEBOOK_MODEL_PATH}\")\n",
        "    print(f\"üìÅ Files: {os.listdir(NOTEBOOK_MODEL_PATH)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a3c1a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and prepare dataset\n",
        "if os.path.exists(NOTEBOOK_DATA_PATH) and os.listdir(NOTEBOOK_DATA_PATH):\n",
        "    print(f\"‚úÖ Dataset already exists at {NOTEBOOK_DATA_PATH}\")\n",
        "else:\n",
        "    print(\"üîÑ Downloading and processing Alpaca dataset...\")\n",
        "    os.makedirs(NOTEBOOK_DATA_PATH, exist_ok=True)\n",
        "\n",
        "    # Load subset of Alpaca dataset\n",
        "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:500]\")\n",
        "\n",
        "    # Load tokenizer for preprocessing\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        NOTEBOOK_MODEL_PATH, use_fast=True, trust_remote_code=True\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def format_instruction(example):\n",
        "        if example.get(\"input\"):\n",
        "            text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
        "        else:\n",
        "            text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
        "        return {\"text\": text}\n",
        "\n",
        "    dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        tokenized = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "        return tokenized\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function, batched=True, remove_columns=[\"text\"]\n",
        "    )\n",
        "    tokenized_dataset.save_to_disk(NOTEBOOK_DATA_PATH)\n",
        "    print(f\"‚úÖ Dataset saved to {NOTEBOOK_DATA_PATH}\")\n",
        "\n",
        "print(\"\\n‚úÖ Model and dataset ready on PVC!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b2c3d4",
      "metadata": {},
      "source": [
        "## Define the Training Function\n",
        "\n",
        "The training function runs inside each training pod as a distributed PyTorch process. TransformersTrainer serializes this function and executes it via `torchrun` on each node.\n",
        "\n",
        "### Training Configuration\n",
        "\n",
        "| Parameter | Value | Description |\n",
        "|-----------|-------|-------------|\n",
        "| `num_train_epochs` | 5 | Multiple epochs to allow time for testing pause/resume |\n",
        "| `per_device_train_batch_size` | 2 | Samples per GPU per step |\n",
        "| `gradient_accumulation_steps` | 4 | Effective batch size = 2 √ó 4 √ó 2 nodes = 16 |\n",
        "| `learning_rate` | 2e-5 | Standard fine-tuning rate |\n",
        "| `save_steps` | 20 | Checkpoint every 20 steps |\n",
        "| `bf16` | True | Use bfloat16 mixed precision |\n",
        "\n",
        "### Key Points\n",
        "\n",
        "- **Supported Trainers:** Use `transformers.Trainer` or `trl.SFTTrainer` - both are auto-instrumented\n",
        "- **No Manual Setup:** JIT checkpointing and progress tracking callbacks are injected automatically\n",
        "- **Local Files Only:** Model and data are loaded from the mounted PVC (no network access needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f6g7h8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_func():\n",
        "    \"\"\"SFT training function using HuggingFace Trainer.\n",
        "\n",
        "    TransformersTrainer automatically:\n",
        "    - Injects JIT checkpoint handler for SIGTERM (preemption-safe)\n",
        "    - Injects KubeflowProgressCallback for real-time metrics\n",
        "    - Auto-resumes from the latest checkpoint when available\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    import torch\n",
        "    from datasets import load_from_disk\n",
        "    from transformers import (\n",
        "        AutoModelForCausalLM,\n",
        "        DataCollatorForLanguageModeling,\n",
        "        PreTrainedTokenizerFast,\n",
        "        Trainer,\n",
        "        TrainingArguments,\n",
        "    )\n",
        "\n",
        "    rank = int(os.environ.get(\"RANK\", 0))\n",
        "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
        "\n",
        "    # Model/data are on the shared PVC mounted at /mnt/kubeflow-checkpoints via SDK's pvc:// URI\n",
        "    model_path = \"/mnt/kubeflow-checkpoints/models/qwen2.5-1.5b-instruct\"\n",
        "    data_path = \"/mnt/kubeflow-checkpoints/data/alpaca_processed\"\n",
        "\n",
        "    print(f\"üöÄ Starting training on rank {rank}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        print(f\"üîß GPU: {torch.cuda.get_device_name(local_rank)}\")\n",
        "\n",
        "    # Load tokenizer directly from tokenizer.json file\n",
        "    # This bypasses AutoTokenizer's hub validation that fails with local paths\n",
        "    print(f\"üì• Loading tokenizer from: {model_path}\")\n",
        "    tokenizer_file = os.path.join(model_path, \"tokenizer.json\")\n",
        "    tokenizer_config_file = os.path.join(model_path, \"tokenizer_config.json\")\n",
        "\n",
        "    # Load tokenizer config to get special tokens\n",
        "    import json\n",
        "\n",
        "    with open(tokenizer_config_file) as f:\n",
        "        tokenizer_config = json.load(f)\n",
        "\n",
        "    tokenizer = PreTrainedTokenizerFast(\n",
        "        tokenizer_file=tokenizer_file,\n",
        "        eos_token=tokenizer_config.get(\"eos_token\", \"<|endoftext|>\"),\n",
        "        pad_token=tokenizer_config.get(\"pad_token\"),\n",
        "        bos_token=tokenizer_config.get(\"bos_token\"),\n",
        "        unk_token=tokenizer_config.get(\"unk_token\"),\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load model directly from local path\n",
        "    print(f\"üì• Loading model from: {model_path}\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map={\"\": local_rank},\n",
        "        local_files_only=True,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Load dataset\n",
        "    print(f\"üì• Loading dataset from: {data_path}\")\n",
        "    tokenized_dataset = load_from_disk(data_path)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "\n",
        "    # TransformersTrainer automatically configures:\n",
        "    # - output_dir: Set from the pvc:// URI (mounted at /mnt/kubeflow-checkpoints)\n",
        "    # - save_strategy, save_steps, save_total_limit: Set from PeriodicCheckpointConfig\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/tmp/output\",  # Placeholder - SDK overrides this\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-5,\n",
        "        logging_steps=5,\n",
        "        report_to=\"none\",\n",
        "        bf16=True,\n",
        "        ddp_find_unused_parameters=False,\n",
        "    )\n",
        "\n",
        "    # Trainer - TransformersTrainer injects:\n",
        "    # - JIT checkpoint handler for SIGTERM\n",
        "    # - KubeflowProgressCallback for real-time metrics\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    print(f\"üíæ Trainer output_dir: {trainer.args.output_dir}\")\n",
        "\n",
        "    # Train - auto-resumes from latest checkpoint if available\n",
        "    trainer.train()\n",
        "\n",
        "    # Save final model (only on rank 0)\n",
        "    if rank == 0:\n",
        "        final_path = os.path.join(trainer.args.output_dir, \"final\")\n",
        "        os.makedirs(final_path, exist_ok=True)\n",
        "        trainer.save_model(final_path)\n",
        "        tokenizer.save_pretrained(final_path)\n",
        "        print(f\"‚úÖ Final model saved to {final_path}\")\n",
        "\n",
        "    print(f\"‚úÖ Training complete on rank {rank}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Training function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i9j0k1l2",
      "metadata": {},
      "source": [
        "## Create the Trainer Client\n",
        "\n",
        "Initialize the TrainerClient with authentication configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m3n4o5p6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create client with authentication\n",
        "api_client = k8s.ApiClient(configuration)\n",
        "\n",
        "backend_config = KubernetesBackendConfig(\n",
        "    client_configuration=api_client.configuration,\n",
        ")\n",
        "\n",
        "client = TrainerClient(backend_config)\n",
        "print(\"‚úÖ TrainerClient created\")\n",
        "\n",
        "# Get the torch-distributed runtime\n",
        "runtime = client.backend.get_runtime(\"torch-distributed\")\n",
        "print(f\"‚úÖ Using runtime: {runtime.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q7r8s9t0",
      "metadata": {},
      "source": [
        "## Submit the Training Job with JIT Checkpointing\n",
        "\n",
        "Now we create and submit the distributed training job with JIT checkpointing enabled.\n",
        "\n",
        "### Job Configuration\n",
        "\n",
        "| Parameter | Value | Description |\n",
        "|-----------|-------|-------------|\n",
        "| `num_nodes` | 2 | Number of GPU nodes for distributed training |\n",
        "| `nvidia.com/gpu` | 1 | GPUs per node |\n",
        "| `cpu` | 4 | CPU cores per node |\n",
        "| `memory` | 16Gi | Memory per node |\n",
        "\n",
        "### JIT Checkpointing Configuration\n",
        "\n",
        "| Parameter | Value | Description |\n",
        "|-----------|-------|-------------|\n",
        "| `enable_jit_checkpoint` | `True` | Save checkpoint on SIGTERM (preemption) |\n",
        "| `periodic_checkpoint_config` | See below | Configure periodic checkpoint saves |\n",
        "| `output_dir` | `pvc://shared/...` | PVC path for checkpoint storage |\n",
        "\n",
        "### How JIT Checkpointing Works\n",
        "\n",
        "When `enable_jit_checkpoint=True`:\n",
        "\n",
        "1. **SIGTERM Handler Registered:** TransformersTrainer registers a signal handler for SIGTERM\n",
        "2. **Safe Checkpoint on Signal:** When SIGTERM is received, training pauses after the current optimizer step\n",
        "3. **Async Checkpoint Save:** Model state is saved asynchronously using CUDA streams (if available)\n",
        "4. **Sentinel File:** A marker file ensures incomplete checkpoints are detected and cleaned up\n",
        "5. **Auto-Resume:** On restart, training automatically resumes from the latest valid checkpoint\n",
        "\n",
        "### Periodic Checkpointing\n",
        "\n",
        "In addition to JIT checkpointing, you can configure periodic saves:\n",
        "\n",
        "```python\n",
        "PeriodicCheckpointConfig(\n",
        "    save_strategy=\"steps\",  # or \"epoch\"\n",
        "    save_steps=20,           # Save every 20 steps\n",
        "    save_total_limit=2,      # Keep only 2 most recent checkpoints\n",
        ")\n",
        "```\n",
        "\n",
        "> **Note:** Periodic checkpointing blocks GPU training during the save operation. Avoid checkpointing too frequently (e.g., every step) as this can significantly increase total training time and waste GPU cycles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1v2w3x4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure periodic checkpointing - SDK injects these into TrainingArguments\n",
        "checkpoint_config = PeriodicCheckpointConfig(\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=20,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Create TransformersTrainer with JIT checkpointing enabled\n",
        "# The output_dir=\"pvc://shared/checkpoints/jit-checkpointing\" tells the SDK to:\n",
        "# - Mount PVC \"shared\" at /mnt/kubeflow-checkpoints on all training pods\n",
        "# - Set TrainingArguments.output_dir to /mnt/kubeflow-checkpoints/checkpoints/jit-checkpointing\n",
        "# - Enable JIT checkpointing (saves state on SIGTERM)\n",
        "# - Auto-resume from latest checkpoint on restart\n",
        "trainer = TransformersTrainer(\n",
        "    func=train_func,\n",
        "    num_nodes=2,\n",
        "    resources_per_node={\n",
        "        \"nvidia.com/gpu\": 1,\n",
        "        \"cpu\": \"4\",\n",
        "        \"memory\": \"16Gi\",\n",
        "    },\n",
        "    # Make the training function cleaner: set offline mode at the pod level\n",
        "    env={\n",
        "        \"HF_HUB_OFFLINE\": \"1\",\n",
        "        \"TRANSFORMERS_OFFLINE\": \"1\",\n",
        "    },\n",
        "    # JIT Checkpointing: Save checkpoint on SIGTERM (preemption)\n",
        "    enable_jit_checkpoint=True,\n",
        "    # Periodic Checkpointing: Save checkpoint every save_steps\n",
        "    periodic_checkpoint_config=checkpoint_config,\n",
        "    # PVC path for checkpoints - SDK handles mounting automatically\n",
        "    output_dir=\"pvc://shared/checkpoints/jit-checkpointing\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ TransformersTrainer configured with:\")\n",
        "print(\"   - JIT Checkpointing: ENABLED (saves on SIGTERM)\")\n",
        "print(\"   - Periodic Checkpointing: Every 20 steps, keep 2 most recent\")\n",
        "print(\"   - Auto-Resume: ENABLED (resumes from latest checkpoint)\")\n",
        "\n",
        "# Submit the training job\n",
        "job_name = client.train(\n",
        "    trainer=trainer,\n",
        "    runtime=runtime,\n",
        ")\n",
        "print(f\"\\n‚úÖ Training job submitted: {job_name}\")\n",
        "print(f\"üíæ Checkpoints will appear in workbench at: {NOTEBOOK_CHECKPOINTS_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y5z6a7b8",
      "metadata": {},
      "source": [
        "## Follow Job Logs\n",
        "\n",
        "Let's fetch our job logs to make sure training is going as expected. The logs will stream in real-time as the training progresses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d0e1f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stream logs (press Ctrl+C to stop if you want to continue with other cells)\n",
        "for logline in client.get_job_logs(job_name, follow=True):\n",
        "    print(logline, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g3h4i5j6",
      "metadata": {},
      "source": [
        "## Get Job Status\n",
        "\n",
        "Check the final status of the training job after completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k7l8m9n0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check job status\n",
        "job = client.get_job(job_name)\n",
        "print(\"Final TrainJob Status:\")\n",
        "print(f\"   Name: {job.name}\")\n",
        "print(f\"   Status: {job.status}\")\n",
        "print(f\"   Created: {job.creation_timestamp}\")\n",
        "print(f\"   Nodes: {job.num_nodes}\")\n",
        "print(f\"   Runtime: {job.runtime.name}\")\n",
        "\n",
        "if job.steps:\n",
        "    print(\"   Steps:\")\n",
        "    for step in job.steps:\n",
        "        print(f\"     - {step.name}: {step.status}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o1p2q3r4",
      "metadata": {},
      "source": [
        "## Verify Checkpoints\n",
        "\n",
        "After training completes (or after a preemption/restart), you can verify the checkpoints saved on the PVC.\n",
        "\n",
        "### Checkpoint Structure\n",
        "\n",
        "The training function saves checkpoints with this structure:\n",
        "```\n",
        "/opt/app-root/src/shared/checkpoints/jit-checkpointing/\n",
        "‚îú‚îÄ‚îÄ checkpoint-<step>/  # Intermediate checkpoints (saved every save_steps)\n",
        "‚îú‚îÄ‚îÄ checkpoint-<N>/    # Checkpoint at final step (N = last step number)\n",
        "‚îî‚îÄ‚îÄ final/             # Final merged model ready for inference\n",
        "```\n",
        "\n",
        "### JIT Checkpoint Behavior\n",
        "\n",
        "If training was interrupted by SIGTERM:\n",
        "- A checkpoint is saved at the last completed optimizer step\n",
        "- Incomplete checkpoints (with sentinel files) are automatically cleaned up on resume\n",
        "- Training resumes from the most recent valid checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s5t6u7v8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# List checkpoints on PVC\n",
        "import os\n",
        "\n",
        "if os.path.exists(NOTEBOOK_CHECKPOINTS_PATH):\n",
        "    print(f\"üìÇ Checkpoints at {NOTEBOOK_CHECKPOINTS_PATH}:\")\n",
        "    for item in sorted(os.listdir(NOTEBOOK_CHECKPOINTS_PATH)):\n",
        "        item_path = os.path.join(NOTEBOOK_CHECKPOINTS_PATH, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            files = os.listdir(item_path)\n",
        "            print(f\"   üìÅ {item}/ ({len(files)} files)\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Checkpoint directory not found: {NOTEBOOK_CHECKPOINTS_PATH}\")\n",
        "    print(\"   This is expected if training hasn't completed yet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w9x0y1z2",
      "metadata": {},
      "source": [
        "## Test the Trained Model (Optional)\n",
        "\n",
        "After training completes, you can load the fine-tuned model from the checkpoint saved on the shared PVC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b4c5d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_most_recent_checkpoint(output_dir):\n",
        "    \"\"\"Find the most recently created checkpoint directory.\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        raise FileNotFoundError(f\"Output directory not found: {output_dir}\")\n",
        "\n",
        "    checkpoint_dirs = [\n",
        "        os.path.join(output_dir, d)\n",
        "        for d in os.listdir(output_dir)\n",
        "        if os.path.isdir(os.path.join(output_dir, d))\n",
        "        and (d.startswith(\"checkpoint-\") or d == \"final\")\n",
        "    ]\n",
        "\n",
        "    if not checkpoint_dirs:\n",
        "        raise FileNotFoundError(f\"No checkpoints found in {output_dir}\")\n",
        "\n",
        "    # Prefer 'final' if it exists\n",
        "    final_path = os.path.join(output_dir, \"final\")\n",
        "    if os.path.exists(final_path):\n",
        "        return final_path\n",
        "\n",
        "    return max(checkpoint_dirs, key=os.path.getctime)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Checkpoint utility defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f8g9h0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find and load the trained model\n",
        "final_checkpoint = find_most_recent_checkpoint(NOTEBOOK_CHECKPOINTS_PATH)\n",
        "print(f\"üìÇ Loading checkpoint from: {final_checkpoint}\")\n",
        "\n",
        "trained_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    final_checkpoint, trust_remote_code=True\n",
        ")\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(\n",
        "    final_checkpoint,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda:0\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "print(f\"üìä Model parameters: {trained_model.num_parameters():,}\")\n",
        "\n",
        "# Test the model\n",
        "test_prompt = \"### Instruction:\\nExplain what machine learning is in one sentence.\\n\\n### Response:\"\n",
        "\n",
        "print(\"\\nüìù Testing model with prompt:\")\n",
        "print(test_prompt)\n",
        "print(\"\\nü§ñ Model response:\")\n",
        "\n",
        "inputs = trained_tokenizer(test_prompt, return_tensors=\"pt\").to(trained_model.device)\n",
        "\n",
        "# Remove token_type_ids if present (not used by some models like Qwen)\n",
        "if \"token_type_ids\" in inputs:\n",
        "    del inputs[\"token_type_ids\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trained_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "\n",
        "response = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response.replace(test_prompt, \"\").strip())\n",
        "\n",
        "print(\"\\n‚úÖ Model test completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i1j2k3l4",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Delete the training job and free resources.\n",
        "\n",
        "**Note:** To fully clean up, you may also want to delete the downloaded model, dataset, and checkpoints from the PVC:\n",
        "```bash\n",
        "rm -rf /opt/app-root/src/shared/models/qwen2.5-1.5b-instruct\n",
        "rm -rf /opt/app-root/src/shared/data/alpaca_processed\n",
        "rm -rf /opt/app-root/src/shared/checkpoints/jit-checkpointing\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m5n6o7p8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the training job\n",
        "client.delete_job(name=job_name)\n",
        "print(f\"‚úÖ Job {job_name} deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q9r0s1t2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Clear CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "gc.collect()\n",
        "print(\"‚úÖ Resources freed, CUDA cache cleared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u3v4w5x6",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You've successfully completed a distributed fine-tuning job with JIT checkpointing on OpenShift AI.\n",
        "\n",
        "### What You Accomplished\n",
        "\n",
        "| Step | Description |\n",
        "|------|-------------|\n",
        "| ‚úÖ Model Download | Downloaded Qwen 2.5 1.5B Instruct to shared PVC |\n",
        "| ‚úÖ Dataset Preparation | Processed Stanford Alpaca dataset for instruction-tuning |\n",
        "| ‚úÖ Distributed Training | Ran 2-node distributed training with PyTorch DDP |\n",
        "| ‚úÖ JIT Checkpointing | Enabled automatic checkpoint saves on SIGTERM |\n",
        "| ‚úÖ Periodic Checkpointing | Configured regular checkpoint saves every 20 steps |\n",
        "| ‚úÖ Auto-Resume | Training can resume from latest checkpoint on restart |\n",
        "| ‚úÖ Model Testing | Loaded and tested the fine-tuned model |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **JIT Checkpointing** makes training preemption-safe:\n",
        "   - Enable with `enable_jit_checkpoint=True`\n",
        "   - Automatically saves state when pod receives SIGTERM\n",
        "   - Uses CUDA streams for async checkpoint saves\n",
        "\n",
        "2. **Periodic Checkpointing** provides regular saves:\n",
        "   - Configure with `PeriodicCheckpointConfig`\n",
        "   - Control save frequency (`save_strategy`, `save_steps`)\n",
        "   - Limit disk usage (`save_total_limit`)\n",
        "\n",
        "3. **Auto-Resume** minimizes training loss:\n",
        "   - Training automatically resumes from latest valid checkpoint\n",
        "   - Incomplete checkpoints are detected and cleaned up\n",
        "   - No manual intervention required\n",
        "\n",
        "4. **PVC Storage** ensures durability:\n",
        "   - Use `output_dir=\"pvc://<pvc-name>/...\"` for automatic mounting\n",
        "   - Checkpoints persist across pod restarts\n",
        "   - All nodes can access the same checkpoint storage\n",
        "\n",
        "### When to Use JIT Checkpointing\n",
        "\n",
        "| Scenario | Recommendation |\n",
        "|----------|----------------|\n",
        "| Spot/Preemptible instances | **Enable** - Instances can be reclaimed anytime |\n",
        "| Kueue-managed workloads | **Enable** - Higher-priority jobs may preempt |\n",
        "| Long-running training | **Enable** - Protect against interruptions |\n",
        "| Short training runs | Optional - May add small overhead |\n",
        "\n",
        "### TransformersTrainer Checkpointing Reference\n",
        "\n",
        "| Parameter | Description | Default |\n",
        "|-----------|-------------|----------|\n",
        "| `enable_jit_checkpoint` | Save checkpoint on SIGTERM | `True` |\n",
        "| `periodic_checkpoint_config` | Configure periodic saves | `None` |\n",
        "| `output_dir` | PVC path for checkpoints (`pvc://...`) | Required |\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Kubeflow Trainer Documentation](https://www.kubeflow.org/docs/components/trainer/)\n",
        "- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)\n",
        "- [Stanford Alpaca Dataset](https://huggingface.co/datasets/tatsu-lab/alpaca)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
